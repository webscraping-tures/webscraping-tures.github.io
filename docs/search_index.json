[["index.html", "Web Scraping mit R Einführung Was ist Web Scraping? Was lernen wir, was nicht? Technische Voraussetzungen", " Web Scraping mit R Jakob Tures 2021-03-15 Einführung WIP! Was ist Web Scraping? Ziele Zugang zu Daten die wir auf klassischem Wege nicht bekommen könnten Zugang zu neuen Formen von Daten die nur im Web denkbar sind (bspw. Kundenreviews, Nutzerverhalten) Was ist möglich: Beispiele Kriesel 3C Methoden Identifikation für unsere Fragestellung relevanter Webinhalte Zugriff auf Webinhalte aus einer Programmiersprache heraus (R, Python, etc.) Extraktion der für unsere Fragestellung relevanten Teilinhalte der Seite Säuberung und Überführung in ein für die Datenanalyse geeignetes Format -&gt; Datenanalyse Ethik Grundregeln, später Sitzung dazu Was lernen wir, was nicht? Grundstruktur des Web HTML, XML Überblick zu anderen Bausteinen die wir in der Wildnis treffen, ohne zu tief hineinzugehen Einführung in R Basics in R Studio Tidyverse tidy data dplyr ggplot rvest evtl. stringr Scraping Scrapen ganzer Seiten mit rvest CSS Selectors und Developement Tools zur Identifikation des Pfades einzelner Inhalte Extraktion dieser Pfade mit rvest Evtl. gezieltes Auslesen von XML Inhalten Was nicht: Umgang mit dynamischen Websites, javascript, Datenbankzugriffen usw. (hier nur grober Überblick was es alles gibt und das es Wege gibt damit umzugehen -&gt; Fortgeschrittene) Umgang mit Strings &amp; regular expressions (Hinweis dass sie nicht darum herumkommen wenn sie den Weg weitergehen möchten, kurzer Überblick dazu wenn Zeit reicht) Scrapen mit anderen packages oder Sprachen als R + rvest (evtl. Hinweis auf weitere verbreitete Möglichkeiten) Technische Voraussetzungen R + R Studio Installation Tutorial? Packages: easy way: install.packages(“tidyverse”) ordentlicher Text Editor Windows: Notepad ++ Mac: ? Linux: Atom Browser Chrome/Chromium (wegen der Developer Tools?) "],["html.html", "1 HTML als Grundbaustein des Web 1.1 HTML-Tags 1.2 Attribute 1.3 Entities Weiterführende Literatur", " 1 HTML als Grundbaustein des Web Was passiert, wenn wir eine URL wie https://webscraping-tures.github.io in einem Browser aufrufen? Wir bekommen zunächst einmal eine visuelle Darstellung der aufgerufenen Seite in unserem Browserfenster. Aus der Sicht des Nutzers einer Website, ist dies auch alles was wir wissen müssen. Unser Ziel – die Seite aufzurufen – ist an diesem Punkt bereits erreicht. Aus der Sicht des Web Scrapers, müssen wir aber verstehen, was im Hintergrund passiert. Der Link https://webscraping-tures.github.io macht nichts anderes, als eine HTML-Datei aufzurufen, in welcher der Inhalt der Website in Form eines spezifischen Codes hinterlegt ist. Dieser Code wird von Ihrem Browser interpetiert und in eine visuelle Darstellung überführt, welche Sie aktuell vor sich sehen. Überzeugen Sie sich selbst. Mit einem Rechtsklick im Bereich dieses Texts und einem weiteren Klick auf “Seitenquelltext anzeigen” öffnet sich der HTML-Code, welcher dieser Seite zu Grunde liegt. Es ist an diesem Punkt völlig legitim, von der Flut aus ungewohnten Zeichen und Begriffen zunächst überfordert zu sein. Wer hätte gedacht, dass eine relativ einfache Website wie diese im Hintergrund so kompliziert sein kann? Aber die gute Nachricht ist, dass wir für’s Web Scraping nicht jedes Wort und Zeichen in einer HTML Datei verstehen müssen. Unser Ziel ist es die Teile einer Website, die für unsere Datensammlung relevant sind zu identifizieren und diese gezielt aus dem HTML-Code zu extrahieren. Dies ist möglicherweise nur eine einzelne Zeile aus einer HTML Datei mit tausenden von Zeilen. Müssen wir alle diese tausend Zeilen völlig verstehen? Nein, aber wir müssen die Struktur einer HTML Datei erfassen können, um diese eine Zeile, an der wir interessiert sind, auch identifizieren zu können. Bis zu diesem Punkt haben wir noch ein gutes Stück Weg vor uns. Am Ende dieses ersten Abschnitts werden Sie aber bereits ein grundlegendes Verständnis der Struktur und der Bestandteile eines HTML-Dokuments haben und der Quelltext wird Ihnen bereits deutlich weniger “beeindruckend” erscheinen. 1.1 HTML-Tags Die “Sprache”, in der HTML-Dateien verfasst sind, ist die Hypertext Markup Language, kurz HTML. Die Grundbausteine dieser Sprache, die “Vokabeln”, sind die sogenannten Tags. Begriffe die unter anderem dazu dienen, das HTML-Dokument zu strukturieren, den Text zu formatieren, Links und Bilder einzufügen oder Listen und Tabellen zu erzeugen. Der Browser kennt die Bedeutung dieser Begriffe, kann sie interpretieren und die Website entsprechend der in den HTML-Tags codierten Anweisungen visuell darstellen. Wie jede Sprache in der IT-Welt, folgt auch HTML dabei einer bestimmten “Grammatik”, der sogenannten Syntax. Glücklicherweise, fällt diese im Falle von HTML sehr einfach aus. Die für uns zentralen Tags und Syntaxregeln werden im Folgenden genauer betrachtet. Betrachten Sie folgendes Beispiel: &lt;b&gt;Hello World!&lt;/b&gt; &lt;b&gt; ist ein Tag. Das b steht hier für bold, auf deutsch fettgedruckt. Tags folgen stets dem selben Muster. Sie werden mit einem &lt; begonnen, dann folgt der Name des Tags – b – und sie werden mit einem &gt; beendet. Wichtig ist dabei, dass ein so geöffneter Tag im Normalfall auch geschlossen werden muss. Dazu wird der selbe Tag nochmals mit einem forward slash geschrieben, in diesem Beispiel also, &lt;/b&gt;. Alles was von öffnendem und schließendem Tag im HTML-Dokument umfasst wird, wird entsprechend der Bedeutung des Tags vom Browser interpretiert. Mit diesem Wissen verstehen wir, was das obige Beispiel bewirkt. Der Tag &lt;b&gt; steht für fettgedruckt und öffnender und schließender Tag &lt;b&gt;...&lt;/b&gt; umfassen den Text Hello World!. Dieser wird entsprechend der Bedeutung des Tags interpretiert, also als fettgedruckt dargestellt: Hello World! 1.1.1 hello_world.html Betrachten wir nun ein erstes vollständiges HTML-Dokument: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; Die Interpretation des Browsers von diesem Dokument können Sie sich unter https://webscraping-tures.github.io/hello_world.html selbst ansehen. Lassen Sie uns die einzelnen Bestandteile des Codes betrachten: Die erste Zeile, die document type declaration, &lt;!DOCTYPE html&gt; informiert den Browser darüber, welche HTML-Version in dem Dokument eingesetzt wurde. &lt;!DOCTYPE HTML&gt; steht dabei für den aktuellen Standard HTML5. Wurde eine ältere HTML Version verwendet, kann dies an dieser Stelle deklariert werden, um eine korrekte visuelle Repräsentation – trotz möglicher Veränderungen oder dem Wegfall bestimmter Funktionen im aktuellen Standard – sicherzustellen. Aus Sicht des Web Scrapers spielt die HTML-Version aber im Normalfall keine Rolle. Interessanterweise bildet der &lt;!DOCTYPE html&gt; Tag eine der wenigen Ausnahmen von der Regel, dass ein einmal geöffneter Tag auch wieder geschlossen werden muss. Dies ist hier nicht nötig. Der eigentliche Inhalt des HTML-Dokuments beginnt in Zeile 3 mit dem &lt;html&gt; Tag. Dieser wird erst in der letzten Zeile wieder geschlossen und umfasst somit den gesamten Inhalt des Dokuments. Der Tag teilt dabei dem Browser mit, dass alles was durch ihn umschlossen wird, HTML ist. Als nächstes folgt der Tag &lt;head&gt;. Alles, was durch ihn umfasst wird, ist noch nicht Teil des Inhalts den wir im Browserfenster sehen. Hier werden in der Praxis vor allem Meta-Informationen hinterlegt sowie erweiterte Funktionalitäten (JavaScript) und Designentscheidungen (Cascading Style Sheets – CSS) definiert oder aus externen Quellen eingebunden. Diese Punkte sollen uns in dieser Einführung in das Web Scraping vorerst nicht weiter ablenken, wir sollten aber wissen, dass Verweise auf .js und .css Dateien im &lt;head&gt; Tag auftauchen können. In unserem Beispiel umfasst der &lt;head&gt; Tag auschschließlich einen weiteren Tag namens &lt;title&gt;, welcher wiederum den Text Hello World! einschließt. Der &lt;title&gt; Tag bestimmt, was wir in der Titelleise unseres Browser sehen, in diesem Fall Hello World!. Alles was der &lt;body&gt; Tag umfasst, beschreibt endlich den Inhalt, den wir im Browserfenster sehen können. In diesem einfachen Beispiel ist nur eine Zeile enthalten. Der bereits bekannte &lt;b&gt;Tag umfasst den Text Hello World!, wodurch uns dieser fettgedruckt im Browserfenster dargestellt wird. Damit kennen Sie nun bereits die Grundstruktur jeder HTML Datei. Bei der Betrachtung des obigen Beispielcodes ist Ihnen möglicherweise aufgefallen, dass bestimmte Zeilen nach rechts eingerückt sind. Dies ist keine Vorraussetzung für funtionstüchtigen HTML Code, aber eine Konvention, die das schnelle Lesen und Erfassen vereinfacht. Dabei repräsentieren eingerückte Zeilen, die unterschiedlichen hierarschichen Ebenen des Codes. &lt;head&gt; ist hierarchisch &lt;html&gt;untergordnet und deshalb einfach eingerückt. &lt;title&gt; wiederum ist &lt;head&gt;untergeordnet und deshalb zweifach eingerückt. Auch ist durch die Schreibweise auf den ersten Blick ersichtlich, dass &lt;body&gt; zwar &lt;html&gt;aber nicht &lt;title&gt;untergeordnet ist, da &lt;body&gt;und &lt;title&gt; jeweils nur einfach eingerückt sind. Diese Konvention wird Ihnen häufig – aber nicht immer – in “realen” HTML-Dokumenten im Internet begegnen. Noch eine Anmerkung zur technischen Seite: HTML-Dokumente lassen sich grundsätzlich in jedem beliebigen Editor per Hand schreiben und müssen mit der Endung .html abgespeichert werden. Sie können dies testen, indem Sie selbst einen beliebigen Texteditor starten, den obigen HTML Code kopieren, die Datei mit der Endung .html abspeichern und in einem Browser ihrer Wahl öffnen. Allerdings werden Websites heutzutage aufgrund ihrer Komplexität im Normalfall nicht mehr per Hand geschrieben. Eine Vielzahl professioneller Tools bieten inzwischen sehr viel effizientere Wege, um Websites zu gestalten. Die Seite, welche Sie gerade betrachten, wurde beispielsweise direkt in R Studio mit Hilfe des Packages “bookdown” verfasst, welches den Großteil der Layoutentscheidungen automatisiert. 1.1.2 Wichtige Tags Wir können an dieser Stelle nicht alle in HTML verfügbaren Tags betrachten, sondern beschränken uns zunächst auf solche, die uns sehr häufig begegnen und die für unsere ersten Web Scraping Projekte besonders relevant sein werden. 1.1.2.1 Seitenstruktur Einen Tag der sich auf die Struktur der Seite bezieht, haben wir bereits weiter oben kennengelernt. Der &lt;body&gt; Tag teilt mit, dass alles was durch ihn umfasst wird, Teil des im Browserfenster dargestellten Inhalts ist. Eine Möglichkeit den Inhalt weiter zu strukturieren, ist der Einsatz der maximal sechs Ebenen von Überschriften, die HTML anbietet. Die Tags &lt;h1&gt; &lt;h2&gt; ... &lt;h6&gt; ermöglichen dies auf einfachem Wege. das h steht dabei für header. Der durch den Tag umfasste Text wird je nach Ebene der Überschrift automatisch nummeriert und in unterschiedlichen Schriftgrößen dargestellt. Als Beispiel sehen Sie hier die Struktur der Überschriften auf dieser Seite als HTML Code: &lt;h1&gt;HTML als Grundbaustein des Web&lt;/h1&gt; &lt;h2&gt;HTML Tags&lt;/h2&gt; &lt;h3&gt;hello_world.html&lt;/h3&gt; &lt;h3&gt;Wichtige Tags&lt;/h3&gt; &lt;h4&gt;Seitenstruktur&lt;/h4&gt; &lt;h4&gt;Formatierung&lt;/h4&gt; &lt;h4&gt;Listen&lt;/h4&gt; &lt;h4&gt;Tabellen&lt;/h4&gt; &lt;h2&gt;Attribute&lt;/h3&gt; &lt;h3&gt;Links&lt;/h4&gt; &lt;h3&gt;Bilder&lt;/h4&gt; &lt;h2&gt;Entities&lt;/h3&gt; Eine weitere häufig auftretende Form der Strukturierung eines HTML-Dokuments sind die über &lt;div&gt; (division) und &lt;span&gt; definierten Gruppierungen. Beide Tags funktionieren grundsätzlich gleich, wobei sich &lt;div&gt; auf eine oder mehrere Zeilen und &lt;span&gt; auf einen Teil einer Zeile bezieht. Beides hat zunächst keinen direkten Einfluss auf die Darstellung der Website, wird aber häufig in Kombination mit in den Cascading Style Sheets – CSS definierten Klassen zur Anpassung der visuellen Interpretation angewandt. Im Normalfall kann uns auch hier egal sein, wie die CSS-Klassen definiert sind und wie sie sich auf die Darstellung auswirken. Warum die Kombination aus &lt;div&gt; bzw. &lt;span&gt; und CSS-Klassen häufig ein sehr praktischer Ansatzpunkt für unsere Web Scraping Vorhaben sind und wie wir dies in unserer Arbeit ausnutzen können, lernen Sie im weiteren Verlauf dieses Seminars. Hier ein vereinfachtes Beispiel, wie beide Tags in HTML Code auftreten können: &lt;div&gt; Dieser Satz ist Teil des div Tags. Dieser Satz ist Teil des div Tags, &lt;span&gt; während dieser Nebensatz Teil des div Tags und des span Tags ist &lt;/span&gt; &lt;/div&gt; Dieser Satz ist nicht Teil des div Tags. 1.1.2.2 Formatierung In HTML stehen eine Vielzahl von Tags zur Formatierung des angezeigten Texts zur Verfügung. Im Folgenden betrachten wir einige besonders häufig auftretende. Der &lt;p&gt; Tag definiert den umfassten Text als Absatz (paragraph) und wird entsprechend in der Darstellung automatisch mit einem Zeilenumbruch abgeschlossen. &lt;p&gt;Dieser Satz ist Teil des Absatz. Ebenso wie dieser. Und dieser. &lt;/p&gt; Dieser Satz ist nicht mehr Teil des Absatz. Dies wird dargestellt als: Dieser Satz ist Teil des Absatz. Ebenso wie dieser. Und dieser. Dieser Satz ist nicht mehr Teil des Absatz. Der Tag &lt;br&gt; (break) leiten einen Zeilenumbruch ein. Dieser Tag ist eine weitere Ausnahme der Regel, dass ein geöffneter Tag auch wieder geschlossen werden muss. In diesem Sonderfall steht die Variante mit öffnendem und schließendem Tag &lt;br&gt;&lt;/br&gt; auch für zwei Zeilenumbrüche, ist also nicht äquivalent zu &lt;br&gt;. Im Unterschied zu dem Zeilenumbruch der durch &lt;p&gt;...&lt;/p&gt; am Ende des Absatz eingefügt wird, wird nach &lt;br&gt; kein weiterer Zeilenabstand eingefügt: Hier kommt etwas Text, der nun umgebrochen wird.&lt;br&gt; Nach dem break Tag wird im Gegensatz zum paragraph Tag kein Zeilenabstand eingefügt.&lt;br&gt; Wird der break Tag auch explizit wieder geschlossen, werden zwei Zeilenumbrüche eingefügt.&lt;br&gt;&lt;/br&gt; Wie hier zu erkennen ist. Dies wird dargestellt als: Hier kommt etwas Text, der nun umgebrochen wird. Nach dem break Tag wird im Gegensatz zum paragraph Tag kein Zeilenabstand eingefügt. Wird der break Tag auch explizit wieder geschlossen, werden zwei Zeilenumbrüche eingefügt. Wie hier zu erkennen ist. Das Schriftbild kann durch Tags wie das bereits bekannte &lt;b&gt; (bold) oder auch &lt;i&gt; (italics, also kursiv) ähnlich zu den bekannten Optionen in gängigen Textbearbeitungsprogrammen angepasst werden: Diese Tags können genutzt werden um Wörter, Sätze und Abschnitte &lt;b&gt;fett&lt;/b&gt; oder &lt;i&gt;kursiv&lt;/i&gt; darzustellen. Dies wird dargestellt als: Diese Tags können genutzt werden um Wörter, Sätze und Abschnitte fett oder kursiv darzustellen. 1.1.2.3 Listen Die Darstellung in Listenform bildet ein sehr dankbares Scraping Ziel. Die beiden häufigsten Varianten sind dabei die ungeordenete Liste, die durch &lt;ul&gt; (unordered list) eingeleitet wird, sowie die geordnete Liste, &lt;ol&gt; (ordered list). Der öffnende und schließende Listen-Tag umfasst dabei sets die gesamte Liste, während jedes einzelne Listenelement in beiden Varianten durch einen &lt;li&gt; Tag eingefasst ist. Dazu zwei kurze Beispiele: &lt;ul&gt; &lt;li&gt;Erstes ungeordnetes Element&lt;/li&gt; &lt;li&gt;Zweites ungeordnetes Element&lt;/li&gt; &lt;li&gt;Drittes ungeordnetes Element&lt;/li&gt; &lt;/ul&gt; Dies wird dargestellt als: Erstes ungeordnetes Element Zweites ungeordnetes Element Drittes ungeordnetes Element &lt;ol&gt; &lt;li&gt;Erstes geordnetes Element&lt;/li&gt; &lt;li&gt;Zweites geordnetes Element&lt;/li&gt; &lt;li&gt;Drittes geordnetes Element&lt;/li&gt; &lt;/ol&gt; Dies wird dargestellt als: Erstes geordnetes Element Zweites geordnetes Element Drittes geordnetes Element 1.1.2.4 Tabellen HTML kann auch genutzt werden um – ohne weitere Anpassungen der Darstellung über CSS zugegebenermaßen nicht besonders ansehnliche – Tabellen darzustellen. Diese werden durch einen &lt;table&gt; Tag geöffnet und entsprechend geschlossen. Innerhalb der Tabelle werden Zeilen durch &lt;tr&gt;...&lt;/tr&gt; (table row) definiert. Innerhalb der Zeilen können Tabellenüberschriften durch &lt;th&gt; (table header) und Zelleninhalte durch &lt;td&gt; (table data) bestimmt werden. Durch &lt;th&gt;...&lt;/th&gt;und &lt;td&gt;...&lt;/td&gt; umfasste Inhalte werden in ihrer Darstellung nicht nur unterschiedlich formatiert, aus Sicht des Web Scrapers ermöglichen uns diese Tags auch eine deutliche Unterscheidung der einzulesenden Tabelleninhalte. Hier ein einfaches Beispiel: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effekt&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; Dies wird dargestellt als: # Tag Effekt 1 “b” bold 2 “i” italics Die Formatierung in einer Art “Tabellenform” wie im obigen Beispiel ist nicht nötig, erhöht aber die intuitive Lesbarkeit. Tatsächlich ist folgende Schreibweise im Ergebnis äquivalent und in der Praxis häufiger anzutreffen: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effekt&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; Dies wird ebenfalls dargestellt als: # Tag Effekt 1 “b” bold 1 “i” italics 1.2 Attribute Viele HTML-Tags können durch den Einsatz sogenannter Attribute in ihrer Funktionsweise und Darstellung weiter angepasst werden. Dabei ist die grundsätzliche Syntax: &lt;tag attribute=\"value\"&gt;...&lt;/tag&gt;. Im öffnendem (nie im schließenden) Tag folgt auf den Namen des Tags der Name des Attributs = dem zuzuweisdenden Wert (value) in einfache oder doppelte Anführungszeichen eingefasst. IN HTML ist dabei &lt;tag attribute=\"value\"&gt; ungleich &lt;tag attribute = \"value\"&gt;. Das Paar aus Attributsname und Wert muss ohne Leerzeichen mit einem = verbunden werden um richtig interpretiert zu werden. Eine Vielzahl von Tags kann mit einer Vielzahl von Attributen modifiziert werden. Zwei der häufigsten und anschaulichsten Anwendungen sind das Einbinden von Links und Bildern, zwei weitere häufig anzutreffende HTML Tags. 1.2.1 Links Links werden über den Tag &lt;a&gt; (anchor) eingebunden. Der erste intuitive Versuch &lt;a&gt;Dies ist ein Link&lt;/a&gt; bleibt leider erfolglos: Dies ist (k)ein Link Der Text wird zwar dargestellt und als Link markiert – also blau und unterstrichen –, führt aber notwendigerweise zu keinem Ziel, da in dem HTML-Dokument nicht definiert wurde, was dieses Ziel sein soll. Hier kommt das erste Attribut ins Spiel. Mit &lt;a href=\"url\"&gt; wird das Ziel des Links definiert. href steht dabei für hypertext reference und dessen zugewiesener Wert kann unter anderem eine Website, eine Email Adresse oder auch eine Datei sein. So führt &lt;a href=\"webscraping-tures.github.io/html.html\"&gt;Dies ist ein Link&lt;/a&gt; zu der Seite die Sie gerade betrachten. Dies ist ein Link Ihnen wird dabei aufgefallen sein, dass sie nun wieder bis zu dieser Stelle scrollen mussten, um weiterzulesen. Ein zweites Attribut kann hier Abhilfe schaffen. Mit &lt;a href=\"webscraping-tures.github.io/html.html\" target=\"_blank\"&gt;Dies ist ein Link&lt;/a&gt; weisen wir den Browser an, den Link in einem neuen tab zu öffnen. Der zugewiesene Wert von target ist hier \"_blank\", was für einen neuen Tab steht, kann aber auch eine Reihe von anderen Werten annehmen. Dies ist ein Link Links werden für uns beim Web Scraping vor allem dann interessant, wenn wir von einer übergeordneten Seite die Links zu allen Unterseiten sammeln, um diese dann gezielt zu scrapen. Aber dazu später mehr. Noch ein Hinweis, der Tag &lt;link&gt; ist nicht mit &lt;a&gt; zu verwechseln und dient zur Einbindung externer Dateien, wie den bereits angesprochenen JavaScript oder CSS Dateien. 1.2.2 Bilder Bilder und Grafiken werden in HTML mit &lt;img&gt; eingebunden, ein weitere Tag der nicht explizit geschlossen werden muss. Damit der Browser auch weiß, welches Bild eingebunden werden soll, wird dies über das src (source) Attribut des Tags festgelegt. So bindet &lt;img src=\"webscraping-tures.github.io/Rlogo.png\"&gt; folgendes Bild ein: Über weitere Attribute ist es beispielsweise auch möglich, die Größe des Bildes in Pixeln anzupassen. &lt;img src=\"webscraping-tures.github.io/Rlogo.png\" width=\"100\" height=\"100\"&gt; Bilder können auch mit Links kombiniert werden. So definiert &lt;a href=\"https://www.r-project.org/\" target=\"_blank\"&gt;&lt;img src=\"webscraping-tures.github.io/Rlogo.png\"&gt;&lt;/a&gt; das Bild als Link, womit ein Klick auf das Bild Sie zu dem angegebenen Link führt: 1.3 Entities Eine Reihe von Schriftzeichen sind für den HTML-Code reserviert. So haben wir bereits gesehen, dass die Zeichen &lt; &gt; \" Bestandteile des Codes sind um Tags sowie Werte von Attributen zu definieren. In vielen Fällen ist es inzwischen möglich, auch reservierte Zeichen direkt in einem Fließtext anzuwenden. Uns werden im Web Scraping aber bis auf Weiteres regelmäßig sogenannte entities statt der eigentlichen Zeichen begegnen. Entities sind kodierte Repräsentationen bestimmter Zeichen. Diese werden stets mit &amp; eingeleitet und mit ; beendet. Zwischen beiden Zeichen steht entweder der Name oder die Nummer der enitity. So steht beispielsweise &amp;lt; für less than, also &lt; und &amp;gt; für greater than, also &gt;. Ein Text mit reservierten Zeichen wie &amp;lt; und &amp;gt; oder dem sogenannten &amp;quot;ampersand&amp;quot; &amp;amp;.` Ein Text mit reservierten Zeichen wie &lt; und &gt; oder dem sogenannten “ampersand” &amp;. Sidenote: Falls Sie an der Herkunft des Begriffs ampersand interessiert sind, empfiehlt sich Wikipedia: https://en.wikipedia.org/wiki/Ampersand Eine weitere entity die uns regelmäßig begegnen wird, ist &amp;nbsp; (non-breaking space), welches statt einem einfach Leerzeichen eingesetzt werden kann. &amp;nbsp; hat den Vorteil, dass dies zum Einen nie durch den Browser umgebrochen wird, zum Anderen ermöglicht es den Einsatz von mehr als einem Leerzeichen: Mit einem Leerzeichen dargestellt Mit vier&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Leerzeichen dargestellt Mit einem Leerzeichen dargestellt Mit vier    Leerzeichen dargestellt Eine Übersicht zu den gebräuchlichsten entities finden Sie hier: https://www.w3schools.com/html/html_entities.asp Die entities aller Unicode Zeichen finden Sie hier: https://unicode-table.com/ Weiterführende Literatur Noch einzubauen "],["R1.html", "2 R &amp; R Studio", " 2 R &amp; R Studio "],["tidyverse.html", "3 Tidyverse 3.1 Philosophie 3.2 Tidy data 3.3 Pipe", " 3 Tidyverse 3.1 Philosophie 3.2 Tidy data 3.3 Pipe "],["rvest1.html", "4 Erstes Scraping mit rvest 4.1 Das rvest package 4.2 hello_world.html 4.3 Countries of the World", " 4 Erstes Scraping mit rvest Mit dem Wissen, wie eine HTML-Datei aufgebaut ist und wie R und R-Studio in den Grundzügen funktionieren, sind wir mit den nötigen Werkzeugen ausgestattet, um unsere ersten Schritte im Web Scraping zu unternehmen. In dieser Sitzung werden wir lernen, wie wir mit Hilfe des R Packages rvest HTML-Quellcode in R-Studio einlesen, gezielt bestimmte Inhalte, an denen wir interessiert sind, extrahieren und die gesammelten Daten in ein R-Objekt überführen, um sie in der Zukunft weiter analysieren zu können. 4.1 Das rvest package Teil des tidyverse ist auch ein Package namens rvest, welches uns alle grundlegenden Funktionen für eine Vielzahl typischer Web Scraping Aufgaben bereitstellt. Dieses wurde bei der Installation des tidyverse Packages zwar bereits mitgeliefert, ist aber nicht Teil des core tidyverse und wird damit auch nicht mit library(tidyverse) in die aktuelle R Session geladen. Deshalb müssen wir dies explizit tun: library(rvest) ## Loading required package: xml2 Der Output in der R-Studio Konsole informiert uns darüber, dass neben rvest außerdem das Package xml2 geladen wurde, auf dem rvest in Teilen basiert und dessen Funktion zum Einlesen von HTML-Dateien wir im Folgenden benötigen werden. 4.2 hello_world.html Als erste Übung bietet es sich an, das bereits in Kapitel 1 beschriebene Hello World Beispiel zu Scrapen. Zur Erinnerung folgt nochmals der Quelltext der HTML-Datei: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; 4.2.1 read_html() Der erste Schritt im Web Scraping ist es, die Seite, an der wir interessiert sind in ein R-Objekt zu überführen. Dies ermöglicht uns die Funktion read_html() aus dem xml2 Package. Diese benötigt als erstes Argument die URL, also die Adresse der Website, die wir einlesen möchten. Die URL muss dabei als String angegeben werden. Die Funktion erlaubt auch, noch weitere Optionen festzulegen. In den meisten Fällen sind die Voreinstellungen aber ausreichend. Wir lesen also die hello_world.html Datei ein, weisen sie gleichzeitig einem neuen R-Objekt zu und lassen uns dieses Objekt im nächsten Schritt ausgeben: hello_world &lt;- read_html(&quot;https://webscraping-tures.github.io/hello_world.html&quot;) hello_world ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; Wie wir im Output sehen, handelt es sich bei dem R-Objekt hello_world um eine Liste mit zwei Einträgen. Der erste Eintrag beinhaltet alles, was durch den &lt;head&gt; Tag umschlossen wird, der zweite Eintrag alles, was durch den &lt;body&gt; Tag umschlossen wird. Der öffnende und schließende &lt;html&gt; Tag ist nicht Teil des Objektes. Wenn wir uns daran einnern, dass HTML-Code hierarchisch strukturiert ist, ist die Liste also anhand der höchsten verbleibenden Ebenen – &lt;title&gt; und &lt;body&gt; – organisiert. Damit haben wir erfolgreich eine Repräsentation der Website in einem R-Objekt angelegt. Doch was machen wir nun damit? Im Falle dieses einfachen Beispiels könnten wir eventuell daran interessiert sein, den Titel der Website oder den auf der Seite dargestellten Text zu extrahieren. 4.2.2 html_nodes() Die Funktion html_nodes() aus dem rvest Package erlaubt es uns, gezielt einzelne Elemente des HTML-Codes zu extrahieren. Dazu benötigt sie als erstes Argument das Objekt aus dem extrahiert werden soll und zusätzlich einen selector. In dieser Einführung werden wir uns ausschließlich auf die sogenannten CSS Selectors konzentrieren. Die Alternative XPath ist zwar nochmals etwas flexibler, CSS Selectors sind aber in den meisten Fällen ausreichend und haben eine intuitivere und kürzere Syntax, was sie hier klar zum Werkzeug der Wahl machen. Wir werden die Möglichkeiten, die CSS Selectors bieten in Kapitel 5.1 genauer behandeln und beschränken uns zunächst auf die Grundlagen. Ein Selector in der Form \"tag\", selektiert alle HTML-Tags des angegebenen Namens. Möchten wir also den &lt;title&gt; Tag extrahieren, können wir dies so erreichen: node_title &lt;- html_nodes(hello_world, css=&quot;title&quot;) node_title ## {xml_nodeset (1)} ## [1] &lt;title&gt;Hello World!&lt;/title&gt; Möchten wir den auf der Website dargestellten Text Hello World! extrahieren, wäre es eine Möglichkeit den kompletten &lt;body&gt; Tag zu selektieren, da ja in diesem Fall kein anderer Text auf der Seite dargestellt wird. node_body &lt;- html_nodes(hello_world, css=&quot;body&quot;) node_body ## {xml_nodeset (1)} ## [1] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; Dies hat zwar grundsätzlich funktioniert, wir haben aber auch die &lt;b&gt; Tags sowie mehrere Zeilenumbrüche (\\n) mit extrahiert und benötigen eigentlich beides nicht. Effizienter wäre es direkt den &lt;b&gt; Tag, der den Text umschließt zu selektieren. node_b &lt;- html_nodes(hello_world, css=&quot;b&quot;) node_b ## {xml_nodeset (1)} ## [1] &lt;b&gt;Hello World!&lt;/b&gt; 4.2.3 html_text() In diesem Fall interessieren wir uns für den Text im Titel und auf der Website, also den Inhalt der Tags. Diesen können wir in einem weiteren Schritt aus den selektierten HTML-Elementen extrahieren. Dies ermöglicht die rvest Funktion html_text(). Diese benötigt als einziges Argument das zuvor extrahierte HTML-Element. html_text(node_title) ## [1] &quot;Hello World!&quot; html_text(node_b) ## [1] &quot;Hello World!&quot; Damit haben wir unser erstes Web Scraping Ziel, die Extraktion des Titels und des auf der Seite dargestellten Texts erfolgreich abgeschlossen. Noch etwas zur Anwendung von html_text() auf Elemente, die selbst weitere Tags enthalten: Weiter oben haben wir das Objekt node_body extrahiert, welches neben dem dargestellten Text auch die &lt;b&gt; Tags sowie meherere Zeilenumbrüche enthält. Auch hier können wir den reinen Text extrahieren. html_text(node_body) ## [1] &quot;\\n Hello World!\\n &quot; Wir sehen, dass die Funktion praktischerweise die &lt;b&gt; Tags, an denen wir nicht interessiert waren, für uns entfernt hat. Es bleiben aber die Zeilenumbrüche sowie mehrere Leerzeichen, sogenannter whitespace. Beides lässt sich mit dem Argument trim=TRUE entfernen. html_text(node_body, trim=TRUE) ## [1] &quot;Hello World!&quot; 4.3 Countries of the World Betrachten wir nun eine etwas realitätsnahere Anwendung. Die Website https://scrapethissite.com/pages/simple/ listet die Namen von 250 Ländern, sowie deren Flagge, Hauptstadt, Einwohnerzahl und Größe in Quadratkilometern. Unser Ziel könnte es sein, diese Informationen für jedes Land in R einzulesen, um sie dann potentiell weiter analysieren zu können. Bevor wir damit beginnen, sollten wir die benötigten Packages laden (das tidyverse Package benötigen wir diese Mal im weiteren Verlauf auch) sowie die Website mit der Funktion read_html() einlesen und einem R-Objekt zuweisen. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.6 ✓ dplyr 1.0.4 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x readr::guess_encoding() masks rvest::guess_encoding() ## x dplyr::lag() masks stats::lag() ## x purrr::pluck() masks rvest::pluck() library(rvest) website &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) Um die Struktur der HTML-Datei nachzuvollziehen, hilft auch hier als erster Schritt der Blick in den Quellcode. Diesen können wir wie immer mit einem Rechtsklick im Browserfenster und einem weiteren Klick auf “Seitenquelltext anzeigen” öffnen. Die ersten gut 100 Zeilen des HTML-Codes enthalten vor allem Informationen zur Gestaltung der Website, die uns an dieser Stelle nicht weiter ablenken sollten. Wir sind rein an den Daten der Länder interessiert. Das erste Land, das auf der Website gelistet wird, ist Andorra. Es bietet sich also an, den Quellcode gezielt nach “Andorra” zu durchsuchen. Mit der Tastenkombination STRG+F öffnet sich die Suchmaske in Ihrem Browser. In Zeile 128 werden wir fündig. Da dieser für Übungszwecke konzipierte Quellcode sehr strukturiert formatiert ist, erkennen wir schnell, dass Zeilen 125–135 als Codeblock auf Andorra bezogen sind. Betrachten wir diese genauer: &lt;div class=&quot;col-md-4 country&quot;&gt; &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; &lt;/div&gt;&lt;!--.col--&gt; Die gesamten Informationen zu Andorra sind von einem &lt;div&gt; Tag umschlossen. Als Erinnerung: ein &lt;div&gt; definiert eine Gruppierung von Code über mehrere Zeilen hinweg. In der Webdesign-Praxis, werden diese Gruppierungen vor allem dazu genutzt, um über das Argument class= dem folgenden Code einen bestimmten CSS Style zuzuweisen, beispielsweise um das Schriftbild festzulegen. Wie die Stile definiert sind, kann uns aus der Web Scraping Perspektive in der Regel egal sein. Wir müssen nur wissen, dass wir diese Zuweisungen von CSS Klassen für unsere Zwecke ausnutzen können. Auf der nächstniedrigeren Ebene, finden wir zwei Blöcke, einen der unter Anderem den Namen des Landes umfasst und einen weiteren, der Informationen zu diesem Land enthält. Betrachten wir zunächst den ersten Block. 4.3.1 Ländernamen &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; Der Name “Andorra” wird von einem &lt;h3&gt; Tag eingefasst, also einer Überschrift der dritten Ebene. Neben dem Namen, finden wir innerhalb des Tags auch einen weiteren Tag über den das Bild der Flagge eingebunden wird. Da wir hier an den Grafiken nicht interessiert sind, können wir dies ignorieren. Auf dieser Website werden alle &lt;h3&gt; Tags ausschließlich dazu genutzt, die Namen der Länder darzustellen. Damit können wir den &lt;h3&gt; Tag als CSS Selector nutzen, um analog zum ersten Beispiel den umschlossenen Text auszulesen. node_country &lt;- html_nodes(website, css=&quot;h3&quot;) text_country &lt;- html_text(node_country, trim=TRUE) head(text_country, n=10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; Das Ergebnis sieht vielversprechend aus. Da die Struktur des Codeblocks für jedes Land gleich ist, wurde auf diesem Wege der Vektor text_country mit 250 Einträgen erzeugt, genau der Anzahl Länder, die auf der Website aufgelistet sind. Aus Gründen der Übersichtlichkeit, ist es häufig sinnvoll, nicht die kompletten und oft sehr langen Vektoren oder Dataframes auszugeben, sondern sich mit der Funktion head() die über das Argument n festgelegte Anzahl von Einträgen, beginnend mit dem ersten, auflisten zu lassen. 4.3.1.1 Die Pipe %&gt;% Spätestens an dieser Stelle sollten wir uns nochmals Gedanken zur Lesbarkeit und Struktur unseres R-Codes machen. Betrachten wir den vorangegangenen Codeblock: node_country &lt;- html_nodes(website, css=&quot;h3&quot;) text_country &lt;- html_text(node_country, trim=TRUE) Wie wir gesehen haben, erreichen wir damit unser Ziel. Wir haben aber dazu auch das Objekt node_country erstellt um den ersten Arbeitsschritt – das Auslesen der &lt;h3&gt; Tags – zwischenzuspeichern. Dieses Objekt werden wir nie wieder benötigen. Nutzen wir stattdessen die bereits vorgestellte Pipe %&gt;%, entfällt die Notwendigkeit des Zwischenspeicherns von Teilergebnissen und wir schreiben gleichzeitig intuitiveren und leichter verständlichen Code. country &lt;- website %&gt;% html_nodes(css=&quot;h3&quot;) %&gt;% html_text(trim=TRUE) head(country, n=10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; Zur Erinnerung: Die Pipe gibt das Ergebnis eines Arbeitschritts an die nächste Funktion weiter, die im tidyverse sowie in vielen anderen R-Funktionen (aber nicht Allen!) Daten als erstes Argument nimmt, welches wir dann nicht mehr explizit definieren müssen. Zum besseren Verständnis betrachten wir obiges Beispiel im Detail. Die erste Zeile übergibt das Objekt website an die Funktion html_nodes(). Wir müssen html_nodes() also nicht mehr mitteilen, auf welches Objekt sie angewandt werden soll, da wir dieses mit der Pipe bereits an die Funktion weitergegeben haben. Die Funktion wird also mit allen weiteren definierten Argumenten – hier css – auf das Objekt website angewandt und das Ergebnis erneut an die nächste Zeile weitergegeben, in der die Funktion html_text() auf dieses angewandt wird. Hier endet die Pipe und das Endergebnis wird dem Objekt country zugewiesen. Wir benötigen nun zwar drei statt zwei Zeilen um zu dem Selben Ergebnis zu kommen, die tatsächliche Tipparbeit hat sich aber verringert – vor allem wenn man die Pipe mit der Tastenkombination STRG+Shift+M erzeugt – und wir haben Code erzeugt der sich mit ein bisschen Übung intuitiver lesen und nachvollziehen lässt. Sollten wir also immer alle Arbeitsschritte mit Pipes verbinden? Nein. In vielen Fällen macht es durchaus Sinn, Zwischenergebnisse in einem Objekt zu speichern, nämlich immer dann wenn wir mehrfach auf dieses zugreifen werden. Wir könnten in unserem Beispiel auch das Einlesen der Website in die Pipe integrieren: country &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) %&gt;% html_nodes(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE) Insgesamt erspart uns dies noch mehr Tipparbeit. Da wir im Weiteren jedoch noch mehrfach auf die ausgelesene Website zugreifen müssen, würde dies auch bedeuten, dass der Ausleseschritt jedes Mal wiederholt werden muss. Zum Einen kann dies bei größeren Datenmengen spürbare Auswirkungen auf die Rechenzeit haben. Zum Anderen bedeutet dies auch, dass wir jedes Mal auf die Server der Website zugreifen und die Daten erneut herunterladen. Ohne triftige Gründe erzeugtes Datenaufkommen, sollten wir aber im Rahmen einer good practice des Web Scrapings vermeiden. Es macht also durchaus Sinn, das Ergebnis der read_html() Funktion in einem R-Objekt zu speichern, um es später mehrfach wiederverwenden zu können. 4.3.2 Haupstädte, Einwohnerzahl und Fläche Wenden wir uns nun den weiteren Informationen für jedes Land zu. Diese befinden sich im zweiten Block des weiter oben betrachteten HTML-Codes: &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; Wie wir sehen, sind sowohl der Name der Hauptstadt, die Einwohnerzahl des Landes sowie dessen Größe in Quadratkilometern in den Zeilen 2–4 jeweils von einem &lt;span&gt; Tag umschlossen. &lt;span&gt; definiert wie &lt;div&gt; Gruppierungen, allerdings nicht über mehrere Zeilen hinweg sondern für eine, oder wie hier einen Teil einer Zeile. Versuchen wir also die Namen der Haupstädte auszulesen, indem wir den &lt;span&gt; Tag als Selector nutzen. website %&gt;% html_nodes(css=&quot;span&quot;) %&gt;% html_text() %&gt;% head(n=10) ## [1] &quot;Andorra la Vella&quot; &quot;84000&quot; &quot;468.0&quot; &quot;Abu Dhabi&quot; ## [5] &quot;4975593&quot; &quot;82880.0&quot; &quot;Kabul&quot; &quot;29121286&quot; ## [9] &quot;647500.0&quot; &quot;St. John&#39;s&quot; Wir bekommen so zwar die Namen der Hauptstädte, aber auch die Einwohnerzahl und die Größe des Landes. span war als Selector zu unspezifisch. Da alle drei Arten von Länderdaten mit &lt;span&gt; Tags umfasst sind, werden auch alle drei ausgelesen. Wir müssen html_nodes() also genauer mitteilen, an welchem &lt;span&gt; wir interessiert sind. Hier kommen nun die bereits angesprochenen CSS-Klassen ins Spiel. Diese unterscheiden sich zwischen den drei Länderinformationen. So ist dem &lt;span&gt;, der den Namen der Hauptstadt umfasst, die Klasse \"country-capital\" zugewiesen. Diese Klasse können wir mit unserem CSS Selector gezielt ansteuern. Um eine Klasse auszuwählen, können wir die Syntax .Klassenname nutzen. Um also alle &lt;span&gt; auszuwählen, die die Klasse \"country-capital\" haben, können wir wie folgend vorgehen: capital &lt;- website %&gt;% html_nodes(css=&quot;span.country-capital&quot;) %&gt;% html_text() head(capital, n=10) ## [1] &quot;Andorra la Vella&quot; &quot;Abu Dhabi&quot; &quot;Kabul&quot; &quot;St. John&#39;s&quot; ## [5] &quot;The Valley&quot; &quot;Tirana&quot; &quot;Yerevan&quot; &quot;Luanda&quot; ## [9] &quot;None&quot; &quot;Buenos Aires&quot; Dies können wir analog für die Einwohnerzahl mit der Klasse country-population wiederholen. population &lt;- website %&gt;% html_nodes(css=&quot;span.country-population&quot;) %&gt;% html_text() head(population, n=10) ## [1] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ## [7] &quot;2968000&quot; &quot;13068161&quot; &quot;0&quot; &quot;41343201&quot; Betrachten wir den so erzeugten Vektor genauer, sehen wir, dass es sich dabei um einen character vector handelt. Dazu könen wir die Funktion str() nutzen, welche uns die Struktur eines R-Objekts ausgibt, darunter auch den genutzten Datentyp. str(population) ## chr [1:250] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ... Die Zahlen wurden also nicht als Zahlen ausgelesen sondern als Strings. Dies erlaubt unter anderem nicht, mit den Zahlen zu rechnen. (population[1] wählt hier das erste Element des Vektors aus, also “84000”). population[1] / 2 ## Error in population[1]/2: non-numeric argument to binary operator Eine Möglichkeit R anzuweisen den aus dem HTML-Code ausgelesenen “Text” als Zahlen zu speichern, ist die Nutzung der Funktion as.numeric(). population &lt;- website %&gt;% html_nodes(css = &quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric() str(population) ## num [1:250] 84000 4975593 29121286 86754 13254 ... population[1] / 2 ## [1] 42000 Auf dem selben Weg, lässt sich auch die Größe in Quadratkilometern mit der Klasse \"country-area\" auslesen. area &lt;- website %&gt;% html_nodes(css = &quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() str(area) ## num [1:250] 468 82880 647500 443 102 ... 4.3.3 Zusammenführen in einem Tibble Wir haben nun vier Vektoren erstellt, welche respektive die Informationen zum Namen des Landes, der zugehörigen Hauptstadt, der Bevölkerunsanzahl und der Größe des Landes beinhalten. Für Andorra: country[1] ## [1] &quot;Andorra&quot; capital[1] ## [1] &quot;Andorra la Vella&quot; population[1] ## [1] 84000 area[1] ## [1] 468 Damit könnten wir bereits weiterarbeiten, für viele Anwendungen ist es aber praktischer, wenn wir die Daten in Tabellenform zusammenstellen. Im tidyverse bietet sich dazu die Form des Tibbles an. countries &lt;- tibble( Land = country, Hauptstadt = capital, Bevoelkerung = population, Flaeche = area ) countries ## # A tibble: 250 x 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # … with 240 more rows Dies ist nicht nur übersichtlicher sondern erleichtert auch alle weiteren potentiellen Analyseschritte. Wenn wir uns sicher sind, dass wir die einzelnen Vektoren nicht benötigen, können wir das Auslesen der Daten und das Erstellen des Tibbles auch in einem einzelnen Schritt durchführen. Im Folgenden sehen Sie, wie der komplette Scraping Prozess in relativ wenigen Zeilen abgeschlossen werden kann. website &lt;- &quot;https://scrapethissite.com/pages/simple/&quot; %&gt;% read_html() countries_2 &lt;- tibble( Land = website %&gt;% html_nodes(css=&quot;h3&quot;) %&gt;% html_text(trim=TRUE), Hauptstadt = website %&gt;% html_nodes(css=&quot;span.country-capital&quot;) %&gt;% html_text(), Bevoelkerung=website %&gt;% html_nodes(css=&quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric(), Flaeche = website %&gt;% html_nodes(css=&quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() ) countries_2 ## # A tibble: 250 x 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # … with 240 more rows "],["css-selectors-developer-tools.html", "5 CSS selectors &amp; Developer Tools 5.1 CSS selectors 5.2 Developer Tools", " 5 CSS selectors &amp; Developer Tools 5.1 CSS selectors In dem vorangegangenen Abschnitt, haben Sie bereits die ersten CSS Selectors kennengelernt. Diese werden im Webdesign eigentlich dazu genutzt, um einzelne Elemente einer Website auszuwählen und einen CSS Style auf diese anzuwenden, also die Darstellung der Elemente zu definieren. Sie wurden also nicht im Hinblick auf Web Scraping Anwendungen entwickelt, wir können sie uns aber trotzdem zu Nutze machen, denn auch wir möchten einzelne Elemente einer Website auswählen, um diese gezielt zu extrahieren. CSS Selectors kommen in rvest als Argument der Funktion read_html() zum Einsatz. Als zweites Argument, das erste legt fest auf welche Daten die Funktion angewandt werden soll, geben wir einen Selector in der Form css = \"selector\" an. Dieser bestimmt welche Elemente des HTML-Code wir extrahieren möchten. Wichtig ist dabei, dass immer der gesamte Selector – unabhängig davon aus wie vielen einzelnen Teilen dieser besteht – von doppelten Anführungszeichen eingefasst ist. Zur Veranschaulichung, werden die CSS Selectors im Folgenden auf die Website https://webscraping-tures.github.io/wahlbeteiligung_2.html angewandt. Den Quellcode können sie auf dem bekannten Weg einsehen. Zunächst laden wir das rvest package und lesen die Website ein. library(rvest) website &lt;- &quot;https://webscraping-tures.github.io/wahlbeteiligung_2.html&quot; %&gt;% read_html() Den einfachsten Selector haben wir bereits kennengelernt. \"Tag\" wählt alle Vorkommnisse des angegebenen HTML-Tags aus. So können wir beispielsweise den Titel der Website – im Tag &lt;title&gt; oder die im Browserfenster dargestellte Überschrift – &lt;h3&gt; – auswählen. website %&gt;% html_nodes(css = &quot;title&quot;) ## {xml_nodeset (1)} ## [1] &lt;title&gt;Wahlbeteiligung Landtagswahlen&lt;/title&gt;\\n website %&gt;% html_nodes(css = &quot;h3&quot;) ## {xml_nodeset (1)} ## [1] &lt;h3&gt;Wahlbeteiligung bei der letzten Landtagswahl nach Bundesland:&lt;/h3&gt; 5.1.1 Klassen Auch den Selector für das Argument class – \".class\" – haben wir bereits im vorangegangenen Abschnitt kennengelernt. Alle &lt;span&gt; tags der Klasse \"bundesland-name\" können wir wie folgt auswählen: website %&gt;% html_nodes(css = &quot;.bundesland-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;span.bundesland-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; Die erste Variante des Selectors sagt, wähle alle Elemente der Klasse \"bundesland-name\" aus. Die zweite Variante sagt, *wähle alle &lt;span&gt; elemente der Klasse \"bundesland-name\" aus. Für diese Website sind beide Schreibweisen im Ergebnis äquivalent, da alle Elemente, denen die Klasse \"bundsland-name\" zugewiesen wurde auch &lt;span&gt; Tags sind. Dies muss aber nicht immer so sein. Unterschiedliche Tags mit unterschiedlichen Inhalten können in der Praxis durchaus die selbe Klasse haben. Grundsätzlich sollten wir uns beim konstruieren von CSS Selectors von zwei Fragen leiten lassen. Wie viele Bestandteile benötigen wir um unser Ziel zu erreichen? Und, wie gut ist der Selector lesbar? Die erste Frage ist dabei leitend. Wenn wir unser Ziel ein bestimmtes Element auszuwählen mit einem Selector nicht erreichen können, ist dieser noch nicht einsetzbar und muss überarbeitet werden. Haben wir einen funktionstüchtigen Selector, sollten wir uns aber auch Gedanken zur Lesbarkeit machen. Möchten wir unseren R Code an andere Menschen weitergeben, sollte dieser so klar geschrieben sein, dass er für Andere nachvollziehbar ist. Aber auch wenn wir selbst in einer Woche oder einem Monat nochmal in unseren Code schauen, kann es hilfreich sein einen möglichst klaren Selector zu lesen, denn wir können uns vermutlich nicht mehr an die Details des HTML-Quellcodes erinnnern. Dabei bedeutet Lesbarkeit auch, eine Balance zwischen Länge und Klarheit des Selectors zu erreichen. Zur Veranschaulichung: website %&gt;% html_nodes(css = &quot;body &gt; div#daten &gt; div[class^=bundesland-] &gt; span.bundesland-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; Dies ist der volle Pfad durch die hierarchische HTML-Struktur, welcher uns zu dem oben bereits gesehenen Ergebnis führt. Diesen müssen sie jetzt noch nicht voll verstehen. Sie sehen aber, dass der Selector lang und kompliziert werden kann, und hier handelt es sich um eine sehr einfach gestaltete Website. Für mich, ist der Selector \"span.bundesland-name\" eine gute Wahl, da er ermöglicht unser Ziel zu erreichen und dabei eine für mich angenehme Balance zwischen Kürze und Lesbarkeit hat. Dies müssen sie aber im Einzelfall für sich selbst entscheiden. 5.1.2 IDs Ein weiteres häufig auftretendes Attribut in HTML-Elementen, ist die id. Diese identifizieren einzelne HTML-Elemente mit einem eindeutigen zugewiesenen Namen. Diese werden unter anderem zu gestalterischen Zwecken, im Rahmen von Scripts zur dynamischen Gestaltung von Websites oder in HTML-Formularen eingesetzt. Uns interessiert dabei vor allem, dass wir sie nutzen können um einzelne Elemente gezielt zu extrahieren. In unserem Beispiel, hat der &lt;span&gt; Tag, welcher den Namen eines jeden Bundeslandes umfasst, eine id, welche das Land mit einem zweistelligen Kürzel identifiziert: &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; Der Selector für IDs wird als \"#id\" geschrieben. website %&gt;% html_nodes(css = &quot;#bw&quot;) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; In diesem Fall haben wir aber mehr ausgewählt, als wir wollten. Denn auch der &lt;img&gt; Tag, welcher die Flaggen der Bundesländer darstellt, hat das Attribut id=\"bw\". Dies verstößt zwar strenggenommen gegen die HTML-Regeln, aber auch dies ist Realität. Wir können uns nicht darauf verlassen, dass die Ersteller einer Website immer sauberen HTML-Code verfassen und müssen somit auch mit “Regelverstößen” und unerwarteten Strukturen umgehen können. Die Lösung ist an dieser Stelle die Kombination von ID und Element im Selector, in der Form Element#id. website %&gt;% html_nodes(css = &quot;span#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; Auch eine Kombination mit dem Klassenattribut ist möglich. website %&gt;% html_nodes(css = &quot;span.bundesland-name#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; Generell ist zu beachten, dass die CSS Selectors case-sensitive sind. Das bedeutet, dass wir auf Groß- und Kleinschreibung achten müssen. Im Eintrag für Hamburg, ist der Name der id groß geschrieben: &lt;span class=&quot;bundesland-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; Möchten wir dieses Element extrahieren, müssen wir diese Schreibweise auch im Selector anwenden, \"id=hh\" wird nicht funktionieren. website %&gt;% html_nodes(css = &quot;span#hh&quot;) ## {xml_nodeset (0)} website %&gt;% html_nodes(css = &quot;span#HH&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; 5.1.3 Attribute ID und Class sind Attribute von HTML-Tags. Da beide im Webdesign besonders relvant sind, existieren die oben vorgestllten shortcuts um diese schnell auszuwählen. Wir können aber alle in einem HTML-Code auftretenden Attribute zur Auswahl von Elementen nutzen. Der zugehörige CSS Selector wird als element[attribute] geschrieben. In unterem Bereich unserer Beispiel HTML, finden sich drei Links – &lt;a&gt; – diese haben neben href=\"url\" - der verlinkten Seite, noch das Attribut target_\"\" dieses legt fest, wie sich der Link öffnen soll. Der Wert \"_blank\" öffnet den Link einem neuen Browsertab, \"_self\" im aktiven Tab. Für den zweiten Link ist kein target=\"\" Attribut festgelegt. Um in unserem Beispiel alle &lt;a&gt; Tags auszuwählen die ein target Attribut haben, also den ersten und dritten, könnte man wie folgt vorgehen: website %&gt;% html_nodes(css = &quot;a[target]&quot;) ## {xml_nodeset (2)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... ## [2] &lt;a href=&quot;https://webscraping-tures.github.io/css-selectors-developer-tool ... Mit \"element[attribute='value']\" ist es möglich, nur Attribute mit einem bestimmten Wert auszuwählen. Dabei ist zu beachten, dass value in einfache Anführungszeichen eingefasst ist. Sie haben ja bereits gesehen, dass die Werte von HTML-Attributen im Quelltext stets von doppelten Anführungszeichen umschlossen sind. Da wir die doppelten Anführungszeichen schon nutzen um Beginn und Ende unseres CSS Selectors zu definieren, können wir diese nicht nochmals innerhalb des Selectors selbst nutzen. Stattdessen, verwenden wir einfache Anführungszeichen um den Wert des Attributs zu definieren. Dies ist eine Konvention die in vielen R Kontexten Anwendung findet. Innerhalb doppelter Anführungszeichen nutzen wir einfache, innerhalb einfacher doppelte, um mehrere Ebenen von Anführunszeichen realisieren zu können. Möchten wir nur den Link auswählen, der sich in einem neuen Tab öffnet: website %&gt;% html_nodes(css = &quot;a[target=&#39;_blank&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... Dies lässt sich weiter modifizieren, in dem wir angeben dass der Wert eines Attributs einem bestimmten Anfang – \"element[attribute^='anfang']\" – oder ein bestimmtes Ende – \"element[attribute$='ende']\" – haben, oder einen bestimmten Teilbegriff enthalten soll – \"element[attribute*='teilbegriff']\". So könnten wir die Links anhand des Beginns, des Endes oder einem beliebigen Bestandteil der dem href=\"\" Attribut zugewiesenen URL auswählen. website %&gt;% html_nodes(css = &quot;a[href^=&#39;https://www&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... website %&gt;% html_nodes(css = &quot;a[href$=&#39;Deutschland&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://de.wikipedia.org/wiki/Flaggen_und_Wappen_der_L%C3%A4nder ... website %&gt;% html_nodes(css = &quot;a[href*=&#39;webscraping&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://webscraping-tures.github.io/css-selectors-developer-tool ... 5.1.4 Hierachieebenen Wir können uns die Hierarchie einer HTML-Struktur analog zu einem Familienstammbaum vorstellen. Eine vereinfachte Darstellung unserer Beispielseite als Stammbaum, könnte so aussehen: Per Klick auf die Elemente des Stammbaums, können sie die Hierarchiebenen ein- und ausklappen. Klicken Sie sich einmal durch die verschiedenen Ebenen und vergleichen Sie dies mit dem HTML-Quellcode der Seite. Wenn Sie damit fertig sind, schlage ich vor&lt;head&gt;einzuklappen und in &lt;body&gt; nur &lt;div id=\"daten\"&gt; und auf der nächst niedrigeren Ebene den ersten &lt;div&gt; auszuklappen. Damit sollten Sie alles sehen, was wir im Folgenden benötigen. Mit der Metapher des Stammbaums und den dazugehörigen Begriffen “descendant”, “child/parent” und “sibling”, sollte es uns leichter fallen, die folgenden etwas fortgeschritteneren Selector Konzepte zu verstehen. 5.1.4.1 Descendant Wenn Element A ein “descendant” von Element B ist, bedeutet dies dass A von B über beliebig viele Generationen “abstammt”. Dies kann eine Generation – also eine direkte Kind-Eltern Beziehung – oder beliebig viele Generationen – also Enkel und Großeltern mit beliebig vielen “Ur” Präfixen – sein. Im Selector schreiben wir dies als \"B A\". Möchten wir beispielsweise alle &lt;span&gt; Tags auswählen die von dem Tag &lt;div id=\"daten\"&gt; abstammen, also eine Enkel-Großeltern Beziehung: website %&gt;% html_nodes(css = &quot;div#daten span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; Dabei ist zu beachten, dass wir mit CSS Selectors immer nur jüngere Generationen auswählen können. Wir können also beispielsweise nicht Die Großeltern der Enkel auswählen indem wir den Selector “herumdrehen”. Um die Großeltern – also &lt;div id=\"daten\"&gt; auszuwählen, müssten wir betrachten, von welchem Element diese abstammen, also beispielsweise &lt;body&gt;. Die CSS Selectors sind nicht darauf beschränkt das Verhältnis von zwei Generationen abzubilden. Ein längerer Selector der vier Generationen beinhaltet und zum selben Ergebnis führt könne so aussehen: website %&gt;% html_nodes(css = &quot;body div#daten div span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; 5.1.4.2 Child/Parent Möchten wir statt Abstammungen über beliebig viele Generationen hinweg, direkte Kind-Eltern Beziehungen definieren, können wir dies mit Selectors in der Form \"B &gt; A\" durchführen. Dies bedeutet, A ist ein direkts Kind von B. Da die zuvor ausgewählten &lt;span&gt; Tags keine direkten Kinder von &lt;div id=\"daten\"&gt; sind, würde \"div#daten &gt; span\" hier nicht zum gewünschten Ziel führen. Die &lt;span&gt; Tags sind aber direkte Kinder der &lt;div&gt; mit den Klassen \"bundesland-odd\" bzw. \"bundesland-even\". Da es keine weitere direkte Kind-Eltern Beziehung von &lt;span&gt; und &lt;div&gt; Tags in unserem Beispiel gibt, wäre der Selector \"div &gt; span\" bereits ausreichend. Ich denke, wir könnten aber expliziter werden um weniger fehleranfälligen Code zu schreiben. Beide &lt;div&gt; Tags ähneln sich darin, dass ihre Klassen mit \"bundesland\" beginnen und wir wissen ja bereits, wie wir dies ausnutzen können: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; Möchten wie nicht alle Kinder auswählen, sondern nur bestimmte, besteht eine Reihe von Möglichkeiten. \":first-child\" wählt das erste Kind eines Elements aus. website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; :first-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... Dies sind jedoch nicht die &lt;span&gt; sondern die &lt;img&gt; Tags , wie uns auch ein schneller Blick in den Stammbaum nochmals vergegenwärtigt. Mit \":nth-child(n)\" können wir nochmals genauer bestimmen, an welchem Kind – gezählt in der Reihenfolge in der sie im HTML-Code auftauchen – wir interessiert sind. Um nochmals klarer zu machen, dass wir nur an &lt;span&gt; Kindern interessiert sind, kombinieren wir \"span\" und \"nth-child(n)\" website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:nth-child(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; Wir möchten also alle &lt;span&gt; selektieren die das zweite Kind von &lt;div&gt; deren Klassen mit \"bundesland\" beginnen sind. Möchten wir nur das letzte Kind selektieren, könnten wir dies mit \"span:nth-child(4)\" selektieren, oder wir nutzen eine Abkürzung und wenden \":last-child\" an. website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:last-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; 5.1.4.3 Sibling(s) Die gemeinsamen Kinder eines Eltern-Elements, lassen sich im Rahmen der Metapher des Stammbaums als Geschwister sehen. Auf der untersten Hierarchieebene, haben wir in unserem Beispiel also pro Bundesland vier Geschwister. Einen &lt;img&gt; und drei &lt;span&gt; Tags. Der Selector \"Element-A ~ Element-B\" wählt dabei alle Geschwister B aus, die auf Geschwister A folgen. website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; website %&gt;% html_nodes(css = &quot;img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; Das Ergebnis bei beiden Schreibweisen ist identisch. Hätten wir in unserem Beispiel &lt;span&gt; Tags die auf &lt;img&gt; Tags folgen auch an anderen Stellen im HTML-Code, wäre die zweite Variante aber nicht mehr explizit genug. Der Selector \"Element-A + Element-B\" funktioniert analog, wählt aber nur das Geschwisterkind B aus, das direkt auf Geschwister A folgt. website %&gt;% html_nodes(css = &quot;img + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;span.bundesland-name + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; Weiter einschränken lässt sich die Selektion der Geschwister mit \"element:first-of-type\", \"element:nth-of-type(n)\" und \"element:last-of-type\". So lassen sich das erste, n-te und letzte Geschwisterkind eines bestimmten Typs auswählen. Die Geschwister werden also nach der Art des Elements unterschieden, womit unsere Stammbaum Metapher langsam etwas überstrapaziert ist. website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:first-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:nth-of-type(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:last-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; 5.1.5 Weitere Selektoren Die wildcard \"*\" wählt alle Elemente aus. So können wir beispielsweise auch auf folgendem Weg, alle Kinder eines Elements selektieren: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; *&quot;) %&gt;% head(n = 8) ## {xml_nodeset (8)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [3] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [4] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [5] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... ## [6] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [7] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [8] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; Möchten wir bestimmte Elemente bei der Selektion ausschließen, ist dies mit \":not(selector)\" möglich. Hier werden alle Elemente ausgewählt, außer denen die wir so explizit augeschlossen haben. Beispielsweise alle Kinder, außer den &lt;img&gt; Elementen: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; :not(img)&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; Mit \"selector-A, selector-B\" lassen sich mehrere Selektoren mit “und” verknüpfen. So wird es beispielsweise möglich, zwei &lt;span&gt; Klassen in einem Schritt zu selektieren: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span.wahljahr, div[class^=&#39;bundesland&#39;] &gt; span.wahlbeteiligung&quot;) %&gt;% head(n = 4) ## {xml_nodeset (4)} ## [1] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [3] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [4] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; Sie kennen jetzt die im Web Scraping gebräuchlisten Selektoren. Eine Übersicht zu diesen und allen weiteren CSS Selectors, findet sich auf den Seiten der W3: https://www.w3schools.com/cssref/css_selectors.asp Neben CSS Selectors gibt es mit XPath eine weitere Möglichkeit Elemente einer HTML-Seite auszuwählen. Diese Methode ist noch etwas flexibler und erlaubt unter anderem auch die Auswahl von “ancestors”, also von höheren Hierarchieebenen, ausgehend von einer niedrigeren. Der Preis für die höhere Flexibilität ist aber auch eine oft längere und etwas kompliziertere Syntax. In den meisten Fällen, werden Ihnen die CSS Selectors völlig ausreichen. Und falls Sie irgendwann an einen Punkt stoßen, an denen sie nicht mehr ausreichen um ihr Scraping Ziel zu erreichen, sind sie bereits so geübt im Umgang mit den CSS Selectors, dass Ihnen der Umstieg auf XPath sehr leicht fallen wird. Auf den Seiten der W3, finden sie dann eine passende Einführung: https://www.w3schools.com/xml/xpath_intro.asp 5.2 Developer Tools Das Erkennen der HTML-Struktur und das identifizieren von funktionstüchtigen Selektoren, kann auf komplexen Websites sehr schwer sein, vor allem dann, wenn der HTML-Code nicht so übersichtlich formatiert ist, wie in unserem Beispiel. Hier kann es hilfreich sein, die in modernen Browsern integrierten Web Developer Tools zu verwenden. Auf den folgenden Screenshots, sehen Sie die Anwendung in Chromium für Linux auf unsere laufende Beispielseite. Das Vorgehen ist aber in anderen Browsern wie Chrome, Firefox oder Edge nahezu identisch. In allen Fälllen öffnen sie die Developer Tools durch einen Rechtsklick im Browserfenster und dann der Auswahl “Untersuchen” bzw. “Inspect”. Je nach Einstellung öffnen sich die devloper tools in einem horizontal oder vertikal abgetrennten Bereich des Browsers. Wir sehen hier eine Vielzahl von Reitern, für diese Einführung konzentrieren wir uns aber rein auf den Reiter “Elements”. Dieser zeigt uns den HTML-Code in seiner hierarchischen Struktur und erlaubt einzelne Elemente ein- und auszuklappen. Wählen wir ein Element in “Elements” aus, wird dieses auch in der Darstellung im Browserfenster markiert. Auch können wir den “Inspector” aktivieren und Elemente direkt im Browserfenster per Klick auswählen. So wird wiederum der korrespondierende Eintrag im “Elements” Reiter ausgewählt. Das untere Ende des “Elements” Reiters, zeigt uns den zu dem ausgewählten Element gehörenden vollen CSS Selector. Mit einem Rechtsklick auf ein Element und der Auswahl von “Copy” -&gt; “Copy selector”, können wir uns außerdem einen CSS Selector in den Zwischenspeicher holen, den wir dann in R Studio mit STRG+V einfügen können. Für das oben ausgewählte Element, also dem Ländernamen für Baden-Württemberg, bekommen wir den Selector “#bw”. In diesem Fall ist dieser Kurze Selector ausreichend, um das Element eindeutig zu identifizieren. Ich empfehle aber, wie oben ausgeführt, die Konstruktion von eindeutigeren und besser nachvollziehbaren Selektoren. Der von den Developer Tools erstellte Selector, sowie vor allem die Darstellung des vollen Pfads im “Elements” Reiter, können jedoch bei der Konstruktion dieser sehr hilfreich sein und ermöglichen ein schnelles Erfassen der Struktur einer Website. Auch haben wir so zwar erfahren, wie wir den namen für Baden-Württemberg auswählen, aber beispielsweise nicht, wie wir alle Namen der Bundesländer selektieren können. Aber auch hier können die Developer Tools hilfreich sein. Eine Herangehensweise wäre es, mehrere der Namenselemente nacheinander auszuwählen und zu vergleichen, wie sich der volle CSS-Selector verändert. Identifizieren wir darüber Gemeinsamkeiten und Unterschiede zwischen den Namenselemente, kann uns dies Ansatzpunkte geben um unseren eigenen Selector zu formulieren. Eine weitere Möglichkeit wäre es, statt der integrierten Developer Tools, die Erweiterung SelectorGadget zu nutzen. Diese ermöglicht unter anderem die Auswahl mehrere Elemente gleichzeitig. Informationen zur Nutzung finden Sie unter: https://selectorgadget.com/ "],["rvest2.html", "6 Scraping von Tabellen &amp; dynamischen Websites 6.1 Scraping von Tabellen 6.2 Dynamische Websites", " 6 Scraping von Tabellen &amp; dynamischen Websites 6.1 Scraping von Tabellen Im Web Scraping werden wir häufig das Ziel verfolgen, die extrahierten Daten in einen Tibble oder Data Frame zu überführen, um diese dann weiter analysieren zu können. Besonders dankbar ist es da, wenn die Daten an denen wir interessiert sind bereits in einer HTML-Tabelle gespeichert sind. Denn rvest erlaubt es uns mit der Funktion html_table() komplette Tabellen schnell und einfach auszulesen. Zur Erinnerung, Der HTML-Code für Tabellen ist in der Grundstruktur so gestaltet: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effekt&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; Der &lt;table&gt; Tag umfasst die gesamte Tabelle. Zeilen werden durch &lt;tr&gt; definiert, Spaltenüberschriften mit &lt;th&gt; und Zellen mit &lt;td&gt;. Bevor mit dem Scraping beginnen, laden wir wie immer die nötigen packages: library(tidyverse) library(rvest) 6.1.1 Tabelle mit CSS Selectors aus Wikipedia Auf der Wikipedia Seite zu “CSS”, findet sich auch eine Tabelle mit CSS Selectors. Diese ist unser Scraping Ziel. Lesen wir zunächst die Website ein: website &lt;- &quot;https://en.wikipedia.org/wiki/CSS&quot; %&gt;% read_html() Betrachten wir den Quellcode und suchen – STRG+F – nach “&lt;table”, erkennen wir, dass diese Seite eine Vielzahl von HTML-Tabellen enthält. Dazu gehören nicht ausschließlich die auf den ersten Blick als “klassische” Tabellen zu erkennenden Elemente sondern unter anderem auch die “Infoboxen” am rechten oberen rand des Artikels oder die ausklappbaren Auflistungen weiterführender Links am unteren Ende. Wenn Sie dies genauer betrachten möchten, können die Web Developer Tools hier sehr hilfreich sein. Statt einfach alle &lt;table&gt; nodes der Seite zu selektieren, könnte eine Strategie sein, mit Hilfe der WDT einen CSS Selector für diese spezifische Tabelle zu erzeugen: \"table.wikitable:nth-child(28)\". Wir selektieren so, die Tabelle der Klasse \"wikitable\" die das 28te Kind der übergeordneten Hierarchiebene ist – hier &lt;div class=\"mw-parser-output\"&gt;. Wenn wir nur ein einzelnes HTML-Element auswählen möchten, kann es hilfreich sein, statt html_nodes() die Funktion html_node() zu nutzen. node &lt;- website %&gt;% html_node(css = &quot;table.wikitable:nth-child(28)&quot;) nodes &lt;- website %&gt;% html_nodes(css = &quot;table.wikitable:nth-child(28)&quot;) nodes ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th ... node ## {html_node} ## &lt;table class=&quot;wikitable&quot;&gt; ## [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th&gt;\\n&lt;th&gt;First defined&lt;br&gt;i ... Der Unterschied besteht vor allem darin, was die Funktion uns “zurückgibt”. Dies ist erkennbar an dem Eintrag innerhalb der geschweiften Klammern im Output. Im ersten Fall, bekommen wir eine Liste von HTML Elementen – ein “xml_nodeset” –, auch wenn diese Liste, wie hier, nur aus einem Eintrag besteht. html_node() gibt uns direkt das HTML Element – “html_node” – als Ergebnis der Funktion. Warum ist dies relevant? In vielen Fällen kann es einfacher sein direkt mit dem HTML-Element statt einer Liste von HTML Elementen zu arbeiten, beispielsweise bei der Überführung von Tabellen in Data Frames und Tibbles, aber dazu später mehr. Um nun die so ausgewählte Tabelle auszulesen, müssen wir nur noch die Funktion html_table() auf unser Element anwenden. css_table_df &lt;- node %&gt;% html_table() css_table_df %&gt;% head(n = 4) ## Pattern ## 1 E ## 2 E:link ## 3 E:active ## 4 E::first-line ## Matches ## 1 an element of type E ## 2 an E element is the source anchor of a hyperlink of which the target is not yet visited (:link) or already visited (:visited) ## 3 an E element during certain user actions ## 4 the first formatted line of an E element ## First definedin CSS level ## 1 1 ## 2 1 ## 3 1 ## 4 1 Das Ergebnis ist ein Data Frame der die gescrapten Inhalte der HTML-Tabelle enthält und dabei die in den &lt;th&gt; Tags hinterlegten Spaltennamen für die Spalten des Data Frames übernimmt. Durch die sehr langen Zellen in der Spalte “Matches”, ist der Output der R Studio Console leider nicht besonders hilfreich. Ein weiterer Vorteil der Nutzung von Tibbles statt Data Frames, ist es, dass lange Zelleninhalte in der Darstellung im Output automatisch abgekürzt werden. Um einen Data Frame in einen Tibble umzuwandeln, können wir die Funktion as_tibble() nutzen. css_table_tbl &lt;- node %&gt;% html_table() %&gt;% as_tibble() css_table_tbl %&gt;% head(n = 4) ## # A tibble: 4 x 3 ## Pattern Matches `First definedin CSS… ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 E an element of type E 1 ## 2 E:link an E element is the source anchor of a hype… 1 ## 3 E:active an E element during certain user actions 1 ## 4 E::first-l… the first formatted line of an E element 1 6.1.2 Scrapen mehrerer Tabellen Es könnte auch unser Scraping Ziel sein, nicht nur die erste sondern alle vier inhaltlichen Tabellen des Wikipedia Artikels zu scrapen. Betrachten wir die vier Tabellen im Quellcode und/oder den WDTs, zeigt sich, dass sie alle die Klasse \"wikitable\" tragen. Damit können wir sie leicht selektieren. Hierbei bitte beachten, dass wieder die Funktion html_nodes() genutzt werden muss, da wir nicht mehr nur ein Element, sondern wieder eine Liste mehrerer selektierter Elemente benötigen. tables &lt;- website %&gt;% html_nodes(css = &quot;table.wikitable&quot;) %&gt;% html_table() Das Ergebnis ist eine Liste von vier Data Frames, welche jeweils eine der vier Tabellen enthalten. Möchten wir einen Einzelnen der Data Frames aus der Liste auswählen um ihn beispielsweise in ein neues Objekt zu überführen, sind wir auf das sogenannte subsetting angewiesen. Grundsätzlich bestehen in R zwei Möglichkeiten des subsettings von Listen. liste[#] und liste[[#]]. Der für uns relevanteste Unterschied besteht darin, was für eine Art Objekt R an uns zurückgibt. Im ersten Fall wird uns immer eine Liste zurückgegeben, auch wenn diese Möglicherweise nur aus einem Element besteht. Die Nutzung doppelter eckiger Klammern, gibt uns hingegen ein einzelnes Element direkt zurück. Der Unterschied ist also ähnlich zu dem zwischen html_nodes() und html_node(). Haben wir das Ziel beispielsweise den dritten Data Frame aus der Liste mit vier Data Frames auszuwählen, welches subsetting müssten wir nutzen? tables[3] %&gt;% str() ## List of 1 ## $ :&#39;data.frame&#39;: 7 obs. of 2 variables: ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... tables[[3]] %&gt;% str() ## &#39;data.frame&#39;: 7 obs. of 2 variables: ## $ Selectors : chr &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## $ Specificity: chr &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... Die Funktion str() zeigt uns die interne Struktur eines R Objekts an. Der eigentliche Inhalt wird dabei verkürzt dargestellt, was die Übersichtlichkeit erhöht und den Fokus auf die uns hier interessierende Struktur lenkt. Im ersten Fall sehen wir, dass wir eine Liste der Länge 1 haben, welche einen Data Frame mit 7 Zeilen und 2 Variablen enthält, sowie weitere Informationen zu diesen Variablen. Im zweiten Fall bekommen wir den Data Frame direkt, also nicht mehr als Element einer Liste. Wir müssen also liste[[]] nutzen um direkt einen einzelneen Data Frame aus einer Liste von Data Frames auswählen zu können. Sind wir stattdessen an einer Auswahl mehrerer Elemente aus einer Liste interessiert, ist dies nur mit liste[] möglich. Statt ein Element mit einer einzelnen Zahl auszuwählen, können wir mehrere mit einem Vektor von Zahlen in einem Schritt selektieren. tables[c(1, 3)] %&gt;% str() ## List of 2 ## $ :&#39;data.frame&#39;: 42 obs. of 3 variables: ## ..$ Pattern : chr [1:42] &quot;E&quot; &quot;E:link&quot; &quot;E:active&quot; &quot;E::first-line&quot; ... ## ..$ Matches : chr [1:42] &quot;an element of type E&quot; &quot;an E element is the source anchor of a hyperlink of which the target is not yet visited (:link) or already visited (:visited)&quot; &quot;an E element during certain user actions&quot; &quot;the first formatted line of an E element&quot; ... ## ..$ First definedin CSS level: int [1:42] 1 1 1 1 1 1 1 1 1 1 ... ## $ :&#39;data.frame&#39;: 7 obs. of 2 variables: ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... Wir bekommen als Ergebnis erneut eine Liste, welche die beiden hier ausgewählten Elemente enthält. 6.1.3 Tabellen mit NAs Was passiert, wenn wir versuchen eine Tabelle mit fehlenden Werten auszulesen? Betrachten Sie dazu folgendes Beispiel: https://webscraping-tures.github.io/table_na.html Auf den ersten Blick ist bereits ersichtlich, dass hier mehrere Zellen der Tabelle unbesetzt sind. Es fehlen also Werte. Versuchen wir die Tabelle trotzdem einmal einzulesen. table_na &lt;- &quot;https://webscraping-tures.github.io/table_na.html&quot; %&gt;% read_html %&gt;% html_node(css = &quot;table&quot;) table_na %&gt;% html_table() ## Error: Table has inconsistent number of columns. Do you want fill = TRUE? Wir bekommen eine hilfreiche Fehlermeldung die darüber informiert, dass die Anzahl der Spalten nicht über die gesamte Tabelle hinweg konstant ist. Netterweise wird uns auch direkt eine mögliche Lösung angeboten. Die Funktion html_table() kann mit dem Argument fill = TRUE dazu angewiesen werden, Zeilen mit abweichender Spaltenanzahl automatisch mit NA aufzufüllen. Dies steht für “Not Available” und repräsentiert fehlende Werte in R. wahlbet &lt;- table_na %&gt;% html_table(fill = TRUE) wahlbet %&gt;% head(n = 4) ## Bundesland Wahljahr Wahlbeteiligung ## 1 Baden-Württemberg 2016.0 NA ## 2 Bayern 2018.0 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg 61.3 NA Wie wir sehen, konnte html_table die vier Zellen mit fehlenden Werten mit NA füllen und die Tabelle trotz der Probleme einlesen. Allerdings, bestehen in dem HTML-Quellcode zwei unterschiedliche Arten von Problemen, welche die automatische Reparatur unterschiedlich gut handhaben kann. Betrachten wir dazu zunächst den Quellcode der ersten beiden Zeilen: &lt;tr&gt; &lt;td&gt;Baden-Württemberg&lt;/td&gt; &lt;td&gt;2016&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bayern&lt;/td&gt; &lt;td&gt;2018&lt;/td&gt; &lt;/tr&gt; Für Baden-Württemberg sehen wir, dass die dritte Spalte zwar im Quellcode angelegt ist, es sich aber kein Inhalt in dieser Zelle befindet. Dies hätte html_table() auch ohne fill = TRUE einlesen können und die Zelle mit einem NA gefüllt. Im Gegensatz dazu, fehlt für Bayern die Zelle komplett. Das bedeutet die zweite Zeile der Tabelle besteht nur aus zwei Spalten während der Rest der Tabelle drei Spalten hat. Dies ist das Problem, auf welches uns die Fehlermeldung hingewiesen hat. Im Ergebnis konnte R in beiden Fällen den richtigen Schluss ziehen und die Zelle in beiden Zeilen mit einem NA füllen. Betrachten wir aber auch die dritte und vierte Zeile im Quellcode: &lt;tr&gt; &lt;td&gt;Berlin&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;66.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Brandenburg&lt;/td&gt; &lt;td&gt;61.3&lt;/td&gt; &lt;/tr&gt; Hier fehlt in beiden Fällen die zweite Spalte. Im ersten Fall ist sie angelegt aber leer, im zweiten existiert sie nicht. Mit dem ersten Fall kann html_table() erneut problemlos umgehen. für Brandenburg stößt die Funktion aber an ihre Grenzen. Wir als menschliche Betrachter erkennen schnell, dass die letzte Landtagswahl in Brandenburg nicht im Jahr 61.3 stattfand, und dies also die Wahlbeteiligung sein muss. R kann dies nicht so einfach unterscheiden und nimmt 61.3 als Wert für die Spalte “Wahljahr” und setzt für “Wahlbeteiligung” ein NA ein. Was tun? Zunächst einmal, sollten wir uns darüber bewusst sein, dass solche Probleme existieren. Wenn html_table()diese Fehlermeldung ausgibt, sollten wir also nicht einfach fill = TRUE setzen, sondern versuchen herauszufinden, warum das Problem besteht und ob die Option es automatisch beheben zu lassen uns auch tatsächlich zum Ziel führt. Ist dies nicht der Fall, könnte ein Lösungsansatz sein, eine eigene “Extraktorfunktion” zu schreiben, welche die Probleme direkt während des Scrapings behebt. Dies ist aber eine eher fortgeschrittene Methode und außerhalb des in dieser Einführung machbaren. Wir können aber immerhin die entstandenen Probleme im Nachhinein korrigieren. Unser Problem liegt ausschließlich in Zeile Vier. Deren zweite Spalte muss in die dritte verschoben werden und die zweite dann selbst als NA gesetzt werden. Dazu benötigen wir erneut subsetting. Im Falle eines Data Frames, müssen wir dabei die Zeile und die Spalte in der Form df[zeile, spalte] angeben um eine Zelle auszuwählen. So können wir R sagen: “Schreibe in Zelle Drei den Inhalt von Zelle Zwei, und dann schreibe in Zelle Zwei NA” wahlbet[4, 3] &lt;- wahlbet[4, 2] wahlbet[4, 2] &lt;- NA wahlbet %&gt;% head(n = 4) ## Bundesland Wahljahr Wahlbeteiligung ## 1 Baden-Württemberg 2016 NA ## 2 Bayern 2018 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg NA 61.3 6.2 Dynamische Websites In der “Realität” des modernen Internets, werden wir immer häufiger auf Websites stoßen, die nicht mehr ausschließlich auf statischen HTML-Dateien basieren, sondern Inhalte dynamisch generieren. Dies kennen Sie beispielsweise in Form von Timelines in Social Media Angeboten die dynamisch auf Basis Ihres Nutzerprofils generiert werden. Andere Websites generieren die dargestellten Inhalte möglicherweise mit JavaScript Funktionen oder als Reaktion auf Eingaben in HTML-Formularen. In vielen dieser Fälle, reicht es aus Web Scraping Perspektive nicht mehr aus, eine HTML-Seite einzulesen und die gesuchten Daten zu extrahieren, da diese im HTML-Quellcode oft nicht enthalten sind, sondern dynamisch im Hintergrund geladen werden. Die gute Nachricht ist, dass es auch hier meist Möglichkeiten gibt, die Informationen trotzdem Scrapen zu können. Möglicherweise bietet der Betreiber einer Seite oder eines Services eine API (Application Programming Interface) an. In diesem Fall können wir uns für einen Zugang zu dieser Schnittstelle registrieren und erhalten dann Zugang zu den Daten von Interesse. Dies ist beispielsweise bei Twitter möglich. In anderen Fällen können wir eventuell in den eingebetteten Scripts identifizieren, wie und aus welcher Datenbank die Informationen geladen werden und diese direkt ansteuern. Oder wir nutzen den Selenium WebDriver um ein Browserfenster “fernzusteuern” und das zu Scrapen, was der Browser “sieht”. Die schlechte Nachricht ist, dass es sich bei all diesen Ansätzen um fortgeschrittene Methoden handelt, die den Umfang dieser Einführung übersteigen. In den Fällen, in denen auf Basis der Eingaben in ein Formular eine HTML-Datei dynamisch erzeugt wird, können wir diese aber mit den bereits bekannten Methoden auslesen. 6.2.1 HTML-Formulare und HTML-Queries Lassen Sie uns als Beispiel den OPAC Katalog der Universitätsbibliothek Potsdam https://opac.ub.uni-potsdam.de/ zunächst im Browser betrachten. Geben wir in das Suchfeld den Begriff “test” ein und Klicken auf Suche, zeigt uns das Browserfenster die Ergebnisse der Suchanfrage. Was uns hier aber eigentlich interessiert ist die Adressleiste des Browsers. Statt der URL “https://opac.ub.uni-potsdam.de/”, steht dort nun eine deutlich längere URL in der Form: “https://opac.ub.uni-potsdam.de/DB=1/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test”. Der erste Teil ist offensichtlich weiterhin die URL der aufgerufenen Website, nennen wir dies Base-URL. An das Ende der URL wurde aber der Teil “CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” angehangen. Dies ist der HTML-Query an dem wir hier interessiert sind. Zwischen Base-URL und Query befinden sich noch ein oder mehrere Bestandteile die sich in diesem Fall auch je nach ihrem Browser unterscheiden können. Diese sind für die eigentliche Suchanfrage aber auch irrelevant. Die verkürzte URL: “https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” führt zu dem selben Ergebnis. Ein Query ist eine Anfrage, in der Daten aus einem HTML-Formular an den Server gesendet werden. Dieser generiert als Reaktion eine neue Website, welche an den Nutzer zurückgesendet und im Browser dargestellt wird. Ausgelöst wurde die Anfrage hier durch den Klick auf den “Suchen” Button. Wenn wir verstehen, was die Bestandteile des Querys bewirken, könnten wir diesen manipulieren und gezielt Nutzen um uns eine Website von Interesse erstellen zu lassen und diese auszulesen. 6.2.2 HTML-Formulare Dazu müssen wir zunächst einen Blick in den HTML-Code des Suchformulars werfen. Um dies nachvollziehen zu können, sollten Sie sich den Quellcode der Seite anzeigen lassen und nach “&lt;form” suchen oder die WDTs nutzen um das Formular und seine Bestandteile zu betrachten. &lt;form action=&quot;CMD&quot; class=&quot;form&quot; name=&quot;SearchForm&quot; method=&quot;GET&quot;&gt; ... &lt;/form&gt; HTML-Formulare werden durch den &lt;form&gt; Tag umfasst. Innerhalb des Tags, können ein oder mehrere Formular-Elemente wie Texteingabefelder, Drop-Down Optionslisten, Buttons usw. platziert werden. &lt;form&gt; selbst trägt in diesem Beispiel eine Reihe von Attributen. Interessant für uns ist als erstes das Attribut method=\"GET\". Dies legt die Methode der Datenübermittlung zwischen Client und Server fest. Wichtig ist dabei, dass die Methode “GET” für die Übermittlung von Daten Queries in der URL nutzt und die Methode “POST” nicht. Wir können Queries somit nur manipulieren, wenn die “GET” Methode genutzt wird. Ist keine Methode im &lt;form&gt; Tag festgelegt, wird als Standard ebenfalls “GET” genutzt. Das zweite für uns interessante Attribut, ist action=\"CMD\". Dies legt fest, welche Aktion ausgelöst werden soll nachdem das Formular übermittelt wurde. Oft ist der Wert von action= der Name einer Datei auf dem Server, an welche die Daten gesendet werden sollen. In diesem Beispiel handelt es sich nicht um eine Datei. Stattdessen wir ein Befehl übermittelt, den der Server verstehen kann. Betrachten wir nun die Elemente des Formulars. Hierzu kann die rvest Funktion html_form() hilfreich sein. &quot;https://opac.ub.uni-potsdam.de/&quot; %&gt;% read_html() %&gt;% html_node(css = &quot;form&quot;) %&gt;% html_form() ## &lt;form&gt; &#39;SearchForm&#39; (GET CMD) ## &lt;select&gt; &#39;ACT&#39; [1/3] ## &lt;select&gt; &#39;IKT&#39; [0/13] ## &lt;select&gt; &#39;SRT&#39; [0/4] ## &lt;input checkbox&gt; &#39;FUZZY&#39;: Y ## &lt;input text&gt; &#39;TRM&#39;: ## &lt;input submit&gt; &#39;&#39;: Suchen Der Output zeigt uns in der ersten Zeile nochmals die Werte für method= und action=, sowie den Namen des Formulars. Die weiteren sechs Zeilen zeigen, dass das Formular aus drei &lt;select&gt; sowie drei &lt;input&gt; Elementen besteht, die Namen dieser Elemente, sowie der Standardwert der beim Übermitteln des Formulars gesendet wird, solange kein anderer Wert ausgewählt oder eingegeben wurde. Schauen wir uns einige dieser Elemente an. &lt;select&gt; Elemente sind Drop-Down Listen mit Optionen die ausgewählt werden können. Dies ist der Quellcode für das erste &lt;select&gt; Element unseres Beispiels: &lt;select name=&quot;ACT&quot;&gt; &lt;OPTION VALUE=&quot;SRCH&quot;&gt;suchen [oder] &lt;OPTION VALUE=&quot;SRCHA&quot; SELECTED&gt;suchen [und] &lt;OPTION value=&quot;BRWS&quot;&gt;Index bl&amp;auml;ttern &lt;/select&gt; Das Attribut name=\"ACT\" legt den Namen des Elements fest. Dieser wird bei der Übermittlung der Formulardaten über den Query genutzt. Über die ’Tags, werden die auswählbaren Optionen, also die Drop-Down Liste, definiert.&lt;value=\"\"&gt;steht dabei für den von Formular übermittelten Wert. Dem Nutzer angezeigt wird der auf den Tag folgende Text. Als Standardwert wird entweder der erste Wert der Liste genutzt, oder wie hier eine Option mit dem Attributselected` explizit als Standard festgelegt. Die drei weiteren Elemente sind &lt;input&gt; Tags. Eingabefelder, deren spezifischer Typ über das Attributtype=\"\" festgelegt wird. Dies können beispielsweise Textboxen (type=\"text\") oder Checkboxes (input=\"checkbox\") sein, es stehen aber weit mehr Optionen zur Verfügung. Eine umfassende Liste finden sie unter: https://www.w3schools.com/html/html_form_input_types.asp. Hier der Quellcode zu Zwei der Drei ’` Elemente auf der Beispielseite: &lt;input type=&quot;text&quot; name=&quot;TRM&quot; value=&quot;&quot; size=&quot;50&quot;&gt; &lt;input type=&quot;submit&quot; class=&quot;button&quot; value=&quot; Suchen &quot;&gt; Der erste Tag ist vom Typ “text”, also ein Textfeld, genau genommen das Textfeld in welches der Suchbegriff eingegeben wird. Neben dem Namen des Elements wird über value=\"\" ein Standardwert des Feldes festgelegt. In diesem Fall ist der Standardwert ein leeres Feld. Der zweite Tag ist vom Typ “submit”. Dies ist der Button “Suchen”, welcher durch einen Klick die Übermittlung der Formulardaten über den Query auslöst. 6.2.3 Der Query Doch was wird jetzt genau übermittelt? Betrachten wir nochmals den Beispiels-Query von weiter oben: CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test Der Wert des action=\"\" Attributes bildet den ersten Bestandteil des Queries und wird nach der URL angehangen. Der Wert des Attributs teilt dem Server mit, was dieser mit den weiteren übermittelten Daten anfangen soll. Darauf folgt ein “?”, welches die zu übermittelnden Daten als mehrere Paare von name=\"\" und value=\"\" Attributen der einzelnen Elemente einleitet. Die Paare sind mit &amp; verbunden. “ACT=SRCHA” steht also dafür, dass im Element mit dem Namen “ACT” die Option “SRCHA” ausgewählt wurde. Wofür die Werte der beiden weiteren &lt;select&gt; Elemente “IKT” und “SRT” stehen, können sie selber mit einem Blick in den Quellcode oder die WDTs nachvollziehen. Als Wert des &lt;input type=\"text\"&gt; mit dem Namen “TRM” wird der in das Feld eingegebene Text übermittelt. Hier “test”. Der Server empfängt so die Formulardaten, kann auf Basis des action=\"\" Attributes, hier “CMD”, nachvollziehen, wie die Daten verarbeitet werden sollen und konstruiert dementsprechend die Website die er an uns zurücksendet und die in unserem Browser dargestellt wird. 6.2.4 Manipulation des Queries und Scrapen des Ergebnis Da wir jetzt wissen, was die Bestandteile des Queries bedeuten, können wir diese auch gezielt manipulieren. Statt nun Queries per Hand zu schreiben, sollten wir R Nutzen um diese für uns zusammenzusetzen. Die Technik, URLs direkt im R-Code zu manipulieren, wird uns außerdem noch häufiger begegnen. Wir sollten sie also frühzeitig erlernen. Die Funktion str_c() setzt die als Argumente aufgelisteten Strings, also Folgen von Buchstaben, zu einem einzelnen String zusammen. Dabei können auch in anderen R Objekten gespeicherte Strings eingebunden werden. Haben wir das Ziel sowohl die Suchmethode als auch den Suchbgegriff zu manipulieren, könnten wir dies so erreichen: base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;test&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test&quot; Verändern wir nun die in den Objekten method und term gespeicherten Strings und generieren die komplette URL erneut, werden diese Bestandteile des Queries entsprechend manipuliert. method &lt;- &quot;SRCH&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCH&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; Die Suchmethode wurde auf den Wert “SRCH” festgelegt, also eine “ODER” Suche, der Suchbegriff auf “web scraping”. Dabei ist wichtig zu beachten, dass im Query keine Leerzeichen auftauchen dürfen und diese beim Absenden des Formulars durch “+” ersetzt werden. Statt “web scraping” müssen wir also den String “web+scraping” nutzen. Als Beispielanwendung, können wir nun den Server eine “UND” Suche zu dem Begriff “web scraping” durchführen lassen, die vom Server generierte HTML-Seite auslesen und die dargestellten 10 Titel extrahieren. base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; website &lt;- url %&gt;% read_html() Die Suchergebnisse werden in der generierten HTML-Datei als Tabellen dargestellt. Der &lt;table&gt; Tag hat dabei die Attribut-Wert Kombination summary=\"hitlist\", was wir für unseren CSS Selector nutzen können: hits &lt;- website %&gt;% html_node(css = &quot;table[summary=&#39;hitlist&#39;]&quot;) %&gt;% html_table() %&gt;% as_tibble() hits %&gt;% head(n=10) ## # A tibble: 10 x 4 ## X1 X2 X3 X4 ## &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 NA NA &quot;&quot; NA ## 2 NA 1 &quot;Introduction to Data Systems : Building from Python/ Bres… NA ## 3 NA NA &quot;&quot; NA ## 4 NA 2 &quot;An Introduction to Data Analysis in R : Hands-on Coding, … NA ## 5 NA NA &quot;&quot; NA ## 6 NA 3 &quot;Quantitative portfolio management : with applications in … NA ## 7 NA NA &quot;&quot; NA ## 8 NA 4 &quot;Automate the boring stuff with Python : practical program… NA ## 9 NA NA &quot;&quot; NA ## 10 NA 5 &quot;Spotify teardown : inside the black box of streaming musi… NA Dies hat zwar funktioniert, wir sehen aber , dass die Tabelle vor allem aus leeren Zeilen und Zellen besteht. Diese sind auf der Website unsichtbar, werden aber zur Formatierung der Darstellung genutzt. Statt die Tabelle nun nachträglich zu reparieren, macht es mehr Sinn, gezielt nur die Zellen zu extrahieren, in welchen die von uns gesuchte Information enthalten sind. Dies sind die &lt;td&gt; Tags mit class=\"hit\" und der Attribut-Wert Kombination align=\"left\". Auf dieser Basis können wir einen eindeutigen CSS Selector konstruieren. hits &lt;- website %&gt;% html_nodes(css = &quot;td.hit[align=&#39;left&#39;]&quot;) %&gt;% html_text(trim = TRUE) hits %&gt;% head(n = 5) ## [1] &quot;Introduction to Data Systems : Building from Python/ Bressoud, Thomas. - 1st ed. 2020. - Cham : Springer International Publishing, 2020&quot; ## [2] &quot;An Introduction to Data Analysis in R : Hands-on Coding, Data Mining, Visualization and Statistics from Scratch/ Zamora Saiz, Alfonso. - 1st ed. 2020. - Cham : Springer International Publishing, 2020&quot; ## [3] &quot;Quantitative portfolio management : with applications in Python/ Brugière, Pierre. - Cham : Springer, [2020]&quot; ## [4] &quot;Automate the boring stuff with Python : practical programming for total beginners/ Sweigart, Albert. - 2nd edition. - San Francisco : No Starch Press, [2020]&quot; ## [5] &quot;Spotify teardown : inside the black box of streaming music/ Eriksson, Maria. - Cambridge, Massachusetts : The MIT Press, [2019]&quot; 6.2.5 Weiterführende Quellen Um diese Informationen weiter zu verarbeiten und beispielsweise in Daten zu Autor, Titel, Jahr usw. zu trennen, sind erweiterte Kenntnisse im Umgang mit Strings notwendig, die leider über das in dieser Einführung abbildbare hinausgehen. Eine gute erste Übersicht finden sie im Kapitel “Strings” aus “R for Data Science” von Wickham und Grolemund: https://r4ds.had.co.nz/strings.html Dazu sei auch der passende “Cheat Sheet” empfohlen: https://raw.githubusercontent.com/rstudio/cheatsheets/master/strings.pdf "],["rvest3.html", "7 Scraping mehrseitiger Websites 7.1 Indexseiten 7.2 Pagination", " 7 Scraping mehrseitiger Websites 7.1 Indexseiten 7.2 Pagination "],["R3.html", "8 Transformation und Visualisierung im Tidyverse 8.1 Transformation mit dplyr 8.2 Visualisierung mit ggplot", " 8 Transformation und Visualisierung im Tidyverse 8.1 Transformation mit dplyr 8.2 Visualisierung mit ggplot "],["ethik.html", "9 Ethik &amp; good practice", " 9 Ethik &amp; good practice "],["projekt1.html", "10 Beispielprojekt 1", " 10 Beispielprojekt 1 "],["projekt2.html", "11 Beispielprojekt 2", " 11 Beispielprojekt 2 "]]
