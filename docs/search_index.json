[["index.html", "Web Scraping using R Introduction What is web scraping? Course contents Technical requirements Conventions Acknowledgements Colophon", " Web Scraping using R Jakob Tures 2021-06-10 Introduction This is an introduction to web scraping using R, designed to accompany a seminar held at the University of Potsdam in summer 2021. What is web scraping? Web scraping encompasses methods of data collection from online sources. This introduction will focus on scraping websites, but other forms on online data, e.g. databases or files, are scrapable as well. Instead of copying and pasting content from a browser window, in web scraping the process is automated through a programming language as R or Python. This is less error prone than manual collection and enables us to collect large amounts of data efficiently or regularly repeat the scraping process to update the data without investing much time. In web scraping your time is invested in planning the scraping process, programming your software and analysing the retrieved data – aka the fun stuff – rather than in time consuming and repetitive manual work. Web scraping enables us to access data, that we could not retrieve using traditional methods of data collection. Mainly because some types of data would not exist without the internet. Think of consumer reviews or social media posts. In other cases, web scraping enables us to collect data much more efficiently compared to traditional methods. Imagine you want to create a data file with demographic information in parliament members. This was obviously possible and done before web scraping became a possibility but included a lot of manual work using printed almanacs and typing the information into a table. Time consuming and error prone work. With web scraping we can set up scripts that gather this data directly from the website of a parliament including the possibility to rerun the collection process after the next election. But we don’t have to stop there. Maybe we want to analyse the sentiment of twitter messages written by parliament members by some socio-demographic variables as gender, age or educational background. We could retrieve both the tweets and the socio-demographic information using web scraping and analyse the gathered data in combination. At this point, your alarm bells should hopefully be ringing. Collecting data from social media profiles sounds like an Orwellian nightmare, and if it is done incorrectly, it is. We should not and will not collect data for the sake of data collection. Before scraping data on the internet we have to think about what data we actually need to answer our specific research question. Next we have to check if we are actually allowed to collect certain types of data and even if we are, if we should collect it. Do we need the names of people or users if we are collecting data? Do we actually need all socio-demographic variables available or can we keep it at a minimum, ensuring that individuals in our data remain unidentifiable? We are not web spiders, collecting every bit of data that is available, whether we need it or not. We are scientists and have to behave in a responsible and reasonable manner when collecting data. Course contents The first part of the course covers HTML as the conerstone of the (traditional) internet. We have to understand the structure of websites before we can start picking out their specific parts. Part II serves as an introduction to R, the programming language we will use for web scraping as well as analysis of the data gathered. This introduction can necessarily only cover the basics of R, as our focus lies on introducing the concepts of web scraping. The introduction to R will be problem-oriented, i.e.  we learn what we need for web scraping, but there will be a lot left out or only covered cursorily. See this as a starting point for your R journey, not its destination. Additional resources for deepening your R knowledge will be provided at later points. Part III forms the main part of the course and introduces basic and intermediate web scraping concepts. We will learn how to identify data of interest on HTML pages, select the data using CSS selectors and extract it using the R package rvest. We will also look at scraping tables, content generated by dynamic websites and scraping of multi-page websites. Part IV will cover the practical application of the acquired scraping knowledge to a small real world data analysis project. In preparation, we will take a closer look on web scraping ethics and good practice, already briefly discussed above, and what we can do to make sure we scrape responsibly. We will also deepen our R knowledge by looking on data cleaning, data transformation and graphical analysis in R using the tidyverse packages. The course will close with a sample project, spanning the process from generating a research question over finding appropriate data and scraping it to cleaning and analysing the data with the goal of answering the formulated question. The internet is an ever changing landscape. Where once every page we ecnountered was a simple HTML document, now many modern pages are less simplistic. Content is generated dynamically for each individual user and displayed information is not actually “present” in the HTML files but loaded through JavaScript or database connections. This also makes scraping harder. While still being possible, this requires advanced techniques that go beyond the scope of this introduction. You will still receive some pointers for where to look, should you want to follow these paths further. Also, when we begin scraping raw text from websites, we will need to learn about advanced handling of strings and regular expressions, whichcan also not be covered here. This introduction focusses on using rvest for scraping in R. This will suffice for many intermediate and advanced scraping projects, but alternatives exist within R and in other languages. While these also can not be covered here, you will gain a solid basis to pursue alternatives, should you so desire. Technical requirements All scraping in this introduction is done in R and RStudio using the tidyverse packages. Information on how to install the software and handle packages will be provided in chapters 2 and 3. We will also need a browser for scraping. While in principle any popular browser should do, I would recommend using Chrome or Chromium, as all examples using the build in Web Developer Tools, are done in Chromium. You should make sure, that the URL bar in your browser shows the full URLs. In Chrome/Chromium this is done by right-clicking the URL bar and choosing “Always show full URLs” in the context menu. While not strictly necessary, i would also advice using a decent text editor for opening HTML files. The options that come with your operating systems but why not do yourselves a favour and use something you can actually work with. Options are aplenty, but some of the more accessible while still powerful text editors I personally would recommend are Notepad++ on Windows (https://notepad-plus-plus.org/) and Atom on all operating systems (https://atom.io/). If your feel you are up to it, you can also dip your feet into Emacs, also available for every OS (https://www.gnu.org/software/emacs/), but note that this is an advanced option. Proceed at your own risk. Conventions All code on the website is set in this code font. This is true for R as well as HTML code. R code that is included in text paragraphs will not always be runnable and often serve illustrative purposes. All R code written in code blocks is runnable and look like this: print(&quot;Hello World!&quot;) ## [1] &quot;Hello World!&quot; If an R command produces relevant output, you will see the output following directly after the command, written in red and introduced by ##. You should in general try to run the code blocks yourself in RStudio. The only way to learn what is happening in R and web scraping, is doing it yourself. Run the examples and experiment with them. Every problem you will run into while changing the code is a learning opportunity. Notice, that code blocks are copyable. Mouse over the code block and you will see a copy symbol, or simply copy &amp; paste like you are used to. But, I would strongly advice on writing as much code as you can yourself. While copying code examples by hand may not be intellectually challenging, but if your aim is to learn the R functions and syntax you have to write a lot of it before it can become second nature. Acknowledgements I would like to thank the authors of bookdown the tidyverse and all R packages used in the creation of this course on web scraping. Very special thanks go to Lukas Höttges for his support in creating the website and in organizing the seminar at the University of Potsdam. To Hannah Gehrmann for the invaluable feedback on an early draft. To Fabian Class for technical support and numerous answers to my many questions. And to Sophia Varma for translating the original German draft and the great support. Colophon The website was built with: sessioninfo::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.4 (2021-02-15) ## os Ubuntu 20.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Europe/Berlin ## date 2021-06-10 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.0.4) ## backports 1.2.1 2020-12-09 [1] CRAN (R 4.0.4) ## bookdown 0.21 2020-10-13 [1] CRAN (R 4.0.4) ## broom 0.7.4 2021-01-29 [1] CRAN (R 4.0.4) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.0.4) ## cli 2.3.0 2021-01-31 [1] CRAN (R 4.0.4) ## colorspace 2.0-0 2020-11-11 [1] CRAN (R 4.0.4) ## crayon 1.4.1 2021-02-08 [1] CRAN (R 4.0.4) ## DBI 1.1.1 2021-01-15 [1] CRAN (R 4.0.4) ## dbplyr 2.1.0 2021-02-03 [1] CRAN (R 4.0.4) ## digest 0.6.27 2020-10-24 [1] CRAN (R 4.0.4) ## dplyr * 1.0.4 2021-02-02 [1] CRAN (R 4.0.4) ## echarts4r * 0.4.0 2021-03-07 [1] CRAN (R 4.0.4) ## ellipsis 0.3.1 2020-05-15 [1] CRAN (R 4.0.4) ## evaluate 0.14 2019-05-28 [1] CRAN (R 4.0.4) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.0.4) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.0.4) ## fs 1.5.0 2020-07-31 [1] CRAN (R 4.0.4) ## generics 0.1.0 2020-10-31 [1] CRAN (R 4.0.4) ## ggplot2 * 3.3.3 2020-12-30 [1] CRAN (R 4.0.4) ## glue 1.4.2 2020-08-27 [1] CRAN (R 4.0.4) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.0.4) ## haven 2.3.1 2020-06-01 [1] CRAN (R 4.0.4) ## hms 1.0.0 2021-01-13 [1] CRAN (R 4.0.4) ## htmltools 0.5.1.1 2021-01-22 [1] CRAN (R 4.0.4) ## htmlwidgets 1.5.3 2020-12-10 [1] CRAN (R 4.0.4) ## httpuv 1.5.5 2021-01-13 [1] CRAN (R 4.0.4) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.0.4) ## jsonlite 1.7.2 2020-12-09 [1] CRAN (R 4.0.4) ## knitr * 1.31 2021-01-27 [1] CRAN (R 4.0.4) ## later 1.1.0.1 2020-06-05 [1] CRAN (R 4.0.4) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.4) ## lubridate * 1.7.9.2 2020-11-13 [1] CRAN (R 4.0.4) ## magrittr 2.0.1 2020-11-17 [1] CRAN (R 4.0.4) ## mime 0.10 2021-02-13 [1] CRAN (R 4.0.4) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.0.4) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.0.4) ## pillar 1.4.7 2020-11-20 [1] CRAN (R 4.0.4) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.0.4) ## promises 1.2.0.1 2021-02-11 [1] CRAN (R 4.0.4) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.0.4) ## R6 2.5.0 2020-10-28 [1] CRAN (R 4.0.4) ## Rcpp 1.0.6 2021-01-15 [1] CRAN (R 4.0.4) ## readr * 1.4.0 2020-10-05 [1] CRAN (R 4.0.4) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 4.0.4) ## reprex 1.0.0 2021-01-27 [1] CRAN (R 4.0.4) ## rlang 0.4.10 2020-12-30 [1] CRAN (R 4.0.4) ## rmarkdown 2.6 2020-12-14 [1] CRAN (R 4.0.4) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.0.4) ## rvest * 0.3.6 2020-07-25 [1] CRAN (R 4.0.4) ## scales 1.1.1 2020-05-11 [1] CRAN (R 4.0.4) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 4.0.4) ## shiny 1.6.0 2021-01-25 [1] CRAN (R 4.0.4) ## stringi 1.5.3 2020-09-09 [1] CRAN (R 4.0.4) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.0.4) ## tibble * 3.0.6 2021-01-29 [1] CRAN (R 4.0.4) ## tidyr * 1.1.2 2020-08-27 [1] CRAN (R 4.0.4) ## tidyselect 1.1.0 2020-05-11 [1] CRAN (R 4.0.4) ## tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 4.0.4) ## vctrs 0.3.6 2020-12-17 [1] CRAN (R 4.0.4) ## withr 2.4.1 2021-01-26 [1] CRAN (R 4.0.4) ## xfun 0.21 2021-02-10 [1] CRAN (R 4.0.4) ## xml2 * 1.3.2 2020-04-23 [1] CRAN (R 4.0.4) ## xtable 1.8-4 2019-04-21 [1] CRAN (R 4.0.4) ## yaml 2.2.1 2020-02-01 [1] CRAN (R 4.0.4) ## ## [1] /home/jakobtures/R/x86_64-pc-linux-gnu-library/4.0 ## [2] /usr/local/lib/R/site-library ## [3] /usr/lib/R/site-library ## [4] /usr/lib/R/library "],["html.html", "1 HTML as a cornerstone of the internet 1.1 HTML-Tags 1.2 Attributes 1.3 Entities", " 1 HTML as a cornerstone of the internet What happens when we call up a URL such as https://webscraping-tures.github.io in a browser? We will get a visualisation of the page in our browser window. From the perspective of the user of a website, this is everything we need to know. Our goal – calling up the website – has already been reached at this point. From the perspective of a web scraper, we need to understand however, what is happening behind the scenes. The link https://webscraping-tures.github.io does nothing but call up an HTML-file in which the content of the website is recorded in form of a specific code. This code is then interpreted by your browser and translated into a visual representation, which you are finding yourselves in front of now. Try it yourself. With a right-click into this area of the text and a further click on “View Page Source”, the HTML-code that the website is based on, is shown. At this point it is completely legitimate to be overwhelmed by the flood of unfamiliar symbols and terminology. Who would have thought that a relatively simple website such as this one, can be so complex and complicated on the backend? But the good news is that we do not need to be able to understand every word and every symbol in an HTML file. Our goal is identifying the parts of a website that are relevant for our data collection and extract them precisely from the HTML-code. This can possibly be only a single line of code in an HTML-file with thousands of lines of code. Do we need to understand every single line? No, but we have to be able to understand the structure of an HTML-file to be able to identify that one line that is of interest to us. Until we reach this point, we still have a ways to go. At the end of this first section, you will have a basic understanding of the structure and components of an HTML document and the source code will already seem way less intimidating. 1.1 HTML-Tags The “language” that HTML-files have been written in, is the Hypertext Markup Language, HTML for short. The basics of this language are the so-called Tags or the “vocabulary”. These terms are used to structure the HTML-document, format it, insert links and images or create lists and tables. The browser knows the meaning of these key phrases, can interpret them and present the website visually according to the coded HTML-Tags. As with any language in the IT-world, HTML also follows certain rules or “grammar” or Syntax. Fortunately for us, in the case of HTML this syntax is rather simple. In the following we will take a closer look at the tags and syntax rules that are important to us. Contemplate the following example: &lt;b&gt;Hello World!&lt;/b&gt; &lt;b&gt; is a tag. The b stands for bold. Tags always follow the same pattern. They begin with a &lt;, followed by the name of the tag – b – and end with a &gt;. It is important to note that an opened tag will need to be closed as well, under normal circumstances. To do this, the same tag is written again with a forward slash, &lt;/b&gt; in our case. Everything that is contained within the opening and closing tag in an HTML-document will be interpreted according to the meaning of the tag. With this knowledge we understand what will happen with our example. The tag &lt;b&gt; means bold, and the opening and closing tag &lt;b&gt;…&lt;/b&gt; include the text Hello World!. The text will be interpreted according to the tag, which means in bold: Hello World! 1.1.1 hello_world.html Let us have a look at a full HTML-document now: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; The interpretation of this document by the browser can be seen under https://webscraping-tures.github.io/hello_world.html. Let us take a look at the single elements of the code: The first line, the document type declaration, &lt;!DOCTYPE html&gt;, informs the browser, which version of HTML is being used in the document. &lt;!DOCTYPE html&gt; stands for the current HTML5 standard. This is the place to declare if an older version of HTML was used, to ensure the most accurate visual representation – in spite of possible changes or the omission of certain standards in the current version. From the point of view of a web-scraper the HTML-version does not necessarily play a massive role. Interestingly enough, the tag &lt;!DOCTYPE html&gt; is one of the few exceptions from the rule that a once-opened tag has to be closed again; it is not necessary here. The actual content of the HTML-document only starts in line 3 with the &lt;html&gt; tag. This will only be closed in the last line and thus contains the total contents of the document. So the tag tells the browser that all it encompasses, is HTML. Next is the tag &lt;head&gt;. What we see the tag encompassing here, is not what we see in the browser window yet. What this means in practice is that here we mainly find meta-information, advanced functionality (JavaScript) and definitions of design choices (Cascading Style Sheets – CSS). This should not distract us too much in this introduction into web scraping, but we should be aware that references to .js and .css files can appear in the &lt;head&gt; tag. In our example the &lt;head&gt; tag exclusively contains another tag called &lt;title&gt;, which in turn contains the text Hello World!. The &lt;title&gt; tag determines what we will see in the title bar of our browser. In this case Hello World! Finally, everything that the &lt;body&gt; tag includes, describes the content we can see in the browser window. In this simple example, only one line is included. The already known &lt;b&gt; tag includes the text Hello World!, which it displays in bold for us to see in the browser window. So now you already know the basic structure of any HTML file. While looking at the sample code above, you may have noticed that certain lines are indented to the right. This is not a requirement for functional HTML code, but a convention that makes it easier for reading and understanding. Indented lines represent the different hierarchical levels of the code. &lt;head&gt; is hierarchically subordinate to &lt;html&gt; and therefore single indented. &lt;title&gt;, in turn, is subordinate to &lt;head&gt; and therefore doubly indented. By writing it this way, it is also obvious at a first glance that &lt;body&gt; is subordinate to &lt;html&gt; but not to &lt;title&gt;, since &lt;body&gt; and &lt;title&gt; are each only single indented. You will often – but not always – encounter this convention in “real” HTML documents on the Internet. One more note on the technical side: HTML documents can basically be written by hand in any editor and must be saved with the extension .html. You can test this by starting any text editor yourself, copying the HTML code above, saving the file with the extension .html and opening it in a browser of your choice. However, due to their complexity, websites are not usually written by hand nowadays. A variety of professional tools now offer much more efficient ways to design websites. For example, the page you are looking at was written directly in RStudio using the “bookdown” package, which automates most of the layout decisions. 1.1.2 Important tags We cannot look at all the tags available in HTML at this point, but will initially limit ourselves to those that we encounter very frequently and that will be particularly relevant for our first web scraping projects. 1.1.2.1 Page structure One tag that relates to the structure of the page, we have already met above. The &lt;body&gt; tag communicates that everything encompassed by it is part of the content displayed in the browser window. One way to further structure the content is to use the maximum of six levels of headings that HTML offers. The tags &lt;h1&gt; &lt;h2&gt; ... &lt;h6&gt; allow this in a simple way. The h stands for header. The text encompassed by the tag is automatically numbered and displayed in different font sizes depending on the level of the heading. As an example, you can see the structure of the headings on this page as HTML code: &lt;h1&gt;HTML as a cornerstone of the internet&lt;/h1&gt; &lt;h2&gt;HTML-Tags&lt;/h2&gt; &lt;h3&gt;hello_world.html&lt;/h3&gt; &lt;h3&gt;Important tags&lt;/h3&gt; &lt;h4&gt;Page structure&lt;/h4&gt; &lt;h4&gt;Formatting&lt;/h4&gt; &lt;h4&gt;Lists&lt;/h4&gt; &lt;h4&gt;Tables&lt;/h4&gt; &lt;h2&gt;Attributes&lt;/h3&gt; &lt;h3&gt;Links&lt;/h4&gt; &lt;h3&gt;Images&lt;/h4&gt; &lt;h2&gt;Entities&lt;/h3&gt; Another frequently occurring form of structuring in HTML documents, are the groupings defined via &lt;div&gt; (division) and &lt;span&gt;. Both tags basically work the same way, with &lt;div&gt; referring to one or more lines and &lt;span&gt; referring to one line or part of a line. Neither have any direct effect on the display of the website at first, but they are often applied in combination with classes that are defined in Cascading Style Sheets – CSS, to adjust the visual interpretation. Normally, we do not care about how the CSS classes are defined and how they affect rendering. You will learn later in this seminar why the combination of &lt;div&gt; or &lt;span&gt; and CSS classes are often a very practical starting point for our web scraping endeavours and how we can exploit this in our work. Here is a simplified example of how both tags can appear in HTML code: &lt;div&gt; This sentence is part of the div tag. This sentence is part of the div tag, &lt;span&gt; while this sentence is part of the div and the span Tags. &lt;/span&gt; &lt;/div&gt; This sentence is not part of the div tag. 1.1.2.2 Formatting HTML provides a variety of tags for formatting the displayed text. In the following we will look at some of the most common ones. The &lt;p&gt; tag defines the enclosed text as a paragraph and is accordingly automatically terminated in the display with a line break. &lt;p&gt;This sentence is part of the paragraph. So is this. And this one.&lt;/p&gt; This sentence is not part of the paragraph. This is represented as: This sentence is part of the paragraph. So is this. And this one. This sentence is not part of the paragraph. The tag &lt;br&gt; introduces a line break. This tag is another exception to the rule that an opened tag must also be closed again. In this special case, using opening and closing tags &lt;br&gt;&lt;/br&gt; stands for two line breaks, so it is not equivalent to &lt;br&gt;. Unlike the line break inserted by &lt;p&gt;...&lt;/p&gt; at the end of the paragraph, no further spacing is inserted after &lt;br&gt;: Here comes some text, which is now broken up in two lines.&lt;br&gt; After the break tag, in contrast to the paragraph tag, no line spacing is inserted.&lt;br&gt; If the break tag is also explicitly closed again, two line breaks are inserted.&lt;br&gt;&lt;/br&gt; As can be seen here. This is represented as: Here comes some text, which is now broken up in two lines. After the break tag, in contrast to the paragraph tag, no line spacing is inserted. If the break tag is also explicitly closed again, two line breaks are inserted. As can be seen here. The typeface can be adjusted by tags like the already known &lt;b&gt; (bold) or &lt;i&gt; (italics) similar to the known options in common text editing programs: These tags can be used to render words, sentences and paragraphs &lt;b&gt;bold&lt;/b&gt; or &lt;i&gt;italic&lt;/i&gt;. This is represented as: These tags can be used to render words, sentences and paragraphs bold or italic. 1.1.2.3 Lists We will often encounter lists in HTML documents. The two most common variants being the unordered list, introduced by &lt;ul&gt;, and the ordered list, &lt;ol&gt;. The opening and closing list-tag covers the entire list, while each individual list element is enclosed by a &lt;li&gt; tag in both variants. Here are two short examples: &lt;ul&gt; &lt;li&gt;First unordered list element&lt;/li&gt; &lt;li&gt;Second unordered list element&lt;/li&gt; &lt;li&gt;Third unordered list element&lt;/li&gt; &lt;/ul&gt; This is represented as: First unordered list element Second unordered list element Third unordered list element &lt;ol&gt; &lt;li&gt;First ordered list element&lt;/li&gt; &lt;li&gt;Second ordered list element&lt;/li&gt; &lt;li&gt;Third ordered list element&lt;/li&gt; &lt;/ol&gt; This is represented as: First ordered list element Second ordered list element Third ordered list element 1.1.2.4 Tables HTML can also be used to display tables, without further adjustments of the display via CSS admittedly not very attractive tables. These are opened by a &lt;table&gt; tag and closed accordingly. Within the table, lines are defined by &lt;tr&gt;...&lt;/tr&gt; (table row). Within the line, table headers can be defined by &lt;th&gt; (table header) and cell contents by &lt;td&gt; (table data). Content encompassed by &lt;th&gt;...&lt;/th&gt; and &lt;td&gt;...&lt;/td&gt; are not only formatted differently in their presentation, from the web scraper’s point of view, these tags also allow us to clearly distinguish the table content to be read. Here is a simple example: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; This is displayed as: # Tag Effect 1 “b” bold 2 “i” italics Formatting the HTML code in a kind of “table form” as in the example above is not necessary but increases intuitive readability. In fact, the following manner of writing it, is equivalent in result and actually more common in practice: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; This will also be displayed as: # Tag Effect 1 “b” bold 1 “i” italics 1.2 Attributes Many HTML tags can be further adapted in their functionality and presentation by using so-called attributes. The basic syntax is: &lt;tag attribute=\"value\"&gt;...&lt;/tag&gt;. In the opening (never in the closing) tag, the name of the tag is followed by the name of the attribute = the value to be assigned, enclosed in single or double inverted commas. In HTML, &lt;tag attribute=\"value\"&gt; is not equal to &lt;tag attribute = \"value\"&gt;. The pair of attribute name and value must be connected with a = without spaces in order to be interpreted correctly. A variety of tags can be modified with a multitude of attributes. Two of the most common and illustrative applications are the inclusion of links and images, two other frequently encountered HTML tags. 1.2.1 Links Links are included using the &lt;a&gt; (anchor) tag. The first intuitive attempt &lt;a&gt;This is a link&lt;/a&gt; is unfortunately unsuccessful: This is (not) a link Although the text is displayed and marked as a link – i.e. blue and underlined – it does not lead to any destination, since it was not defined in the HTML document what this destination should be. This is where the first attribute comes into play. With &lt;a href=\"url\"&gt; the target of the link is defined. href stands for hypertext reference and its assigned value can be, among other things, a website, an email address or even a file. For example, &lt;a href=\"webscraping-tures.github.io/html.html\"&gt;This is a link&lt;/a&gt; links to the page you are viewing. This is a link You may have noticed that you had to scroll to this point again to continue reading. A second attribute can remedy this. With &lt;a href=\"webscraping-tures.github.io/html.html\" target=\"_blank\"&gt;This is a link&lt;/a&gt; we instruct the browser to open the link in a new tab. The assigned value of target here is \"_blank\", which stands for a new tab, but it can also take on a number of other values. This is a link Links are of particular interest to us in web scraping when we collect links to all sub-pages from a parent page in order to scrape them specifically. But more about that later. One more note: the &lt;link&gt; tag is not to be confused with &lt;a&gt; and is used to integrate external files, such as the JavaScript or CSS files already mentioned. 1.2.2 Images Images and graphics are integrated in HTML with &lt;img&gt;, another tag that does not have to be explicitly closed. So that the browser knows which image is to be included, this is specified via the src (source) attribute of the tag. Thus &lt;img src=\"webscraping-tures.github.io/Rlogo.png\"&gt; includes the following image: Using further attributes, it is also possible, for example, to adjust the size of the image in pixels. &lt;img src=\"webscraping-tures.github.io/Rlogo.png\" width=\"100\" height=\"100\"&gt; leads to a resized display of the image. Images can also be combined with links. So &lt;a href=\"https://www.r-project.org/\" target=\"_blank\"&gt;&lt;img src=\"webscraping-tures.github.io/Rlogo.png\"&gt;&lt;/a&gt; defines the image as a link, where a click on the image takes you to the specified link: 1.3 Entities A number of characters are reserved for the HTML code. We have already seen that the characters &lt; &gt; \" are part of the code to define tags and values of attributes. In many cases, more current HTML versions allow for the usage of reserved characters directly in continuous text. For the time being, however, we will regularly encounter so-called entities instead of the actual characters in web scraping. Entities are coded representations of certain characters. They are always introduced with &amp; and ended with ;. Between the two characters is either the name or the number of the entity. For example, &amp;lt; stands for less than, i.e. &lt; and &amp;gt; for greater than, i.e. &gt;. A text with reserved characters like &amp;lt; und &amp;gt; or the so-called &amp;quot;ampersand&amp;quot; &amp;amp;. Is displayed as: A text with reserved characters like &lt; und &gt; or the so-called “ampersand” &amp;. Sidenote: If you are interested in the origin of the term ampersand, I recommend its Wikipedia article, which makes for an interesting read: https://en.wikipedia.org/wiki/Ampersand Another entity we will encounter regularly is &amp;nbsp; (non-breaking space), which can be used instead of a simple space. The advantage of this is that there is never a line break in the browser, and it allows the use of more than one space: Displayed with one space Displayed with four&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;spaces Displayed with one space Displayed with four    spaces An overview of the most common entities can be found here: https://www.w3schools.com/html/html_entities.asp The entities of all Unicode characters can be found here: https://unicode-table.com/ "],["R1.html", "2 R &amp; R Studio 2.1 Installing R 2.2 RStudio 2.3 Hello World! 2.4 Objects 2.5 Vectors 2.6 Functions 2.7 If/Else statements", " 2 R &amp; R Studio 2.1 Installing R R is a freely available programming language used predominantly for data science and statistical computations. For more information on the language and to access the documentation, visit: https://www.r-project.org/. From there you can also follow the link to CRAN, the Comprehensive R Archive Network, or access it directly by visiting https://cran.r-project.org/. The latest versions of R will always be hosted at CRAN. At the top of the landing page, you will find links to the installers for each operating system. If you are using Windows, please choose “base” after following the link and then download the offered file. In the case of Mac OS download the first file listed under “Latest release”. In both cases, execute the file and install R to a directory of your choice. If you are using Linux, the link on CRAN will offer installation advice for some of the more popular distributions. In any case, you can check the package manager of your choice for the latest available release for your system. All examples used on this Website were written and tested with R version 4.0.4 “Lost Library Book”. While it is not to be expected, they might, nonetheless, return errors in newer versions of R. 2.2 RStudio The basic R installation provides a simple GUI – Graphical User Interface – that could in principle be used to follow the contents of this introduction to Web Scraping. The widely more common approach is to use an external IDE – Integrated Development Environment –, the most popular being RStudio. Using an IDE will dramatically improve your workflow and I would strongly recommend using RStudio for this purpose. RStudio is also freely available and can be found at https://www.rstudio.com/. Following the “Download” link and scrolling down (ignoring the different versions offered for professional usage) you will find the latest installers for several operating systems offered as downloads. In most cases, simply installing RStudio after R has been installed, will work “out of the box”. 2.2.1 Overview The RStudio interface consists of four sub-areas. The bottom-left shows the “Console” as well as additional tabs which you will rarely need in the beginning. The console can be used to evaluate R code live. We will begin working with the console soon, so this will make more sense to you in a bit. The top-left shows opened files, e.g. R scripts. This is where you will actually spend most of your time. This introduction proceeds from using one-time commands in the console to writing your code in scripts that can be re-opened, re-run and shared. The top-right has several tabs of which “Environment” should be our main concern at this point. Here you will see all data objects created in your RStudio session. More on this later. Finally, the bottom-right shows us, amongst other things, the “Files” in a selected folder, graphical output under “Plots” and requested “Help” on packages and functions. 2.3 Hello World! So, let’s begin with putting R &amp; RStudio to use. For now, we will write our commands directly into the console. You will notice a &gt; sign at the beginning of the last line in the console. This is a prompt, as in “write commands here”. Try writing this and executing the command with the “Enter” key: print(&quot;Hello World!&quot;) ## [1] &quot;Hello World!&quot; You just entered your first R command, received your first output and also used your first function. We will address functions in more detail later. For now, it is enough to know that the command print() prints everything that is enclosed in its parentheses to the output. The output begins with [1], indicating that this is the first, and in this case the only, element of the output generated by the executed command. Please note, that RStudio will not print ## before the output. In the shown code segments on this website, ## is inserted before the output to allow copying the code directly to RStudio, as one or mutliple # indicate that a line is a comment, and thus is not evaluated as a command by R. 2.3.1 Calculating with R R understands the basic arithmetic symbols + - * / and thus the console can be used as a calculator. Many functions for more involved calculations, e.g.  sqrt() for taking a square root of the content enclosed in the parenthesis, are available. x^y can be used to write x to the power of y. For now, you should write the code below line for line into the R console and execute each line with the “Enter” key. 17 + 25 ## [1] 42 99 - 57 ## [1] 42 4 * 10.5 ## [1] 42 84 / 2 ## [1] 42 sqrt(1764) ## [1] 42 6.480741 ^ 2 ## [1] 42 2.3.2 Comparison operators We can use comparison operators to compare two values and receive the test result as output. To test if two values are equal, we write ==. To test if they are not equal, we can use != 42 == 42 ## [1] TRUE 42 != 42 ## [1] FALSE We can also compare if the first value is less &lt;, less or equal &lt;=, larger &gt; or larger or equal &gt;=, compared to the second value. 10 &lt; 42 ## [1] TRUE 42 &lt;= 42 ## [1] TRUE 10 &gt; 42 ## [1] FALSE 90 &gt;= 42 ## [1] TRUE 2.4 Objects Some of the power of using a language like R for computation, comes from the ability to store data or results for later use and further analysis. In R, all types of data are stored in objects. On a basic level, an object is a name that we define that has some form of data assigned to it. To assign data to a name, we use the assignment operator &lt;-. the_answer &lt;- 42 A handy keyboard shortcut for writing the assignment operator is pressing the “Alt” and “-” keys simultaneously. Learning this shortcut early, will safe you on a lot of typing and keyboard gymnastics. After we assigned a value to an object, we can recall that value, by writing the object’s name. the_answer ## [1] 42 We can also use defined objects in calculations and function calls (more on those later). Note, that if we assign a value to an already defined object, the stored value is overwritten by the new one. the_answer &lt;- the_answer / 2 the_answer ## [1] 21 a &lt;- 17 b &lt;- 4 the_answer &lt;- (a + b) * 2 the_answer ## [1] 42 All objects we define are listed in the “Environment” tab, seen in the upper right of RStudio. If we ever want to remove objects from the environment, we can use the rm() function. In general, this is not necessary, but it can help with keeping the list from getting cluttered. rm(the_answer) 2.5 Vectors When we assigned a number to an object, we actually created a vector. A vector is a one-dimensional data structure that can contain multiple elements. The number of elements determine the length of the vector. So a vector with only one element is still a vector, but with a length of 1. To assign multiple elements to a vector, we use the combine function c(). All values inside the parentheses, separated by ,, are combined as elements to form the vector. v &lt;- c(7, 8, 9) v ## [1] 7 8 9 2.5.1 Subsetting If we want to access certain elements of a vector, we have to use subsetting. This is achieved by adding square brackets to the object’s name, containing the position of the element in its vector. In order to access the first or third element, we can write: v[1] ## [1] 7 v[3] ## [1] 9 We can also access multiple elements at once, using c() inside the brackets or by defining a range of positions using :. v[c(1, 3)] ## [1] 7 9 v[2:3] ## [1] 8 9 2.5.2 Types of vectors Observing the vector v we created in the environment, we notice that RStudio writes num [1:3] before listing the values of the elements. The second part, indicates the length of 3, while the first part shows the type of the vector we created. In this case the type is numeric. Numeric vectors, as you might have guessed, contain numbers. We can also use str() to receive info on type, length and content of a vector. str(v) ## num [1:3] 7 8 9 There are a number of other types of vectors, the two most important – besides numeric vectors – being logical and character vectors. Logical vectors can only contain the values TRUE and FALSE. Strictly speaking, they – as the other types of vectors – can also contain NA, indicating a missing value. We will talk more about NAs later on. Logical vectors are often created when we test for something. For example, we can test, if the elements in a numerical vector are larger or equal to 5 and receive a logical vector containing the test results. x &lt;- c(1, 7, 3, 5) x &gt;= 5 ## [1] FALSE TRUE FALSE TRUE Character vectors contain strings of characters. When assigning strings, they have to be enclosed in quotation marks. char_v &lt;- c(&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;character&quot;, &quot;vector!&quot;) We can compare character vectors only for (non-)equality, not for being smaller or larger. &quot;same&quot; == &quot;same&quot; ## [1] TRUE &quot;same&quot; == &quot;not the same&quot; ## [1] FALSE &quot;same&quot; != &quot;not the same&quot; ## [1] TRUE Character vectors also cannot be used to calculate. This can get problematic, if numbers are stored as characters, which arises frequently when Web Scraping. a &lt;- c(1, 2, 3) b &lt;- c(&quot;7&quot;, &quot;8&quot;, &quot;9&quot;) str(a) ## num [1:3] 1 2 3 str(b) ## chr [1:3] &quot;7&quot; &quot;8&quot; &quot;9&quot; a + b ## Error in a + b: non-numeric argument to binary operator As we enclosed the elements of vector b in quotation marks, R interprets the data as characters instead of numbers. Since characters cannot be used for calculations, we received an error message. But we can make R interpret the characters as numbers by using as.numeric(). a + as.numeric(b) ## [1] 8 10 12 2.5.3 A brief look at lists Note that a vector of a certain type, can only contain elements of that type. So we cannot mix data types in the same vector. If we want to mix data types, we can use lists instead of vectors. l &lt;-list(1, TRUE, &quot;Hello World!&quot;) str(l) ## List of 3 ## $ : num 1 ## $ : logi TRUE ## $ : chr &quot;Hello World!&quot; Lists can also contain other lists to represent hierarchical data structures. We will see lists “in action” later on in this course. 2.6 Functions Functions provide an easy and concise way of performing more or less complex tasks using predefined bits of R code that are provided in “base R” – i.e. that come with the basic R installation – or in the various additional packages that are available for installation. We have already used a number of functions up to this point, e.g. print(). To “call” a function, we write its name, followed by parentheses. Inside the parentheses additional arguments are provided to R. In most cases, some data has to be entered as the first argument. For example, print() writes the text provided as argument to the output. More complex functions often allow for more than one argument. Sometimes these are required, but more often these additional arguments are optional and can be used to change some options from the default value to the one desired. 2.6.1 Help But how do we know which arguments can or have to be provide to use a function and what their effects are? We can check the documentation on CRAN or use Google to find additional information. Another often more convenient way, is to use the help functionality build into R. By writing ? in front of the function name into the console and executing the line by pressing “Enter”, the help file is opened in the lower right of the RStudio window. Let’s try this for the function rnorm(). ?rnorm() The help file tells us several things. rnorm() is part of a family of functions that are related to the normal distribution, each providing a distinct functionality. The functionality of rnorm() being the generation of random numbers stemming from the normal distribution. We also learn, that three arguments can be provided. n, the number of observations to be generated, as well as mean and sd, the mean and the standard deviation of the normal distribution to be drawn from. We also see that mean and sd are provided with the standard values 0 and 1 respectively, indicated by the =. We also see that n has no standard value. So we have to provide a value for n, but not for mean and sd. Just writing rnorm() will result in an error. rnorm() ## Error in rnorm(): argument &quot;n&quot; is missing, with no default To provide an argument to a function, we write the name of the argument, followed by = and the value to be provided. Note that, since rnorm() draws random numbers, your output will differ from the output presented here. rnorm(n = 10) ## [1] 0.9536784 -1.7869952 0.9081572 0.6752944 -0.5935582 -1.5253821 ## [7] -1.0646667 -1.0885872 -1.0620062 -0.4097539 In the same vein, additional arguments that are allowed by the function can be defined, instead of using their default values. rnorm(n = 10, mean = 10, sd = 0.5) ## [1] 9.437122 9.684778 9.891459 9.722530 10.142810 10.208368 10.453979 ## [8] 9.785268 10.114224 9.355369 We can also skip writing the names of arguments in many cases. As the n argument is the first listed in the function’s parentheses, R also understands the call, if we just provide the value to be used as the first argument. You will often encounter the convention that the first argument is written without its name and any further arguments are written in full. rnorm(10, mean = 10, sd = 0.5) ## [1] 9.215647 9.234822 9.917779 9.947024 10.380677 9.812325 9.023497 ## [8] 10.203344 10.453516 9.694109 2.6.2 Examples: Basic statistical functions Base R provides us with some basic statistical functions that are used for data analysis. We should start with defining a numerical vector that contains some data to be analysed. data &lt;- c(4, 8, 15, 16, 23, 42) We could be interested in describing this data by its arithmetic mean, median and standard deviation. For this purpose we can use the functions mean(), median(), and sd() provided by base R. All three do not require additional arguments besides the data to be analysed which we can provide using the object data we created beforehand. mean(data) ## [1] 18 median(data) ## [1] 15.5 sd(data) ## [1] 13.49074 2.7 If/Else statements Often we want to write code that reacts to the value of a certain object and thus “decides” by itself which action to perform, based on this value. To achieve this, we can use conditional execution by using if and else statements. The basic syntax look as follows: if (condition) { code to be executed when the condition is TRUE } else { code to be executed when the condition is FALSE } if and else are both followed by {}, which enclose the code to be executed. While it is not strictly necessary to stretch the code over multiple lines, I would highly recommend doing this. It increases readability of your code for yourself and anyone you share it with, which makes it easier to understand and also makes it easier to spot mistakes and correct them. For the same reasons, it is also recommended to indent the code to be executed. Conditions are created by applying comparison operators, like ==, &lt;, &gt;, != and so on. Thus the condition compares two values and returns TRUE if the condition is met, and FALSE if it is not met. When the condition for the if statement is TRUE, the code between the {} following it is executed. If it is FALSE, the code between the {} following the else statement is executed instead. x &lt;- 42 if (x == 42) { print(&quot;This is the answer&quot;) } else { print(&quot;This is not the answer&quot;) } ## [1] &quot;This is the answer&quot; x &lt;- 24 if (x == 42) { print(&quot;This is the answer&quot;) } else { print(&quot;This is not the answer&quot;) } ## [1] &quot;This is not the answer&quot; In the code above, we first asign a value to the object x and follow this with an if statement that checks if the value equals 42 and returns a string to the console, indicating if the condition was met or not. We can also chain multiple if and else statements to create more complex decisions. While it is possible to chain a high number of statements in this way, it is not recommended to overuse this technique. The code will get long, hard to understand and thus mistakes get more likely. To chain statements, we use else if statements after the first if and before the last else. The basic syntac stays the same. Contemplate the example below. x &lt;- 42 if (x == 42) { print(&quot;This is the answer&quot;) } else if (x &lt; 42) { print(&quot;The value is too low to be the answer&quot;) } else { print(&quot;The value is too high to be the answer&quot;) } ## [1] &quot;This is the answer&quot; x &lt;- 24 if (x == 42) { print(&quot;This is the answer&quot;) } else if (x &lt; 42) { print(&quot;The value is to low to be the answer&quot;) } else { print(&quot;The value is to high to be the answer&quot;) } ## [1] &quot;The value is to low to be the answer&quot; x &lt;- 84 if (x == 42) { print(&quot;This is the answer&quot;) } else if (x &lt; 42) { print(&quot;The value is to low to be the answer&quot;) } else { print(&quot;The value is to high to be the answer&quot;) } ## [1] &quot;The value is to high to be the answer&quot; In the first case, the if condition is met and the code following the if statement is executed. In the second case, the if condition is not met, so the code checks if the condition for the else if statement is TRUE. It is in this case, so the code following it is executed. In the third case, both conditions return FALSE and thus the code following the else statement is run. We can also combine conditions by applying logical operators to create more complex conditions. &amp;&amp; – AND – checks if both conditions combined by it are TRUE and only returns TRUE if both are at the same time. || – OR – checks if at least on of the conditions is TRUE and returns TRUE if one or both are. When combining two conditions, we thus have these possibilities: TRUE &amp;&amp; TRUE returns TRUE TRUE &amp;&amp; FALSE, FALSE &amp;&amp; TRUE and FALSE &amp;&amp; FALSE all return FALSE TRUE || TRUE, TRUE || FALSE and FALSE || TRUE all return TRUE FALSE || FALSE returns FALSE In most cases, you could also use single &amp; and | operators, to combine conditions. &amp;&amp; and || are the safer alternatives, as they always return a single TRUE or FALSE and if statements can not interpret a logical vector containing multiple values. &amp; and | can return such a vector if at least one object used for comparison is a vector of a length longer than 1. For more details on this, you can read the corresponding chapter in “R for Data Science” by Wickham &amp; Grolemund: https://r4ds.had.co.nz/functions.html#conditional-execution. x &lt;- 42 y &lt;- 42 if (x == 42 &amp;&amp; y == 42) { print(&quot;Both x and y are the answer&quot;) } else { print(&quot;At least one of x and y is not the answer&quot;) } ## [1] &quot;Both x and y are the answer&quot; x &lt;- 42 y &lt;- 24 if (x == 42 &amp;&amp; y == 42) { print(&quot;Both x and y are the answer&quot;) } else { print(&quot;At least one of x and y is not the answer&quot;) } ## [1] &quot;At least one of x and y is not the answer&quot; x &lt;- 42 y &lt;- 24 if (x == 42 || y == 42) { print(&quot;At least one of x and y is the answer&quot;) } else { print(&quot;Both x and y are not the answer&quot;) } ## [1] &quot;At least one of x and y is the answer&quot; x &lt;- 12 y &lt;- 24 if (x == 42 || y == 42) { print(&quot;At least one of x and y is the answer&quot;) } else { print(&quot;Both x and y are not the answer&quot;) } ## [1] &quot;Both x and y are not the answer&quot; We can use combined conditions and chaining to create more complex decisions. But be wary to not overuse both techniques, for the same reasons as stated above. x &lt;- 42 y &lt;- 42 if (x == 42 &amp;&amp; y == 42) { print(&quot;Both x and y are the answer&quot;) } else if (x == 42) { print(&quot;x is the answer&quot;) } else if (y == 42) { print(&quot;y is the answer&quot;) } else { print(&quot;Both x and y are not the answer&quot;) } ## [1] &quot;Both x and y are the answer&quot; x &lt;- 42 y &lt;- 24 if (x == 42 &amp;&amp; y == 42) { print(&quot;Both x and y are the answer&quot;) } else if (x == 42) { print(&quot;x is the answer&quot;) } else if (y == 42) { print(&quot;y is the answer&quot;) } else { print(&quot;Both x and y are not the answer&quot;) } ## [1] &quot;x is the answer&quot; x &lt;- 24 y &lt;- 12 if (x == 42 &amp;&amp; y == 42) { print(&quot;Both x and y are the answer&quot;) } else if (x == 42) { print(&quot;x is the answer&quot;) } else if (y == 42) { print(&quot;y is the answer&quot;) } else { print(&quot;Both x and y are not the answer&quot;) } ## [1] &quot;Both x and y are not the answer&quot; Please note, that the code following the statements can be more complex than the code used in these introductory examples and can be comprised of anything that is valid R code. Also, each {}can hold multiple lines of code. "],["R2.html", "3 RStudio in practice &amp; the tidyverse 3.1 RStudio Workflow 3.2 R packages 3.3 Tidyverse 3.4 Additional resources", " 3 RStudio in practice &amp; the tidyverse 3.1 RStudio Workflow Up until now, we wrote our code directly into the RStudio console, pressed “Enter” and received the desired output. This works but will not satisfy our needs in the long run. The main problem is, that the code we wrote essentially disappears after running it. Imagine that you want to rerun your code a week from now or even tomorrow. Maybe you took notes and can recreate it, but that means a lot of unsatisfying and error prone work. Also, maybe at some point you want to share code with colleagues, fellow students, or the R community in general. At the same time, as our code gets more complex, spans multiple lines and consists of many interdependent blocks of code, you will inevitably run into the situation where you realise you made a mistake or have to change some code at the very beginning of your R session. This would mean, recreating and rerunning most or all of the code you have already written. These are some of the reasons why we should start writing our code into so called R Scripts. 3.1.1 R Scripts To create a new R Script, you can click on “File” &gt; “New File” &gt; “R Script”, or more conveniently press “CTRL” + “Shift” + “N” simultaneously. This creates an untitled script that we can write our code into. Let’s start with something simple by recreating some of the code from last week. a &lt;- 17 b &lt;- 4 the_answer &lt;- (a + b) * 2 the_answer ## [1] 42 We assign two numerical values two the objects a and b, assign a calculation based on these objects to the new object the_answer and prompt R to return its value to us. Instead of writing the code line by line into the console, now we write the whole block into the newly created script. We can now run the complete script by clicking on “Source” in the upper toolbar attached to the script’s tab. In most cases I prefer running the script line by line though. This allows full control of the process and enables you to stop in certain lines to e.g. contemplate what the code is doing, check for errors or change details of the code before moving on to the next line. You can do this either by clicking on “Run” in the toolbar or pressing “CTRL” + “Enter” simultaneously. In both cases, RStudio copies the line of code where your text cursor is currently residing into the console and runs it for you. The text cursor then conveniently jumps to the next line in the script. In this way you can quickly run your script line by line, while having full control over when to stop. You can decide for yourself what the right approach to running your code is, based on the given situation. But remember that R always assumes that you know what you’re doing. There will be no warning prompts if you are about to overwrite work you have previously done. When you are done writing your script, you might want to save it to the hard drive, preserving your work for later re-runs or for sharing. By clicking on “File” &gt; “Save” or presing “CTRL” + “S” you can save the file with a name of your choosing. The file extension for R Scripts is always “.R”. One problem – that you will run into sooner or later – is that you will try to run incomplete code from a script, most commonly a missing closing bracket. In this case, RStudio puts the code to be run into the console and begins a new line, starting with +, and then nothing happens. R assumes that your code will continue in a further line and waits for you to enter it after the +. In most cases the right approach is to cancel the entered command, fix your code and re-run it afterwards. To cancel an already entered command, you have to click into the “Console” tab and press “CTRL” + “C” or “Esc” on your keyboard. The &gt; prompt will reappear in the console and you can continue with your work. 3.1.2 Projects In many cases, your work will consist of multiple scripts, data files, graphics saved to the disk or additional output. So it makes sense to assign your files to a place on your hard drive. You can do this “by hand” but a convenient approach might be to use RStudio’s project functionality. By clicking on “File” &gt; “New Project”, you can start the project creation wizard. If you have already created a folder on your hard drive that shall contain the project, you can click on “Existing Directory”, select the folder and click on “Create Project”. You can also create the folder on the fly by clicking on “New Directory” &gt; “New Project” and then choosing a folder name and the sub-folder where it should be placed, before creating the project. RStudio will now close all files currently open and switch to your newly created project. The name you chose for the project’s folder will also be its name, seen in RStudio’s title bar. When you look at the “File” tab (lower right), you will also see that you are now in the project’s folder. This is your current working directory, a concept we will talk about momentarily. All scripts you create while working in your project will become a part of it. So when you want to return to continuing your work, you can now click on “File” &gt; “Open Project”. All files opened the last time you worked on the project will be reopened and you will again be in the project’s working directory. This is an easy and convenient way to keep your work tidy. At this point, I would advise you to create a project for this introduction to web scraping and create R scripts for each chapter as parts of the project. The name and sub-folder you choose is not important from the point of view of functionality, but it should make sense to you. We should now briefly talk about the working directory. If you try to open or save a file directly from an R script – without specifying a complete path – R will always assume you refer to your working directory. If you created a project, this automatically set the project’s folder as the working directory. You can always check for your current working directory by entering getwd() into the console. You can change your current working directory by clicking on “Session” &gt; “Set Working Directory” &gt; “Choose Directory…” or by using the function setwd() with the desired path enclosed by \" as the function’s argument. 3.1.3 Comments You should get into the habit of commenting your code as early as possible. Comments are started with one or multiple #. All code following the # will not be evaluated by R and thus serves as the perfect place to comment on what you were doing and thinking while writing the code. Why do this? When you reopen a script that you have not been working on in a while, it can be hard to understand what you tried to do in the first place. Commented code makes this much easier. This is even more true if you share your code with other people. They may have very different approaches to certain R problems and clearly commented code will help them to quickly understand it. You should see this as a sign of respect towards the time your peers may invest in helping you with your coding problems. # assigning objects a &lt;- 17 b &lt;- 4 # calculating the answer the_answer &lt;- (a + b) * 2 the_answer ## [1] 42 # but what is the question? If you plan on using setwd() in your script, it is a good idea to comment this line before sharing your script. Other people will have different folder structures and will want to decide for themselves. The same goes for all lines that will save something to the hard drive, e.g. data sets or exported graphics. The R and RStudio communities are very welcoming and you will always find people that are willing to lend you their help, so you should return the favour and be polite in your code. This includes writing clear comments and not cluttering anyone’s hard drive with files they may not want to have. 3.2 R packages The R world is open and collaborative by nature. Besides the packages that come with your R installation – base R – an ever growing number of additional packages, written by professionals and users, is available for download by anyone. Every package is focussed on a specific use case and brings with it a number of functions that enable R to be used for tasks that the original software designers did not have in mind or at the very least provide a smoother user experience in cases where the original base R solutions are more complicated. The packages, its documentation and various other related information are hosted at CRAN – “Comprehensive R Archive Network”– which you already got to know during the installation of R. If you install a package directly from RStudio, it uses CRAN to find and download the package and the associated files. 3.2.1 Installing and loading packages To install a package we can use the R function install.packages() where the name of the package to be installed is written enclosed by \" between the parentheses. Normally we do this using the console. Installing packages from an R script works as well, but as we only need to perform the installation once, there is no benefit in it. It actually slows things down if we repeat the installation every time we run a script. At the same time, if we share our script, it is impolite to force an (re)installation on somebody else. For this introduction we will focus on the packages of the tidyverse – more on them below. To install the core tidyverse package, you should type: install.packages(&quot;tidyverse&quot;) R will output a lot of information concerning the installation process, and close with a satisfying DONE (tidyverse) if everything went according to plan. Now that the installation is complete, we can load the package. This should normally be done in the first lines of a script. This way all necessary packages are loaded at the beginning of running a script and other users that see your code also immediately see which packages are required. Loading a package is done with library() with the name of the package in the parentheses, this time without the need for enclosing it in \". library(tidyverse) Loading the tidyverse package returns a lot of information to us, some of which we will look at in more detail during the course of this chapter. Please note that not all packages are that verbose in their loading process. Often you will get no output at all which is a good sign, as this also means that the package loaded correctly. If anything goes wrong, R will return an error message. 3.2.2 Namespaces Looking at the last lines of the returned message when loading the tidyverse package, we’re informed that there are two conflicts. These arise when two or more loaded packages include functions with the same name. Here we can see that the tidyverse package dplyr masks the functions filter() and lag() from the base R package stats. If we would have used filter() without loading dplyr, the function from the stats package would have been used. After loading it, the function from dplyr masks the function from stats and is used instead. If we had a case where we want to load dplyr, but still use filter() from stats, we can still do this by explicitly declaring the namespace which we are referring to. The namespace basically is a reference for R where to look up the function we have called. If we just write the function’s name, R looks for it in the list of loaded packages, which would result in applying filter() from dplyr here. But we can tell R to look up the function in another namespace, by using the notation namespace::function. So to call filter() from stats while the function is masked by the similarly named function from dplyr, we could write stats::filter(). As the function will not work without further arguments, we can’t try this out directly, but the same principle applies to loading the help files: ?dplyr::filter() ?stats::filter() 3.3 Tidyverse While we will use some base R functions throughout this course, our main focus will lie on the tidyverse packages. The tidyverse is a collection of R packages, all following a shared philosophy concerning the syntax of their functions and the way in which data is represented. We will see how the philosophy underlying the tidyverse can lead to more intuitive R code, especially when using the pipe (%&gt;%), in the next chapter. If you want to learn more about the concept of tidy data, the structure of data representation underlying the tidyverse, a read of the chapter on this concept from “R for Data Science” by Wickham &amp; Grolemund is highly recommended: https://r4ds.had.co.nz/tidy-data.html. Right now, the core tidyverse consists of eight packages. These are the packages that are loaded when we type library(tidyverse) and that are listed in the corresponding output under “Attaching packages”. As the name suggests, the packages comprise the core functionalities that define the tidyverse. This includes reading, cleaning and transforming data, handling certain data types, plotting graphs and more. Over the course of this introduction to web scraping, we will make use of several of these packages, so in most chapters we will begin our scripts with loading the tidyverse package. Besides the core tidyverse, a number of additional and more specialised packages are part of the tidyverse and were already installed when you ran install.packages(\"tidyverse\") above. Among them, the package rvest is of special importance to us, as it will be our main tool for web scraping throughout the course. For a full list of tidyverse packages and the corresponding descriptions of their functionality, you can visit: https://www.tidyverse.org/packages/ 3.3.1 Tibbles The “tibble” package is part of the core tidyverse and offers an alternative to the data frame data structure that is used in base R to represent data in tabular form. The differences between data frames and tibbles are relatively minor. If you are interested in the details, you can read up on them in this section from “R for Data Science” and the chapter on tibbles in general: https://r4ds.had.co.nz/tibbles.html#tibbles-vs.-data.frame. For now, it will suffice to know that tibbles are used throughout this introduction, but that all examples will also work with the classic data frames. The syntax to create a tibble is simple. Every column represents a variable, every row an observation. You should think of the columns as vectors, where the first position in each vector corresponds to the first observation (row), the second position in each vector to the second observation, and so on. In this way, we can create tibbles vector by vector or variable by variable, using the function tibble(). We assign a name to the variable followed by = and the data to be assigned to the variable. The variable-data pairs are separated by ,: tibble(numbers = c(0, 1, 2), strings = c(&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;), logicals = c(FALSE, TRUE, TRUE)) ## # A tibble: 3 x 3 ## numbers strings logicals ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 0 zero FALSE ## 2 1 one TRUE ## 3 2 two TRUE For longer code like this, it is advisable to use multiple lines and a more clear formatting to create code that is readable and intuitive: tibble( numbers = c(0, 1, 2), strings = c(&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;), logicals = c(FALSE, TRUE, TRUE) ) ## # A tibble: 3 x 3 ## numbers strings logicals ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 0 zero FALSE ## 2 1 one TRUE ## 3 2 two TRUE R understands that all five lines are part of one command as it evaluates everything between the opening and closing bracket of the tibbles() function together. We just have to make sure, that we don’t miss the closing bracket or a , that separates the variable-data pairs. This actually is a main source of errors and will be high on your list of things to check if something does not work as planned. We can also use calculations and functions directly in tibble creation, circumventing the need to assign the results to an object first: tibble( numbers = c(1, 2, 3), roots = sqrt(numbers), rounded = round(roots) ) ## # A tibble: 3 x 3 ## numbers roots rounded ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 ## 2 2 1.41 1 ## 3 3 1.73 2 Sidenote: The function sqrt() takes the square roots of the data it is applied to. 3.4 Additional resources When learning R and when using functions and packages that are new to you, you will regularly run into situations where you need help in understanding what is happenning and what you can do. Luckily, there a lot of resources that will help you on your R journey. You have already learned about the built-in help functionalities of R. Many packages also come with so called vignettes which offer more in-depth introductions to the functionalities of the packages. Let’s see if the tibble package comes with vignettes. To do this we can write: vignette(package = &quot;tibble&quot;) We get a list of all vignettes avaialable for the specific package. To access a specific vignette, we also use the vignette() function, this time with the specific name of the vignette as the function’s argument: vignette(&quot;types&quot;) You can also always check the CRAN page for the package in question. Here you can access the documentation as well as available vignettes, e.g.: https://cran.r-project.org/web/packages/tibble/index.html. Another highly recommended resource are the RStudio cheatsheets found at: https://www.rstudio.com/resources/cheatsheets/. These are available for many popular packages and present a comprehensive list of the functions offered by the packages. The RStudio homepage also offers many more resources for learning R and specific packages, including a number of webinars and tutorial videos available under the menu “Resources”: https://www.rstudio.com/ In general, the internet offers a lot of resources that you can access. One of the most important skills you have to develop as an aspiring R user is to understand the problem you are facing to the best of your abilities and formulate a short but precise google search. In most cases you can assume, that you are not the first or last person to have a specific problem. Someone will have written a blogpost, asked a question on https://stackoverflow.com/, made a video tutorial, and so on. If you can find these resources, you are already halfway there. There are also a lot of books available on R and RStudio in general, as well as on more specific applications in R. I want to reccomend two of them in particular, both avalaible as paperback or online: Intro to R for Social Scientists by Jasper Tjaden. An accessible introduction to R that expands on the concepts only touched here. Written for a seminar at the University of Potsdam in summer 2021. Available under: https://jaspertjaden.github.io/course-intro2r/ R Cookbook, 2nd Edition by J.D. Long &amp; Paul Teetor. The book is comprised of recipes for specific tasks, you might want to perform. It is not designed as a course but rather as reference for concrete questions. Available under: https://rc2e.com/ R for Data Science by Hadley Wickham and Garrett Grolemund. An introduction to data science using (almost) exclusively the tidyverse packages. Available under: https://r4ds.had.co.nz/ "],["rvest1.html", "4 First scraping with rvest 4.1 The rvest package 4.2 hello_world.html 4.3 Countries of the World", " 4 First scraping with rvest With the knowledge of how an HTML file is constructed and how R and RStudio work in basic terms, we are equipped with the necessary tools to take our first steps in web scraping. In this session we will learn how to use the R package rvest to read HTML source code into RStudio, extract targeted content we are interested in, and transfer the collected data into an R object for further analysis in the future. 4.1 The rvest package Part of the tidyverse is a package called rvest, which provides us with all the basic functions for a variety of typical web scraping tasks. This package was included in the installation of the tidyverse package, but it is not part of the core tidyverse and thus is not loaded into the current R session with library(tidyverse). Therefore, we have to do this explicitly: library(rvest) The output in the RStudio console informs us that besides rvest, the package xml2 has also been loaded, on which rvest is partly based and whose function for reading HTML files, we will need in the following. 4.2 hello_world.html As a first exercise, it is a good idea to scrape the Hello World example already described in chapter 1. As a reminder, here is the HTML source code: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; 4.2.1 read_html() The first step in web scraping is to convert the page we are interested in into an R object. This is made possible by the function read_html() from the xml2 package. read_html() “parses” the website, i.e. it reads the HTML, understands its source code and transforms it into a representation R can understand. This function needs the URL, i.e. the address of the website we want to read in, as its first argument. The URL must be given as a string, so we have to enclose it in \". The function also allows you to specify other options. In most cases, however, the default settings are sufficient. So we read in the hello_world.html file, assign it to a new R object at the same time and have this object put out in the next step: hello_world &lt;- read_html(&quot;https://webscraping-tures.github.io/hello_world.html&quot;) hello_world ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; As we can see in the output, the R object hello_world is a list with two entries. The first entry contains everything enclosed by the &lt;head&gt; tag, the second entry everything enclosed by the &lt;body&gt; tag. The opening and closing &lt;html&gt; tag is not part of the object. Remembering that HTML code is hierarchically structured, the list is thus organised based on the highest remaining levels – &lt;title&gt; and &lt;body&gt;. We have thus successfully created a representation of the website in an R object. But what do we do with it now? In the case of this simple example, we might be interested in extracting the title of the website or the text displayed on the page. 4.2.2 html_nodes() The function html_nodes() from the rvest package allows us to extract individual elements of the HTML code. To do this, it needs the object to be extracted from, as the first argument and a selector as well. In this introduction, we will concentrate exclusively on the so-called CSS selectors. The alternative XPath is a bit more flexible, but CSS selectors are sufficient in most cases and have a shorter and more intuitive syntax, which clearly makes them the tool of choice here. We will discuss the possibilities offered by CSS selectors in more detail in chapter 5.1 and will limit ourselves to the basics for now. A selector in the form \"tag\", selects all HTML tags of the specified name. If we want to extract the &lt;title&gt; tag, we can do so in this way: node_title &lt;- html_nodes(hello_world, css = &quot;title&quot;) node_title ## {xml_nodeset (1)} ## [1] &lt;title&gt;Hello World!&lt;/title&gt; If we want to extract the text Hello World! shown on the website, one possibility would be to select the complete &lt;body&gt; tag, since in this case no other text is displayed on the page. node_body &lt;- html_nodes(hello_world, css = &quot;body&quot;) node_body ## {xml_nodeset (1)} ## [1] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; This works in principle, but we also extracted the &lt;b&gt; tags as well as multiple new lines (\\n), which we do not need both. It would be more efficient to directly select the &lt;b&gt; tag enclosing the text. node_b &lt;- html_nodes(hello_world, css = &quot;b&quot;) node_b ## {xml_nodeset (1)} ## [1] &lt;b&gt;Hello World!&lt;/b&gt; 4.2.3 html_text() In this case, we are interested in the text in the title and on the website, i.e. the content of the tags. We can extract this from the selected HTML elements in an additional step. This is made possible by the rvest function html_text(). This requires the previously extracted HTML element as the only argument. html_text(node_title) ## [1] &quot;Hello World!&quot; html_text(node_b) ## [1] &quot;Hello World!&quot; With this, we have successfully completed our first web scraping goal, the extraction of the title and the text displayed on the page. One more thing about the application of html_text() to elements that themselves contain further tags: Further above we extracted the object node_body, which contains the &lt;b&gt; tags as well as several line breaks in addition to the displayed text. Here, too, we can extract the pure text. html_text(node_body) ## [1] &quot;\\n Hello World!\\n &quot; We see that the function has conveniently removed the &lt;b&gt; tags we were not interested in, for us. However, the line breaks and several spaces, so-called whitespace, remain. Both can be removed with the additional argument trim = TRUE. html_text(node_body, trim = TRUE) ## [1] &quot;Hello World!&quot; 4.3 Countries of the World Let us now look at a somewhat more realistic application. The website https://scrapethissite.com/pages/simple/ lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further. Before we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object. library(tidyverse) library(rvest) website &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) To understand the structure of the HTML file, the first step is to look at the source code. As always, we can open it by right-clicking in the browser window and then clicking on “View Page Source”. The first 100 or so lines of HTML code mainly contain information on the design of the website, which should not distract us further at this point. We are purely interested in the data of the countries. The first country listed on the website is Andorra. It therefore makes sense to search the source code specifically for “Andorra”. The key combination CTRL+F opens the search mask in your browser. We find what we are looking for in line 128. Since this source code, designed for practice purposes, is formatted in a very structured way, we quickly realise that lines 125-135 are code blocks related to Andorra. Let’s look at these more closely: &lt;div class=&quot;col-md-4 country&quot;&gt; &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; &lt;/div&gt;&lt;!--.col--&gt; All the information about Andorra is enclosed in a &lt;div&gt; tag. As a reminder, a &lt;div&gt; defines a grouping of code across multiple lines. In web design practice, these groupings are mainly used to assign a certain CSS style to the following code via the argument class=, for example to define the typeface. From a web scraping perspective, we generally don’t care how the styles are defined. We just need to know that we can exploit these CSS assignments of classes for our purposes. At the next level down, we find two blocks, one containing, among other things, the name of the country and another containing information about that country. Let’s look at the first block first. 4.3.1 Country names &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; The name “Andorra” is enclosed in an &lt;h3&gt; tag, i.e., a third-level heading. In addition to the name, we also find another tag within the tag that includes the image of the flag. Since we are not interested in the graphics here, we can ignore this. On this website, all &lt;h3&gt; tags are used exclusively to display the names of the countries. Thus, we can use the &lt;h3&gt; tag as a CSS selector to read out the enclosed text analogous to the first example. node_country &lt;- html_nodes(website, css = &quot;h3&quot;) text_country &lt;- html_text(node_country, trim = TRUE) head(text_country, n = 10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; The result looks promising. Since the structure of the code block is the same for each country, the vector text_country was created in this way with 250 entries, exactly the number of countries listed on the website. For reasons of clarity, it often makes sense not to put out the complete and often very long vectors, data frames or tibbles, but to use the function head() to list the number of entries specified by the argument n, starting with the first. 4.3.1.1 The pipe %&gt;% At this point, we should think again about the readability and structure of our R code. Let us consider the preceding code block: node_country &lt;- html_nodes(website, css = &quot;h3&quot;) text_country &lt;- html_text(node_country, trim = TRUE) As we have seen, this achieves our goal. However, we have also created the node_country object to temporarily save the first step – reading the &lt;h3&gt; tags. We will never need this object again. If we use the already introduced pipe %&gt;% instead, the need to cache partial results is eliminated and we write code that is more intuitive and easier to understand at the same time. country &lt;- website %&gt;% html_nodes(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE) head(country, n = 10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; As a reminder, the pipe passes the result of a work step along to the next function, which in the tidyverse as well as in many other R-functions (but not ALL!) takes data as the first argument, which we then do not have to define explicitly. For a better understanding, let’s look at the above example in detail. The first line passes the object website along to the function html_nodes(). So we don’t have to tell html_nodes() which object to apply to, because we already passed it along to the function with the pipe. The function is applied to the object website with all other defined arguments – here css – and the result is passed along again to the next line, where the html_text() function is applied to it. Here the pipe ends, and the final result is assigned to the object country. We now need three instead of two lines to get the same result, but the actual typing work has been reduced – especially if you create the pipe with the key combination CTRL+Shift+M – and we have created code that can be read and understood more intuitively with a little practice. So should we always connect all steps with the pipe? No. In many cases it makes sense to save intermediate results in an object, namely whenever we will access it multiple times. In our example, we could also integrate the import of the website into the pipe: country &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) %&gt;% html_nodes(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE) Overall, this saves us even more typing. However, since we still have to access the selected website multiple times later on, this would also mean that the parsing process has to be repeated each time. On the one hand, this can have a noticeable impact on the computing time for larger amounts of data. On the other hand, it also means accessing the website’s servers and downloading the data again each time. However, we should avoid data traffic generated without good reasons as part of a good practice of web scraping. So it makes perfect sense to save the result of the read_html() function in an R object so that it can be reused multiple times later. 4.3.2 Capitals, population and area Let us now turn to the further information for each country. These are located in the second block of the HTML code considered above: &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; As we can see, both the name of the capital, the population of the country, and its size in square kilometers are enclosed by a &lt;span&gt; tag in lines 2–4 respectively. Like &lt;div&gt;, &lt;span&gt; defines groupings, but not across multiple lines but for one, or as here, part of a line. So let’s try to read the names of the capitals, using the &lt;span&gt; tag as a selector. website %&gt;% html_nodes(css = &quot;span&quot;) %&gt;% html_text() %&gt;% head(n = 10) ## [1] &quot;Andorra la Vella&quot; &quot;84000&quot; &quot;468.0&quot; &quot;Abu Dhabi&quot; ## [5] &quot;4975593&quot; &quot;82880.0&quot; &quot;Kabul&quot; &quot;29121286&quot; ## [9] &quot;647500.0&quot; &quot;St. John&#39;s&quot; So we get the names of the capitals, but also the population and the size of the country. span was too unspecific as a selector. Since all three types of country data are enclosed with &lt;span&gt; tags, all three are also selected. So we have to tell html_nodes() more precisely which &lt;span&gt; we are interested in. This is where the CSS classes we mentioned earlier come into play. These differ between the three countries’ information. For example, the &lt;span&gt; that includes the name of the capital city is assigned the class \"country-capital\". We can target this class with our CSS selector. To select a class, we can use the syntax .class-name. So, to select all &lt;span&gt; that have the class \"country-capital\", we can do as follows: capital &lt;- website %&gt;% html_nodes(css = &quot;span.country-capital&quot;) %&gt;% html_text() head(capital, n = 10) ## [1] &quot;Andorra la Vella&quot; &quot;Abu Dhabi&quot; &quot;Kabul&quot; &quot;St. John&#39;s&quot; ## [5] &quot;The Valley&quot; &quot;Tirana&quot; &quot;Yerevan&quot; &quot;Luanda&quot; ## [9] &quot;None&quot; &quot;Buenos Aires&quot; We can repeat this in an analogue manner for the number of inhabitants with the class \"country-population\". population &lt;- website %&gt;% html_nodes(css = &quot;span.country-population&quot;) %&gt;% html_text() head(population, n = 10) ## [1] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ## [7] &quot;2968000&quot; &quot;13068161&quot; &quot;0&quot; &quot;41343201&quot; If we take a closer look at the vector created in this way, we see that it is a character vector. For inspection we can use the function str(), which gives us the structure of an R object, including the data type used. str(population) ## chr [1:250] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ... So the numbers were not read out as numbers but as strings. Among other things, this does not allow for calculation with the numbers. population[1] selects the first element of the vector. population[1] / 2 ## Error in population[1]/2: non-numeric argument to binary operator As you remember, one way to tell R to interpret the “text” read from the HTML code as numbers is to use the function as.numeric(). population &lt;- website %&gt;% html_nodes(css = &quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric() str(population) ## num [1:250] 84000 4975593 29121286 86754 13254 ... population[1] / 2 ## [1] 42000 In the same way, the size in square kilometers can be read with the class \"country-area\". area &lt;- website %&gt;% html_nodes(css = &quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() str(area) ## num [1:250] 468 82880 647500 443 102 ... 4.3.3 Merge into one tibble We have now created four vectors, which respectively contain the information about the name of the country, the associated capital, the number of population and the size of the country. For Andorra: country[1] ## [1] &quot;Andorra&quot; capital[1] ## [1] &quot;Andorra la Vella&quot; population[1] ## [1] 84000 area[1] ## [1] 468 We could already continue working with this, but for many applications it is more practical if we compile the data in tabular form. In the tidyverse, the form of the tibble is suitable for this purpose. countries &lt;- tibble( Land = country, Hauptstadt = capital, Bevoelkerung = population, Flaeche = area ) countries ## # A tibble: 250 x 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # … with 240 more rows This is not only more readable but also facilitates all further potential analysis steps. If we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines. website &lt;- &quot;https://scrapethissite.com/pages/simple/&quot; %&gt;% read_html() countries_2 &lt;- tibble( Land = website %&gt;% html_nodes(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE), Hauptstadt = website %&gt;% html_nodes(css = &quot;span.country-capital&quot;) %&gt;% html_text(), Bevoelkerung = website %&gt;% html_nodes(css = &quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric(), Flaeche = website %&gt;% html_nodes(css = &quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() ) countries_2 ## # A tibble: 250 x 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # … with 240 more rows "],["css-selectors-developer-tools.html", "5 CSS selectors &amp; Developer Tools 5.1 CSS selectors 5.2 Developer Tools", " 5 CSS selectors &amp; Developer Tools 5.1 CSS selectors In the previous section, you already got to know the first CSS selectors. These are actually used in web design to select individual elements of a website and apply a CSS style to them, i.e. to define the display of the elements. So they were not developed with web scraping applications in mind, yet we can still make use of them, because we also want to select individual elements of a website in order to extract them. CSS selectors are used in rvest as an argument of the read_html() function. As a second argument – the first one determines which data the function should be applied to –we specify a selector in the form css = \"selector\". This determines which elements of the HTML code we want to extract. It is important here that the entire selector – regardless of how many individual parts it consists of – is always passed along as a string to the argument. In the following, the CSS selectors are applied to the website https://webscraping-tures.github.io/turnout.html for illustration. You can view the source code the usual way. Firstly, we load the rvest package and parse the website. library(rvest) website &lt;- &quot;https://webscraping-tures.github.io/turnout.html&quot; %&gt;% read_html() We have already learned about the simplest selector. \"Tag\" selects all occurrences of the specified HTML tag. For example, we can select the title of the website – in the &lt;title&gt; tag – or the heading displayed in the browser window – in &lt;h3&gt;. website %&gt;% html_nodes(css = &quot;title&quot;) ## {xml_nodeset (1)} ## [1] &lt;title&gt;Voter turnout for German federal state elections&lt;/title&gt;\\n website %&gt;% html_nodes(css = &quot;h3&quot;) ## {xml_nodeset (1)} ## [1] &lt;h3&gt;Voter turnout for the last federal state elections in Germany:&lt;/h3&gt; 5.1.1 Classes We also got to know the selector for the argument class – \".class\" – in the previous section. We can select all &lt;span&gt; tags of the class \"state-name\" as follows: website %&gt;% html_nodes(css = &quot;.state-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;span.state-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; The first variation of the selector says, select all elements of the class \"state-name\". The second variation says, select all elements of the class \"state-name\". For this website, both variations are equivalent in result, since all elements assigned the class \"state-name\" are also &lt;span&gt; tags. However, this does not always have to be the case. Different tags with different content may very well have the same class in practice. Basically, constructing CSS selectors is always a balancing act between functionality and readability. The selector should select exactly only the elements we are interested in, and at the same time be understandable. The latter is especially important when others – or you yourself a few weeks later – read the code. Readability also means achieving a balance between length and clarity of the selector. To illustrate: website %&gt;% html_nodes(css = &quot;body &gt; div#data &gt; div[class^=state-] &gt; span.state-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; This is the full path through the hierarchical HTML structure, which leads us to the result already seen above. You don’t need to fully understand this one yet. But you can see that the selector can get long and complicated, and here we are dealing with a very simply designed website. For me, the selector \"span.state-name\" is a good choice, because it allows us to reach our goal and has a balance between brevity and readability that is pleasant for me. But you must decide this for yourself in each individual case. 5.1.2 IDs Another common attribute in HTML elements, is the id. These identify individual HTML elements with a unique assigned name. Among other things, these are used for design purposes, as part of scripts for the dynamic design of websites or in HTML forms. We can also use them to extract individual elements specifically. In our example, the &lt;span&gt; tag, which includes the name of each state, has an id that identifies the state with a two-character abbreviation: &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; The selector for IDs is being written as \"#id\". website %&gt;% html_nodes(css = &quot;#bw&quot;) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; In this case, however, we have selected more than we wanted. Because the &lt;img&gt; tag, which represents the flags of the federal states, has the attribute id=\"bw\" as well. Strictly speaking, this is against the HTML rules, but this is also a reality. We can’t rely on the creators of a website to always write clean HTML code, so we have to be able to deal with “rule violations” and unexpected structures. The solution at this point is the combination of ID and tag in the selector, in the form \"tag#id\". website %&gt;% html_nodes(css = &quot;span#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; A combination with the class attribute is also possible. website %&gt;% html_nodes(css = &quot;span.state-name#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; In general, it should be noted that the CSS selectors are case-sensitive. This means that we also need to be case sensitive. In the entry for Hamburg, the name of the id is capitalized: &lt;span class=&quot;state-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; If we want to extract this element, we have to use this way of writing in the selector as well, \"#hh\" will not work. website %&gt;% html_nodes(css = &quot;span#hh&quot;) ## {xml_nodeset (0)} website %&gt;% html_nodes(css = &quot;span#HH&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; 5.1.3 Attributes ID and Class are attributes of HTML tags. Since both are particularly relevant in web design, the shortcuts presented above exist to select them quickly. However, we can use all attributes occurring in an HTML code to select elements. The corresponding CSS selector is written as tag[attribute]. At the bottom of our example HTML code, there are three &lt;a&gt; tags. These have, in addition to href=\"url\" – the linked page – the attribute target_=\"\". This specifies how the link should open. The value \"_blank\" opens the link in a new browser tab, \"_self\" in the active tab. For the second link no target=\"\" attribute is set. In our example, to select all &lt;a&gt; tags that have a target attribute, i.e. the first and third, we could proceed as follows: website %&gt;% html_nodes(css = &quot;a[target]&quot;) ## {xml_nodeset (2)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... ## [2] &lt;a href=&quot;https://webscraping-tures.github.io/css-selectors-developer-tool ... With \"element[attribute='value']\" it is possible to select only attributes with a certain value. Note that the value is enclosed in single quotes. The manner of writing \"element[attribute=\"value\"]\" would be split into the two strings \"element[attribute=\" and \"]\" and the R object value, which cannot be interpreted by R in this combination. Instead, we use single quotes to define the value of the attribute. This is a convention used in many programming contexts. Inside “ we use ‘, inside ‘ we use “ to be able to realize multiple levels of quotes. If we want to select only the link that opens in a new tab: website %&gt;% html_nodes(css = &quot;a[target=&#39;_blank&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... This can be further modified by specifying that the value of an attribute should have a certain beginning – \"element[attribute^='beginning']\" – a certain end – \"element[attribute$='end']\" – or contain a certain partial term – \"element[attribute*='partial term']\". Thus, we could select the links based on the beginning, the end, or any part of the URL assigned to the href=\"\" attribute. website %&gt;% html_nodes(css = &quot;a[href^=&#39;https://www&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... website %&gt;% html_nodes(css = &quot;a[href$=&#39;Deutschland&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://de.wikipedia.org/wiki/Flaggen_und_Wappen_der_L%C3%A4nder ... website %&gt;% html_nodes(css = &quot;a[href*=&#39;webscraping&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://webscraping-tures.github.io/css-selectors-developer-tool ... 5.1.4 Hierarchy levels We can think of the hierarchy of an HTML structure as analogous to a family tree. A simplified representation of our example page as a family tree could look like this: By clicking on the elements of the family tree, you can expand and collapse the hierarchy levels. Click once through the different levels and compare this with the HTML source code of the page. When you’re done with that, I suggest collapsing &lt;head&gt; and expanding only &lt;div id=\"data\"&gt; in &lt;body&gt; and the first &lt;div&gt; at the next level down. With that, you should see everything we need below. With the metaphor of the family tree and the associated terms “descendant”, “child/parent”, and “sibling”, we should have an easier time understanding the slightly more advanced Selector concepts that follow. 5.1.4.1 Descendant If element A is a “descendant” of element B, this means that A “descended” from B over any number of generations. This can be one generation – i.e. a direct child-parent relationship; it can also be any number of generations – i.e. grandchildren and grandparents with any number of “grand” prefixes. In the selector, we write this as \"B A\". For example, if we want to select all &lt;span&gt; tags that descend from the &lt;div id=\"data\"&gt; tag – that is, a grandchild-grandparent relationship: website %&gt;% html_nodes(css = &quot;div#data span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; It should be noted that we can only select younger generations with CSS Selectors. So, for example, we can’t select the grandparents of the grandchildren by “flipping” the selector around. To select the grandparents – i.e. &lt;div id=\"data\"&gt;, we would need to look at which element they descended from, for example &lt;body&gt;. CSS selectors are not limited to mapping the relationship of two generations. A longer selector containing four generations and leading to the same result could look like this: website %&gt;% html_nodes(css = &quot;body div#data div span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; 5.1.4.2 Child/Parent If we want to define direct child-parent relationships instead of lineages across any number of generations, we can do this with selectors in the form \"B &gt; A\". This means A is a direct child of B. Since the &lt;span&gt; tags selected earlier are not direct children of &lt;div id=\"data\"&gt;, \"div#data &gt; span\" would not achieve the desired goal here. However, the &lt;span&gt; tags are direct children of &lt;div&gt; with classes \"state-odd\" and \"state-even\" respectively. Since there is no other direct child-parent relationship of &lt;span&gt; and &lt;div&gt; tags in our example, the selector \"div &gt; span\" would already be sufficient. However, we could become more explicit to write less error-prone code. Both &lt;div&gt; tags are similar in that their classes start with \"state\" and we already know how to exploit this: website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; If we do not want to select all children, but only certain ones, there are a number of options. \":first-child\" selects the first child of an element: website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; :first-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... However, these are not the &lt;span&gt; but the &lt;img&gt; tags, as a quick look at the family tree reminds us again. With \":nth-child(n)\" we can again specify more precisely which child – counted in the order in which they appear in the HTML code – we are interested in. To make it clearer again that we are only interested in &lt;span&gt; children, we combine \"span\" and \"nth-child(n)\": website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; span:nth-child(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; So we want to select all &lt;span&gt; tags that are the second child of the &lt;div&gt; tags whose classes start with \"state\". If we would like to select only the last child, we could do this with :last-child: website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; span:last-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; 5.1.4.3 Sibling(s) The common children of a parent element can be seen as siblings within the metaphor of the family tree. On the lowest hierarchical level, we thus have four siblings per federal state in our example. One &lt;img&gt; and three &lt;span&gt; tags. The selector \"Element-A ~ Element-B\" selects all siblings B that follow sibling A: website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; website %&gt;% html_nodes(css = &quot;img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; The result for both notations is identical. If we had &lt;span&gt; tags that follow &lt;img&gt; tags in other places in the HTML code in our example, the second variant would no longer be explicit enough. The selector \"Element-A + Element-B\" works in the same way, but only selects the sibling B that follows directly after sibling A: website %&gt;% html_nodes(css = &quot;img + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;span.state-name + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; The selection of siblings can be further restricted with \"element:first-of-type\", \"element:nth-of-type(n)\" and \"element:last-of-type\". In this way, the first, nth and last sibling of a certain type can be selected. The siblings are thus differentiated according to the type of element, with which our family tree metaphor is being somewhat overused. website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; span:first-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; span:nth-of-type(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; span:last-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; 5.1.5 Further selectors The wildcard \"*\" selects all elements. For example, we can also select all children of an element in the following way: website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; *&quot;) %&gt;% head(n = 8) ## {xml_nodeset (8)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [3] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [4] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [5] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... ## [6] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [7] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [8] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; If we want to exclude certain elements from the selection, this is possible with \":not(selector)\". Here, all elements are selected, except those that we have explicitly excluded. For example, all children except the &lt;img&gt; elements: website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; :not(img)&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;state-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;state-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; With \"selector-A, selector-B\", several selectors can be linked with “and/or”. This makes it possible, for example, to select two &lt;span&gt; classes in one step: website %&gt;% html_nodes(css = &quot;div[class^=&#39;state&#39;] &gt; span.election-year, div[class^=&#39;state&#39;] &gt; span.election-turnout&quot;) %&gt;% head(n = 4) ## {xml_nodeset (4)} ## [1] &lt;span class=&quot;election-year&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;election-turnout&quot;&gt;70.4&lt;/span&gt; ## [3] &lt;span class=&quot;election-year&quot;&gt;2018&lt;/span&gt; ## [4] &lt;span class=&quot;election-turnout&quot;&gt;72.4&lt;/span&gt; You now know the selectors most commonly used in web scraping. An overview of these and all other CSS selectors can be found on the W3 pages: https://www.w3schools.com/cssref/css_selectors.asp In addition to CSS selectors, there is another way to select elements of an HTML page: XPath. This method is even more flexible and allows, among other things, the selection of “ancestors”, i.e. higher hierarchical levels, starting from a lower one. The price for the higher flexibility, however, is an often longer and somewhat more complicated syntax. In most cases, the CSS selectors will suffice. And if at some point you reach a point where they are no longer sufficient to achieve your scraping goal, you will already be so proficient in using the CSS selectors that you will find it very easy to switch to XPath. On the pages of the W3, you will find a suitable introduction.: https://www.w3schools.com/xml/xpath_intro.asp 5.2 Developer Tools Recognising the HTML structure and identifying functional selectors can be very difficult on complex websites, especially if the HTML code is not as clearly formatted as in our example. This is where it can be helpful to use the web developer tools built into modern browsers. On the following screenshots, you can see the application in Chromium for Linux on our running example page. However, the procedure is almost identical in other browsers such as Chrome, Firefox or Edge. In all cases, open the Developer Tools by right-clicking in the browser window and then selecting “Inspect Element”. Depending on the setting, the Developer Tools open in a horizontally or vertically separated area of the browser. We see a variety of tabs here, but for this introduction we will concentrate purely on the “Elements” tab. This shows us the HTML code in its hierarchical structure and allows us to expand and c ollapse individual elements. If we select an element in “Elements”, it will also be marked in the display in the browser window. We can also activate the “Inspector” and select elements directly in the browser window by clicking on it. This in turn selects the corresponding entry in the “Elements” tab. The bottom of the “elements” tab, shows us the full CSS selector belonging to the selected element. By right-clicking on an element and selecting “Copy” -&gt; “Copy selector”, we can also get a CSS selector in the clipboard, which we can then paste into RStudio with CTRL+V. For the element selected above, i.e. the state name for Baden-Württemberg, we get the selector \"#bw\". In this case, this short selector is sufficient to uniquely identify the element. However, as explained above, I recommend the construction of more unique and comprehensible selectors. However, the selector created by the Developer Tools, and especially the display of the full path in the “Elements” tab, can be very helpful in constructing these and allow us to quickly grasp the structure of a website. Also, although we learned how to select the name for Baden-Württemberg, we did not learn, for example, how to select all the names of the federal states. But here, too, the developer tools can be helpful. One approach would be to select several of the name elements one after the other and compare how the full CSS selector changes. If we identify similarities and differences between the name elements, this can give us starting points for formulating our own selector. Another possibility would be to use the SelectorGadget extension instead of the integrated developer tools. Among other things, this enables the selection of several elements at the same time. You can find information on how to use it at:https://selectorgadget.com/ "],["rvest2.html", "6 Scraping of tables &amp; dynamic websites 6.1 Scraping of tables 6.2 Dynamische Websites", " 6 Scraping of tables &amp; dynamic websites 6.1 Scraping of tables In web scraping, we will often pursue the goal of transferring the extracted data into a tibble or data frame in order to be able to analyse it further. It is particularly helpful if the data we are interested in is already stored in an HTML table. Because rvest allows us to read out complete tables quickly and easily with the function html_table(). As a reminder, the basic structure of the HTML code for tables is as follows: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; The &lt;table&gt; tag covers the entire table. Rows are defined by &lt;tr&gt;, column headings with &lt;th&gt; and cells with &lt;td&gt;. Before we start scraping, we load the necessary packages as usual: library(tidyverse) library(rvest) 6.1.1 Table with CSS selectors from Wikipedia On the Wikipedia page on “CSS”, there is also a table with CSS selectors. This is our scraping target. First we parse the website: website &lt;- &quot;https://en.wikipedia.org/wiki/CSS&quot; %&gt;% read_html() If we look at the source code and search – CTRL+F – for “&lt;table”, we see that this page contains a large number of HTML tables. These include not only the elements that are recognisable at first glance as “classic” tables, but also, among other things, the “info boxes” at the top right edge of the article or the fold-out lists of further links at the bottom. If you want to look at this more closely, the Web Developer Tools can be very helpful here. Instead of simply selecting all &lt;table&gt; nodes on the page, one strategy might be to use the WDTs to create a CSS selector for that specific table: \"table.wikitable:nth-child(28)\". We thus select the table of class \"wikitable\" which is the 28th child of the parent hierarchy level – &lt;div class=\"mw-parser-output\"&gt;. If we only want to select a single HTML element, it can be helpful to use the function html_node() instead of html_nodes(). node &lt;- website %&gt;% html_node(css = &quot;table.wikitable:nth-child(28)&quot;) nodes &lt;- website %&gt;% html_nodes(css = &quot;table.wikitable:nth-child(28)&quot;) nodes ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th ... node ## {html_node} ## &lt;table class=&quot;wikitable&quot;&gt; ## [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th&gt;\\n&lt;th&gt;First defined&lt;br&gt;i ... The difference is mainly in the output of the function. This is recognisable by the entry inside the { } in the output. In the first case, we get a list of HTML elements – an “xml_nodeset” – even if this list, as here, consists of only one entry. html_node() returns the HTML element itself – the “html_node” – as the function’s output. Why is this relevant? In many cases it can be easier to work directly with the HTML element instead of a list of HTML elements, for example when transferring tables into data frames and tibbles, but more on that later. To read out the table selected in this way, we only need to apply the function html_table() to the HTML node. css_table_df &lt;- node %&gt;% html_table() css_table_df %&gt;% head(n = 4) ## Pattern ## 1 E ## 2 E:link ## 3 E:active ## 4 E::first-line ## Matches ## 1 an element of type E ## 2 an E element is the source anchor of a hyperlink of which the target is not yet visited (:link) or already visited (:visited) ## 3 an E element during certain user actions ## 4 the first formatted line of an E element ## First definedin CSS level ## 1 1 ## 2 1 ## 3 1 ## 4 1 The result is a data frame that contains the scraped contents of the HTML table and adopts the column names stored in the &lt;th&gt; tags for the columns of the data frame. Due to the very long cells in the “Matches” column, the output of the RStudio console is unfortunately not particularly helpful. Another advantage of using tibbles instead of data frames is that long cell contents are automatically abbreviated in the output. To convert a data frame into a tibble, we can use the function as_tibble(). css_table_tbl &lt;- node %&gt;% html_table() %&gt;% as_tibble() css_table_tbl %&gt;% head(n = 4) ## # A tibble: 4 x 3 ## Pattern Matches `First definedin CSS… ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 E an element of type E 1 ## 2 E:link an E element is the source anchor of a hype… 1 ## 3 E:active an E element during certain user actions 1 ## 4 E::first-l… the first formatted line of an E element 1 6.1.2 Scraping multiple tables It could also be our scraping goal to scrape not only the first, but all four content tables of the Wikipedia article. If we look at the four tables in the source code and/or the WDTs, we see that they all carry the class \"wikitable\". This allows us to select them easily. Please note that the function html_nodes() must be used again, as we no longer need just one element, but a list of several selected elements. tables &lt;- website %&gt;% html_nodes(css = &quot;table.wikitable&quot;) %&gt;% html_table() The result is a list of four data frames, each of which contains one of the four tables. If we want to select an individual data frame from the list, for example, to transfer it into a new object, we have to rely on subsetting. We have learned about basic subsetting for vectors using [#], in chapter 2. For lists, things can get a little bit more complicated. There are basically two ways of subsetting lists in R: list_name[#] and list_name[[#]]. The most relevant difference for us is what kind of object R returns to us. In the first case, the returned object is always a list, even if it may only consist of one element. Using double square brackets, on the other hand, returns a single element directly. So the difference is not dissimilar to that between html_nodes() and html_node(). For example, if our goal is to select the third data frame from the list of four data frames, which subsetting should we use? tables[3] %&gt;% str() ## List of 1 ## $ :&#39;data.frame&#39;: 7 obs. of 2 variables: ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... tables[[3]] %&gt;% str() ## &#39;data.frame&#39;: 7 obs. of 2 variables: ## $ Selectors : chr &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## $ Specificity: chr &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... In the first case, we see that we have a list of length 1, which contains a data frame with 7 lines and 2 variables, as well as further information about these variables. In the second case, we get the data frame directly, i.e. no longer as an element of a list. So we have to use list_name[[]] to directly select a single data frame from a list of data frames. If we are interested in selecting several elements from a list instead, this is only possible with list_name[]. Instead of selecting an element with a single number, we can select several with a vector of numbers in one step. tables[c(1, 3)] %&gt;% str() ## List of 2 ## $ :&#39;data.frame&#39;: 42 obs. of 3 variables: ## ..$ Pattern : chr [1:42] &quot;E&quot; &quot;E:link&quot; &quot;E:active&quot; &quot;E::first-line&quot; ... ## ..$ Matches : chr [1:42] &quot;an element of type E&quot; &quot;an E element is the source anchor of a hyperlink of which the target is not yet visited (:link) or already visited (:visited)&quot; &quot;an E element during certain user actions&quot; &quot;the first formatted line of an E element&quot; ... ## ..$ First definedin CSS level: int [1:42] 1 1 1 1 1 1 1 1 1 1 ... ## $ :&#39;data.frame&#39;: 7 obs. of 2 variables: ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... As a result, we get a list again that contains the two elements selected here. 6.1.3 Tabellen mit NAs What happens when we try to read a table with missing values? Consider the following example: https://webscraping-tures.github.io/table_na.html At first glance, it is already obvious that several cells of the table are unoccupied here. Values are missing. Let’s try to read in the table anyway. table_na &lt;- &quot;https://webscraping-tures.github.io/table_na.html&quot; %&gt;% read_html %&gt;% html_node(css = &quot;table&quot;) table_na %&gt;% html_table() ## Error: Table has inconsistent number of columns. Do you want fill = TRUE? We get a helpful error message informing us that the number of columns is not constant across the entire table. We are also kindly offered a possible solution directly. The function html_table() can be instructed with the argument fill = TRUE to automatically fill rows with a different number of columns with NA. This stands for “Not Available” and represents missing values in R. wahlbet &lt;- table_na %&gt;% html_table(fill = TRUE) wahlbet %&gt;% head(n = 4) ## Bundesland Wahljahr Wahlbeteiligung ## 1 Baden-Württemberg 2016.0 NA ## 2 Bayern 2018.0 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg 61.3 NA As we can see, html_table was able to fill the four cells with missing values with NA and read the table despite the problems. However, there are two different types of problems in the HTML source code, which the automatic repair handles differently. Let’s first look at the source code of the first two lines: &lt;tr&gt; &lt;td&gt;Baden-Württemberg&lt;/td&gt; &lt;td&gt;2016&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bayern&lt;/td&gt; &lt;td&gt;2018&lt;/td&gt; &lt;/tr&gt; For “Baden-Württemberg”, we see that the third column is created in the source code, but there is no content in this cell. html_table() could have read this without fill = TRUE and would have filled the cell with a NA. In contrast, for “Bayern” the cell is completely missing. This means that the second row of the table consists of only two columns, while the rest of the table has three columns. This is the problem that the error message pointed out to us. As a result, R was able to draw the correct conclusion in both cases and fill the cell in both rows with an NA. But let’s also look at the third and fourth rows in the source code: &lt;tr&gt; &lt;td&gt;Berlin&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;66.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Brandenburg&lt;/td&gt; &lt;td&gt;61.3&lt;/td&gt; &lt;/tr&gt; The second column is missing in both cases. In the first case it is created but empty, in the second it does not exist. In the first case, html_table() can again handle it without any problems. For “Brandenburg”, however, the function reaches its limits. We, as human observers, quickly realise that the last state election in Brandenburg did not take place in 61.3 and that this must therefore be the turnout. R cannot distinguish this so easily and takes 61.3 as the value for the column “Election year” and inserts a NA for “Voter turnout”. What to do? First of all, we should be aware that such problems exist. So if html_table() gives this error message, we should not simply set fill = TRUE, but try to find out why the problem exists and whether the option to have it fixed automatically will actually get us there. If this is not the case, one approach could be to write our own extractor function that fixes the problems directly during scraping. However, this is a rather advanced method and outside the scope of what is feasible in this introduction. However, we can at least correct the problems that arise afterwards. Our problem lies exclusively in row four. Its second column must be moved to the third and the second must then itself be set as NA. For this we need subsetting again. In the case of a data frame, we need to specify the row and column in the form df[row, column] to select a cell. So we can tell R: “Write in cell three the content of cell two, and then write in cell two NA”. wahlbet[4, 3] &lt;- wahlbet[4, 2] wahlbet[4, 2] &lt;- NA wahlbet %&gt;% head(n = 4) ## Bundesland Wahljahr Wahlbeteiligung ## 1 Baden-Württemberg 2016 NA ## 2 Bayern 2018 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg NA 61.3 6.2 Dynamische Websites In the “reality” of the modern internet, we will increasingly encounter websites that are no longer based exclusively on static HTML files, but generate content dynamically. You know this, for example, in the form of timelines in social media offerings that are generated dynamically based on your user profile. Other websites may generate the displayed content with JavaScript functions or in response to input in HTML forms. In many of these cases, it is no longer sufficient from a web scraping perspective to read in an HTML page and extract the data you are looking for, as this is often not contained in the HTML source code but is loaded dynamically in the background. The good news is that there are usually ways of scraping the information anyway. Perhaps the operator of a page or service offers an API (Application Programming Interface). In this case, we can register for access to this interface and then get access to the data of interest. This is possible with Twitter, for example. In other cases, we may be able to identify in the embedded scripts how and from which database the information is loaded and access it directly. Or we use the Selenium WebDriver to “remotely control” a browser window and scrape what the browser “sees”. However, all of these approaches are advanced methods that are beyond the scope of this introduction. But in cases where an HTML file is dynamically generated based on input into a HTML form, we can read it using the methods we already know. 6.2.1 HTML forms and HTML queries As an example, let’s first look at the OPAC catalogue of the Potsdam University Library https://opac.ub.uni-potsdam.de/ in the browser. If we enter the term “test” in the search field and click on Search, the browser window will show us the results of the search query. But what actually interests us here is the browser’s address bar. Instead of the URL “https://opac.ub.uni-potsdam.de/”, there is now a much longer URL in the form: “https://opac.ub.uni-potsdam.de/DB=1/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test”. The first part is obviously still the URL of the website called up, let’s call this the base URL. However, the part “CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” was added to the end of the URL. This is the HTML query we are interested in here. Between the base URL and the query there are one or more components, which in this case may also differ depending on your browser. However, these are also irrelevant for the actual search query. The shortened URL: “https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” leads to the same result. A query is a request in which data from an HTML form is sent to the server. In response, the server generates a new website, which is sent back to the user and displayed in the browser. In this case, the query was triggered by clicking on the “Search” button. If we understand what the components of the query do, we could manipulate it and use it specifically to have a website of interest created and parsed. 6.2.2 HTML forms To do this, we first need to take a look at the HTML code of the search form. To understand this, you should display the source code of the page and search for “&lt;form” or use the WDTs to look at the form and its components. &lt;form action=&quot;CMD&quot; class=&quot;form&quot; name=&quot;SearchForm&quot; method=&quot;GET&quot;&gt; ... &lt;/form&gt; HTML forms are encompassed by the &lt;form&gt; tag. Within the tag, one or more form elements such as text entry fields, drop-down option lists, buttons, etc. can be placed. &lt;form&gt; itself carries a number of attributes in this example. The first attribute of interest to us is the method=\"GET\" attribute. This specifies the method of data transfer between client and server. It is important to note that the method “GET” uses queries in the URL for the transmission of data and the method “POST” does not. We can therefore only manipulate queries if the “GET” method is used. If no method is specified in the &lt;form&gt; tag, “GET” is also used as the default. The second attribute of interest to us is action=\"CMD\". This specifies which action should be triggered after the form has been submitted. Often the value of action= is the name of a file on the server to which the data will be sent and which then returns a dynamically generated HTML page back to the user. Let us now look at the elements of the form. For this, the rvest function html_form() can be helpful. &quot;https://opac.ub.uni-potsdam.de/&quot; %&gt;% read_html() %&gt;% html_node(css = &quot;form&quot;) %&gt;% html_form() ## &lt;form&gt; &#39;SearchForm&#39; (GET CMD) ## &lt;select&gt; &#39;ACT&#39; [1/3] ## &lt;select&gt; &#39;IKT&#39; [0/13] ## &lt;select&gt; &#39;SRT&#39; [0/4] ## &lt;input checkbox&gt; &#39;FUZZY&#39;: Y ## &lt;input text&gt; &#39;TRM&#39;: ## &lt;input submit&gt; &#39;&#39;: Suchen The output shows us in the first line again the values for method= and action= as well as the name of the form. The other six lines show that the form consists of three &lt;select&gt; and three &lt;input&gt; elements. We also see the names of these elements as well as the default value that is sent when the form is submitted, as long as no other value is selected or entered. Let’s look at some of these elements. &lt;select&gt; elements are drop-down lists of options that can be selected. This is the source code for the first &lt;select&gt; element in our example: &lt;select name=&quot;ACT&quot;&gt; &lt;OPTION VALUE=&quot;SRCH&quot;&gt;suchen [oder] &lt;OPTION VALUE=&quot;SRCHA&quot; SELECTED&gt;suchen [und] &lt;OPTION value=&quot;BRWS&quot;&gt;Index bl&amp;auml;ttern &lt;/select&gt; The attribute name=\"ACT\" defines the elements name, which is used when transmitting the data from the form via the query. The &lt;option&gt; tags define the selectable options, i.e. the drop down menu. &lt;value=\"\"&gt; represents the value transmitted by the form. The user is being shown the text following the tag. The default selection is either the first value in the list or – like in this case – the option with the attribute selected is being explicitly chosen as the default. The three other elements are &lt;input&gt; tags. Input fields whose specific type is specified via the attribute type=\"\". These can be, for example, text boxes (type=\"text\") or checkboxes (input=\"checkbox\"), but there are many more options available. A comprehensive list can be found at: https://www.w3schools.com/html/html_form_input_types.asp. Here is the source code for two of the three &lt;input&gt; elements on the example page: &lt;input type=&quot;text&quot; name=&quot;TRM&quot; value=&quot;&quot; size=&quot;50&quot;&gt; &lt;input type=&quot;submit&quot; class=&quot;button&quot; value=&quot; Suchen &quot;&gt; The first tag is of the type “text”, i.e. a text field, in this case the text field into which the search term is entered. In addition to the name of the element, a default value of the field is specified via value=\"\". In this case, the default value is an empty field. The second tag is of the type “submit”. This is the “Search” button, which triggers the transmission of the form data via the query by clicking on it. 6.2.3 The query But what exactly is being transmitted? Let’s look again at the example query from above: CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test The value of the action=\"\" attribute forms the first part of the query and is appended after the base URL. The value of the attribute tells the server what to do with the other transmitted data. This is followed by a ?, which introduces the data to be transmitted as several pairs of name=\"\" and value=\"\" attributes of the individual elements. The pairs are connected with &amp;. ACT=SRCHA thus stands for the fact that the option “SRCHA” has been selected in the element with the name “ACT”. What the values of the two other &lt;select&gt; elements “IKT” and “SRT” stand for, you can understand yourself with a look into the source code or the WDTs. The text entered in the field is transmitted as the value of the &lt;input type=\"text\"&gt; with the name “TRM”. Here “test”. The server receives the form data in this way, can then take a decision on the basis of the action=\"\" attribute, here “CMD”, how the data is to be processed and constructs the website accordingly, which it sends back to us and which is displayed in our browser. 6.2.4 Manipulating the query and scraping the result Now that we know what the components of the query mean, we can manipulate them. Instead of writing queries by hand, we should use R to combine them for us. We will also encounter the technique of manipulating URLs directly in the R code more often. So we should learn it early. The function str_c() assembles the strings listed as arguments, i.e. sequences of letters, into a single string. Strings stored in other R objects can also be included. If we have the goal of manipulating both the search method and the search handle, we could achieve this in this way: base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;test&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test&quot; If we now change the strings stored in the method and term objects and generate the complete URL again, these components of the query are manipulated accordingly. method &lt;- &quot;SRCH&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCH&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; The search method was set to the value “SRCH”, i.e. an “OR” search, the search term to “web scraping”. It is important to note that no spaces may appear in the query and that these are replaced by “+” when the form is submitted. So instead of “web scraping” we have to use the string “web+scraping”. As an example application, we can now have the server perform an “AND” search for the term “web scraping”, read out the HTML page generated by the server and extract the 10 titles displayed. base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; website &lt;- url %&gt;% read_html() The search results are displayed as tables in the generated HTML file. The &lt;table&gt; tag has the attribute-value combination summary=\"hitlist\", which we can use for our CSS selector: hits &lt;- website %&gt;% html_node(css = &quot;table[summary=&#39;hitlist&#39;]&quot;) %&gt;% html_table() %&gt;% as_tibble() hits %&gt;% head(n=10) ## # A tibble: 10 x 4 ## X1 X2 X3 X4 ## &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 NA NA &quot;&quot; NA ## 2 NA 1 &quot;Introduction to Data Systems : Building from Python/ Bres… NA ## 3 NA NA &quot;&quot; NA ## 4 NA 2 &quot;An Introduction to Data Analysis in R : Hands-on Coding, … NA ## 5 NA NA &quot;&quot; NA ## 6 NA 3 &quot;Quantitative portfolio management : with applications in … NA ## 7 NA NA &quot;&quot; NA ## 8 NA 4 &quot;Automate the boring stuff with Python : practical program… NA ## 9 NA NA &quot;&quot; NA ## 10 NA 5 &quot;Spotify teardown : inside the black box of streaming musi… NA This worked, but we see that the table consists mainly of empty rows and cells. These are invisible on the website, but are used to format the display. Instead of repairing the table afterwards, it makes more sense to extract only the cells that contain the information we are looking for. These are the &lt;td&gt; tags with class=\"hit\" and the attribute-value combination align=\"left\". On this basis, we can construct a unique CSS selector. hits &lt;- website %&gt;% html_nodes(css = &quot;td.hit[align=&#39;left&#39;]&quot;) %&gt;% html_text(trim = TRUE) hits %&gt;% head(n = 5) ## [1] &quot;Introduction to Data Systems : Building from Python/ Bressoud, Thomas. - 1st ed. 2020. - Cham : Springer International Publishing, 2020&quot; ## [2] &quot;An Introduction to Data Analysis in R : Hands-on Coding, Data Mining, Visualization and Statistics from Scratch/ Zamora Saiz, Alfonso. - 1st ed. 2020. - Cham : Springer International Publishing, 2020&quot; ## [3] &quot;Quantitative portfolio management : with applications in Python/ Brugière, Pierre. - Cham : Springer, [2020]&quot; ## [4] &quot;Automate the boring stuff with Python : practical programming for total beginners/ Sweigart, Albert. - 2nd edition. - San Francisco : No Starch Press, [2020]&quot; ## [5] &quot;Spotify teardown : inside the black box of streaming music/ Eriksson, Maria. - Cambridge, Massachusetts : The MIT Press, [2019]&quot; 6.2.5 Additional resources In order to process this information further and, for example, separate it into data on author, title, year, etc., advanced knowledge in dealing with strings is necessary, which unfortunately goes beyond the scope of this introduction. A good first overview can be found in the chapter “Strings” from “R for Data Science” by Wickham and Grolemund: https://r4ds.had.co.nz/strings.html The appropriate “cheat sheet” is also recommended: https://raw.githubusercontent.com/rstudio/cheatsheets/master/strings.pdf "],["rvest3.html", "7 Scraping of multi-page websites 7.1 Index-pages 7.2 Pagination", " 7 Scraping of multi-page websites In many cases, we do not want to scrape the content of a single website, but several sub-pages in one step. In this session we will look at two common variations. Index pages and pagination. 7.1 Index-pages An index page, in this context, is a website on which links to the various sub-pages are listed. We can think of this as a table of contents. The website for the tidyverse packages serves as an example: https://www.tidyverse.org/packages/. Under the point “Core tidyverse” the eight packages are listed, which are loaded in R with library(tidyverse). In addition to the name and icon, a short description of the package and a link to further information, are part of the list. Let’s look at one of the sub-pages for the core packages. Since they all have the same structure, you can choose any package as an example. It could be our scraping goal to create a table with the names of the core packages, the current version number, and the links to CRAN and the matching chapter in “R for Data Science” by Wickham and Grolemund. By now we have all the tools to extract this data from the websites. We could now “manually” scrape the individual sub-pages and merge the data. It would be more practical, however, if we could start from the index page and scrape all eight sub-pages and the data of interest they contain in one step. This is exactly what we will look at in the following. 7.1.1 Scraping of the index library(tidyverse) library(rvest) As a first step, we need to extract the links to the sub-pages from the source code of the index page. As always, we download the website and parse it. website &lt;- &quot;https://www.tidyverse.org/packages/&quot; %&gt;% read_html() In this case, the links are stored twice in the source code. In one case the image of the icon is linked, in the other the name of the package. You can follow this in the source code and/or with the WDTs yourself by now. However, we need each link only once. One of several ways to select them could be to select the &lt;a&gt; tags that directly follow the individual &lt;div class=\"package\"&gt; tags. a_nodes &lt;- website %&gt;% html_nodes(css = &quot;div.package &gt; a&quot;) a_nodes ## {xml_nodeset (8)} ## [1] &lt;a href=&quot;https://ggplot2.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... ## [2] &lt;a href=&quot;https://dplyr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [3] &lt;a href=&quot;https://tidyr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [4] &lt;a href=&quot;https://readr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [5] &lt;a href=&quot;https://purrr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [6] &lt;a href=&quot;https://tibble.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class= ... ## [7] &lt;a href=&quot;https://stringr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... ## [8] &lt;a href=&quot;https://forcats.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... Since we need the actual URLs to be able to read out the sub-pages in the following, we should now extract the values of the href=\"\" attributes. links &lt;- a_nodes %&gt;% html_attr(name = &quot;href&quot;) links ## [1] &quot;https://ggplot2.tidyverse.org/&quot; &quot;https://dplyr.tidyverse.org/&quot; ## [3] &quot;https://tidyr.tidyverse.org/&quot; &quot;https://readr.tidyverse.org/&quot; ## [5] &quot;https://purrr.tidyverse.org/&quot; &quot;https://tibble.tidyverse.org/&quot; ## [7] &quot;https://stringr.tidyverse.org/&quot; &quot;https://forcats.tidyverse.org/&quot; 7.1.2 Iteration with map() Before starting to parse the sub-pages, we must think about how we can get R to apply these steps automatically to several URLs one after the other. One possibility from base R would be to apply a “For Loop”. However, I would like to introduce the map() functions family, from the tidyverse package purrr. These follow the basic logic of the tidyverse, can easily be included in pipes and have a short and intuitively understandable syntax. The map() function takes a vector or list as input, applies a function specified in the second argument to each of the elements of the input, and returns to us a list of the results of the applied function. x &lt;- c(1.28, 1.46, 1.64, 1.82) map(x, round) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1 ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 2 For each element of the numerical vector x, map() individually applies the function round(). round() does what the name suggests, and rounds the input up or down, depending on the numerical value. As a result, map() returns a list. If we want to have a vector as output, we can use specific variants of the map functions depending on the desired type – logical, integer, double or character. Here is a quote from the help on ?map: “map_lgl(), map_int(), map_dbl() and map_chr() return an atomic vector of the indicated type (or die trying).” For example, if we want to have a numeric vector instead of a list as output for the above example, we can use map_dbl(): x &lt;- c(1.28, 1.46, 1.64, 1.82) map_dbl(x, round) ## [1] 1 1 2 2 Or for a character vector we can apply map_chr(). The function toupper() used here, puts returns the input as uppercase letters. x &lt;- c(&quot;abc&quot;, &quot;def&quot;, &quot;gah&quot;) map_chr(x, toupper) ## [1] &quot;ABC&quot; &quot;DEF&quot; &quot;GAH&quot; If we want to change the arguments of the applied function, the arguments are listed after the name of the function. Here, the number of decimal places to be rounded is set from the default value of 0 to 1. x &lt;- c(1.28, 1.46, 1.64, 1.82) map_dbl(x, round, digits = 1) ## [1] 1.3 1.5 1.6 1.8 This gives us an overview of iteration with map(), but this can necessarily only be a first introduction. For a more detailed introduction to For Loops and the map functions, I recommend the chapter on “Iteration” from “R for Data Science”: https://r4ds.had.co.nz/iteration.html For a more interactive German introduction, I recommend the section “Schleifen” in the StartR app by Fabian Class: https://shiny.lmes.uni-potsdam.de/startR/#section-schleifen 7.1.3 Scraping the sub-pages We can now use map() to parse all sub-pages in one step. As input, we use the character vector that contains the URLs of the sub-pages, and as the function to be applied, the familiar read_html(). For each of the eight URLs, the function is applied to the respective URL one after the other. As output we get a list of the eight parsed sub-pages. pages &lt;- links %&gt;% map(read_html) If we look at the sub-pages in the browser, we can see that the HTML structure is identical for each sub-page in terms of the information we are interested in – name, version number and CRAN as well as “R for Data Science” links. We can therefore extract the data for each of them using the same CSS selectors. pages %&gt;% map(html_node, css = &quot;a.navbar-brand&quot;) %&gt;% map_chr(html_text) ## [1] &quot;ggplot2&quot; &quot;dplyr&quot; &quot;tidyr&quot; &quot;readr&quot; &quot;purrr&quot; &quot;tibble&quot; &quot;stringr&quot; ## [8] &quot;forcats&quot; The name of the package is displayed in the menu bar in the upper section of the pages. This is enclosed by an &lt;a&gt; tag. For example, for https://ggplot2.tidyverse.org/ this is: &lt;a class=\"navbar-brand\" href=\"index.html\"&gt;ggplot2&lt;/a&gt;. The CSS selector used here is one of the possible options to retireve the desired information. So what happens in detail in the code shown? The input is the previously created list with the eight parsed websites. In the second line, by using map(), the function html_node() with the argument css = \"a.navbar-brand\" is applied to each of the parsed pages. For each of the eight pages, the corresponding HTML-node is selected in turn. These are passed through the pipe to the third line, where iteration is again performed over each node, this time using the familiar function html_text(). For each of the eight selected nodes, the text between the start and end tag is extracted. Since map_chr() is used here, a character vector is returned as output. pages %&gt;% map(html_node, css = &quot;span.version.version-default&quot;) %&gt;% map_chr(html_text) ## [1] &quot;3.3.2&quot; &quot;1.0.6&quot; &quot;1.1.3&quot; &quot;1.4.0&quot; &quot;0.3.4&quot; ## [6] &quot;3.1.2&quot; &quot;1.4.0.9000&quot; &quot;0.5.1&quot; The extraction of the current version number of the packages works the same way. For ggplot2, these are contained in the following tag: &lt;span class=\"version version-default\" data-toggle=\"tooltip\" data-placement=\"bottom\" title=\"Released version\"&gt;3.3.2&lt;/span&gt;. This can also be selected easily, but we see an interesting detail. Namely, the class name here contains a space. This indicates that the &lt;span&gt; tag carries both the class version and version-default. We can select this by attaching both class names to span in the selector with a . Strictly speaking, however, we do not need to do this here. Both class names by themselves are sufficient for selection, as they do not appear anywhere else on the website. In terms of the most explicit CSS selectors possible, I would still recommend to use both class names, but this is also a matter of taste. pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(1) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ## [1] &quot;https://cloud.r-project.org/package=ggplot2&quot; ## [2] &quot;https://cloud.r-project.org/package=dplyr&quot; ## [3] &quot;https://cloud.r-project.org/package=tidyr&quot; ## [4] &quot;https://cloud.r-project.org/package=readr&quot; ## [5] &quot;https://cloud.r-project.org/package=purrr&quot; ## [6] &quot;https://cloud.r-project.org/package=tibble&quot; ## [7] &quot;https://cloud.r-project.org/package=stringr&quot; ## [8] &quot;https://cloud.r-project.org/package=forcats&quot; pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(4) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ## [1] &quot;https://r4ds.had.co.nz/data-visualisation.html&quot; ## [2] &quot;http://r4ds.had.co.nz/transform.html&quot; ## [3] &quot;https://r4ds.had.co.nz/tidy-data.html&quot; ## [4] &quot;http://r4ds.had.co.nz/data-import.html&quot; ## [5] &quot;http://r4ds.had.co.nz/iteration.html&quot; ## [6] &quot;https://r4ds.had.co.nz/tibbles.html&quot; ## [7] &quot;http://r4ds.had.co.nz/strings.html&quot; ## [8] &quot;http://r4ds.had.co.nz/factors.html&quot; The extraction of the links also follows the same basic principle. The selector is a little more complicated, but can easily be understood using the WDTs. We select the &lt;a&gt; tags of the first and fourth &lt;li&gt; children of the unordered list with the class list-unstyled. Here we apply the function html_attr() with the argument name = \"href\" to each of the eight selected nodes to get the data of interest, the URLs of the links. If we are only interested in the final result, we can also extract the data of the sub-pages directly during the creation of a tibble: tibble( name = pages %&gt;% map(html_node, css = &quot;a.navbar-brand&quot;) %&gt;% map_chr(html_text), version = pages %&gt;% map(html_node, css = &quot;span.version.version-default&quot;) %&gt;% map_chr(html_text), CRAN = pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(1) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;), Learn = pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(4) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ) ## # A tibble: 8 x 4 ## name version CRAN Learn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ggplot2 3.3.2 https://cloud.r-project.org/… https://r4ds.had.co.nz/data-v… ## 2 dplyr 1.0.6 https://cloud.r-project.org/… http://r4ds.had.co.nz/transfo… ## 3 tidyr 1.1.3 https://cloud.r-project.org/… https://r4ds.had.co.nz/tidy-d… ## 4 readr 1.4.0 https://cloud.r-project.org/… http://r4ds.had.co.nz/data-im… ## 5 purrr 0.3.4 https://cloud.r-project.org/… http://r4ds.had.co.nz/iterati… ## 6 tibble 3.1.2 https://cloud.r-project.org/… https://r4ds.had.co.nz/tibble… ## 7 stringr 1.4.0.90… https://cloud.r-project.org/… http://r4ds.had.co.nz/strings… ## 8 forcats 0.5.1 https://cloud.r-project.org/… http://r4ds.had.co.nz/factors… 7.2 Pagination Another common form of dividing a website into several sub-pages is pagination. You all know this from everyday life on the internet. We enter a search term in Google and get results that are divided over several pages. These are accessible and navigable via the numbered links and the forward/back arrows at the bottom of the browser content. This is “pagination in action” and we encounter similar variants on many websites. In the press release archive of the website of the Brandenburg state parliament, pagination is used to distribute the releases over several sub-pages. To illustrate the scraping of such a website, we can, for example, aim at scraping the date, title and further link for all press releases from 2020 and summarise them in a tibble. You can find the website at: https://www.landtag.brandenburg.de/de/aktuelles/presse/pressemitteilungsarchiv/archiv_pressemitteilungen_2020/980521 7.2.1 The query Let’s first look at the page in the browser and examine what happens when we click through the sub-pages. If we select the second sub-page, the URL in the browser window changes. We see that the end of the URL is extended from “980521” to “980521?skip=15”. So a query is sent to the server and the corresponding sub-page is sent back and displayed. On the third sub-page, the query changes again to “980521?skip=30”. What could “skip=15” mean? If we look at the list of press releases, we see that exactly 15 releases are displayed on each sub-page. We can therefore assume that “skip=15” instructs the server to skip the first 15 releases and thus display entries 16-30. “skip=30” then skips the first 30 and so on. The principle can be further confirmed by testing what “skip=0” triggers. The first sub-page is displayed again. So “980521” is actually functionally equivalent to “980521?skip=0”. With this we already know that we will be able to manipulate the URLs directly from Rstudio and thus scrape the sub-pages. 7.2.2 Scraping the sub-pages Before we start scraping all the press releases, we first need to find out how many sub-pages there are. The easiest way would be to do this by eye. We can see in the browser that the highest selectable sub-page is “12”. But we can also find this out in the scraping process itself. This has several advantages. We might not only want to scrap the messages from 2020, but those from several or all years. To do this, we would have to check for each year in the browser how many sub-pages there are and adjust this accordingly. If we extract the page number in the R code, this can be easily generalised to other years with different page numbers. If, on the other hand, the goal were to scrape the messages from 2021, we would have to check again whether the page number has changed every time we repeat the process over the course of the year, as new press releases are added. By extracting it in the code, this step is eliminated and we can run the script regularly with minimal effort and be sure that it remains functional. The links to the sub-pages, are contained in the HTML code in an unordered list (&lt;ul&gt;). The penultimate list element &lt;li&gt; contains the page number of the last page, here “12”. Please note that the last list element is the “Forward” button. With this information we can construct a selector and extract the highest page number, as the second to last list element. website &lt;- &quot;https://www.landtag.brandenburg.de/de/aktuelles/presse/pressemitteilungsarchiv/archiv_pressemitteilungen_2020/980521&quot; %&gt;% read_html() max &lt;- website %&gt;% html_node(css = &quot;ul.pagination.pagination-sm &gt; li:nth-last-child(2)&quot;) %&gt;% html_text() %&gt;% as.numeric() In the last line of code above, you again see the function as.numeric(). Remember, that html_text() always returns a character vector. Since we need a numeric value to be able to calculate with it in R, we have to convert it into a number. Now we can start constructing the links to all sub-pages directly in our R script. To do this, we need two components that we can then combine to create the URLs. First we have to define the constant part of the URL, the base URL. In this case, this is the complete URL up to “?skip=” inclusive. In addition, we need the values that are inserted after “?skip=”. We can easily calculate these. Each sub-page contains 15 press releases. So we can multiply the number of the sub-page by 15, but then we have to subtract another 15, because the 15 press releases shown on the current sub-page should not be skipped. So for page 1 we calculate: \\(1 ∗ 15 - 15 = 0\\), for page 2: \\(2 ∗ 15 - 15 = 15\\) and so on. To do this in one step for all sub-pages, we can use 1:max * 15 - 15 to instruct R to repeat the calculation for all numbers from 1 to the maximum value – which we previously stored in the object max. : stands for “from-to”. In this way we get a numeric vector with the values for “?skip=”. In the third step we can combine the base URL and the calculated values with str_c() to complete URLs and parse them in the fourth step with map(). base_url &lt;- &quot;https://www.landtag.brandenburg.de/de/aktuelles/presse/pressemitteilungsarchiv/archiv_pressemitteilungen_2020/980521?skip=&quot; skips &lt;- 1:max * 15 - 15 skips ## [1] 0 15 30 45 60 75 90 105 120 135 150 165 links &lt;- str_c(base_url, skips) pages &lt;- links %&gt;% map(read_html) Now we can extract the data we are interested in. Let’s start with the date of the press release. This is enclosed by a &lt;p&gt; tag of the class date. With map() we first select the corresponding nodes and in the next step extract the text of these nodes. Since at this point a list of lists is returned, and this would be unnecessarily complicated in further processing, we can use unlist() to receive a character vector as output. pages %&gt;% map(html_nodes, css = &quot;p.date&quot;) %&gt;% map(html_text) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;30.12.2020&quot; &quot;17.12.2020&quot; &quot;11.12.2020&quot; &quot;10.12.2020&quot; &quot;04.12.2020&quot; However, we can go one step further and store the data as a vector of the type “Date”. This could be advantageous for potential further analyses. The tidyverse package lubridate makes it easy to convert dates from character or numeric vectors into the “Date” format. The package is not part of the core tidyverse and has to be loaded explicitly. Among other things, it offers a number of functions in the form dmy(), mdy(), ymd() and so on. d stands for “day”, m for “month” and y for “year”. With the order in which the letters appear in the function name, we tell R which format the data we want to convert to the “date” format has. On the website of the Brandenburg state parliament, the dates are written in the form Day.Month.Year, which is typical in Germany. So we use the function dmy(). If, for example, they were in the form Month.Day.Year, which is typical in the USA, we would have to use mdy() accordingly. It is irrelevant whether the components of the date are separated with “.”, “/”, “-” or spaces. Even written out or abbreviated month names can be processed by lubridate. library(lubridate) pages %&gt;% map(html_nodes, css = &quot;p.date&quot;) %&gt;% map(html_text) %&gt;% unlist() %&gt;% dmy() %&gt;% head(n = 5) ## [1] &quot;2020-12-30&quot; &quot;2020-12-17&quot; &quot;2020-12-11&quot; &quot;2020-12-10&quot; &quot;2020-12-04&quot; More about the handling of dates and times in R as well as the further possibilities opened up by lubridate, can be found in the corresponding chapter in “R for Data Science”: https://r4ds.had.co.nz/dates-and-times.html Next, we can extract the titles of the press releases. By now you will be able to understand the code and the CSS selector for this yourself. pages %&gt;% map(html_nodes, css = &quot;p.result-name&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;Zum Tode von Paul-Heinz Dittrich&quot; ## [2] &quot;Symbol für Zusammenhalt auch in schwierigen Zeiten: Parlament und Regierung erhalten Fotocollage zu Erntekronen&quot; ## [3] &quot;Termine des Landtages Brandenburg in der Zeit vom 12. bis 20. Dezember 2020&quot; ## [4] &quot;Hinweise für Medien zu den Plenarsitzungen des Landtages vom 15. bis 18. Dezember 2020&quot; ## [5] &quot;Termine des Landtages Brandenburg in der Zeit vom 7. bis 13. Dezember 2020&quot; The last thing to extract, are the links to the individual messages. Here, too, nothing surprising happens at first. pages %&gt;% map(html_nodes, css = &quot;li.ce.ce-teaser &gt; a&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;/de/meldungenzum_tode_von_paul-heinz_dittrich/980480?_referer=980521&quot; ## [2] &quot;/de/meldungensymbol_fuer_zusammenhalt_auch_in_schwierigen_zeiten:_parlament_und_regierung_erhalten_fotocollage_zu_erntekronen/979730?_referer=980521&quot; ## [3] &quot;/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_12._bis_20._dezember_2020/978690?_referer=980521&quot; ## [4] &quot;/de/meldungenhinweise_fuer_medien_zu_den_plenarsitzungen_des_landtages_vom_15._bis_18._dezember_2020/976366?_referer=980521&quot; ## [5] &quot;/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_7._bis_13._dezember_2020/974725?_referer=980521&quot; But, the links stored in the HTML code only describe a part of the complete URL. We could now construct the complete URLs again with str_c(). However, we still need a new concept for this. The pipe passes the result of a step along to the next line. If we use str_c() within the pipe, it receives the extracted end part of the URLs as the first argument. str_c(\"https://www.landtag.brandenburg.de\") would therefore lead to the end part of the URL being appended before “https://www.landtag.brandenburg.de”. However, we want this to happen the other way round. To do this, we need to tell str_c(), to use the data passed through the pipe as the second argument. We can achieve this by using .. . refers to the data passed through the pipe. In this way we can combine the URLs correctly: pages %&gt;% map(html_nodes, css = &quot;li.ce.ce-teaser &gt; a&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% str_c(&quot;https://www.landtag.brandenburg.de&quot;, .) %&gt;% head(n = 5) ## [1] &quot;https://www.landtag.brandenburg.de/de/meldungenzum_tode_von_paul-heinz_dittrich/980480?_referer=980521&quot; ## [2] &quot;https://www.landtag.brandenburg.de/de/meldungensymbol_fuer_zusammenhalt_auch_in_schwierigen_zeiten:_parlament_und_regierung_erhalten_fotocollage_zu_erntekronen/979730?_referer=980521&quot; ## [3] &quot;https://www.landtag.brandenburg.de/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_12._bis_20._dezember_2020/978690?_referer=980521&quot; ## [4] &quot;https://www.landtag.brandenburg.de/de/meldungenhinweise_fuer_medien_zu_den_plenarsitzungen_des_landtages_vom_15._bis_18._dezember_2020/976366?_referer=980521&quot; ## [5] &quot;https://www.landtag.brandenburg.de/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_7._bis_13._dezember_2020/974725?_referer=980521&quot; As always, we can perform the complete extraction of the data during the construction of a tibble: tibble( date = pages %&gt;% map(html_nodes, css = &quot;p.date&quot;) %&gt;% map(html_text) %&gt;% unlist() %&gt;% dmy(), name = pages %&gt;% map(html_nodes, css = &quot;p.result-name&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist(), link = pages %&gt;% map(html_nodes, css = &quot;li.ce.ce-teaser &gt; a&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% str_c(&quot;https://www.landtag.brandenburg.de&quot;, .) ) ## # A tibble: 175 x 3 ## date name link ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2020-12-30 Zum Tode von Paul-Heinz Dittrich https://www.landtag.brandenburg… ## 2 2020-12-17 Symbol für Zusammenhalt auch in … https://www.landtag.brandenburg… ## 3 2020-12-11 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## 4 2020-12-10 Hinweise für Medien zu den Plena… https://www.landtag.brandenburg… ## 5 2020-12-04 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## 6 2020-11-27 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## 7 2020-11-25 UN-Women-Flagge weht im Innenhof… https://www.landtag.brandenburg… ## 8 2020-11-23 Hinweise für Medien zur Sondersi… https://www.landtag.brandenburg… ## 9 2020-11-23 Attikafiguren von Perseus und An… https://www.landtag.brandenburg… ## 10 2020-11-20 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## # … with 165 more rows "],["files.html", "8 Downloading and saving files 8.1 CSV files 8.2 Downloading files 8.3 Saving data files", " 8 Downloading and saving files WIP! Content is due to change! In this section we will concern ourselves with downloading files from within our R scripts as well as saving the scraping results locally for reuse without the need to scrape the data again. As the examples below will show the download of CSV files and saving is also often done as CSV, but more on this in the last subsection, we should at first talk briefly about what CSV files are and how we can use them in R. 8.1 CSV files One of the more widespread types of data files we will encounter on the internet, are CSV files – comma-separated values. While they are limited to the display of two-dimensional tables, their simplicity and portability makes them one of the most used formats for this type of data. You can open and write CSV files with statistical software, spreadsheet software like Excel and with even the most basic text editor. A simple CSV file may look like this: column1, column2, column3 data1_1, data1_2, data1_3 data2_1, data2_2, data2_3 data3_1, data3_2, data3_3 In essence, CSV files are tables. The rows of the table are delimited by line breaks, the columns by commas. Often the first row represents the column names, as in the example above (column1, column2, column3). Be aware, that this is not necessarily always the case. A CSV file may not contain any column names at all, and start with a first row of data cells. This is a vital difference when it comes to parsing the file. The row of column names, if present, also ends with a line break. 8.1.1 Parsing a CSV file To convert the data contained in the CSV file into a format we can use for data analysis we have to parse it first. The function used for parsing “understands” the representation of a table in the CSV syntax, can discern column names from data cells, assigns data cells to the correct rows and columns and returns an R object representing the table. read.csv() from base R returns a data frame while read_csv() from the readr package, which we will use here, returns a tibble. As readr is part of the core tidyverse we can just load the tidyverse package. We will also need rvest soon, so let us begin with loading both libraries. library(tidyverse) library(rvest) The readr function read_csv() takes a CSV file as it’s first argument. To test it, we can also use literal data for the file argument. So we can pass the example defined above as a string – contained in \" – as the fileargument. We won’t do this much in practice, but it serves as a good first example. read_csv( &quot;column1, column2, column3 data1_1, data1_2, data1_3 data2_1, data2_2, data2_3 data3_1, data3_2, data3_3&quot; ) ## # A tibble: 3 x 3 ## column1 column2 column3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 data1_1 data1_2 data1_3 ## 2 data2_1 data2_2 data2_3 ## 3 data3_1 data3_2 data3_3 read_csv() guesses the column types by the data entered. Here all three columns were automatically defined as character vectors. Automatic guessing works reliably in most situations. Let’s try a CSV with multiple different column types: read_csv( &quot;name, age, size, retired Peter, 42, 1.68, FALSE Paul, 84, 1.82, TRUE Mary, 24, 1.74, FALSE&quot; ) ## # A tibble: 3 x 4 ## name age size retired ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Peter 42 1.68 FALSE ## 2 Paul 84 1.82 TRUE ## 3 Mary 24 1.74 FALSE read_csv() correctly guessed that the first column is of type character, the third of the type double and the fourth is a logical vector. It also guessed that the second column should be of the type double, i.e. floating point numbers. This will do, but we could redefine this vector as integer manually by using the col_types argument. This will not be necessary in most situations but could increase computational speed with very large datasets, as integers can be saved more efficiently. More on the definition of column types can be found in the help file. What happens when we have CSV data without column names defined in the first line? read_csv( &quot;Peter, 42, 1.68, FALSE Paul, 84, 1.82, TRUE Mary, 24, 1.74, FALSE&quot; ) ## # A tibble: 2 x 4 ## Peter `42` `1.68` `FALSE` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Paul 84 1.82 TRUE ## 2 Mary 24 1.74 FALSE read_csv() can not by itself discern if the authors of the CSV file intended the first line to be used for names or not. If there are no column names in the CSV data, as in this case, we have to explicitly tell read_csv() not to use the first line for column names We can do this by specifying the col_names() argument as FALSE, instead of the default TRUE. In this case, read_csv() chooses default names for the columns, which we can change later on. read_csv( &quot;Peter, 42, 1.68, FALSE Paul, 84, 1.82, TRUE Mary, 24, 1.74, FALSE&quot;, col_names = FALSE) ## # A tibble: 3 x 4 ## X1 X2 X3 X4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Peter 42 1.68 FALSE ## 2 Paul 84 1.82 TRUE ## 3 Mary 24 1.74 FALSE 8.2 Downloading files Let us look at some real CSV files to download and parse. The website https://www.bundeswahlleiter.de/bundestagswahlen/2017/ergebnisse/repraesentative-wahlstatistik.html holds multiple CSV files containing inferential statistics on the German federal election held in 2017. Among them, two files with statistics on the number of people eligible to vote and on the number that actually voted, both by binary gender and year of birth as well as additional indicators. Your first instinct could be, to download the files manually and then parse them. This will work just fine but we could also download the files directly from our code. Due to the small number of files in this example, this may not even be the most efficient way, but when you start handling larger amounts of files or want to regularly update files, downloading them from within your code will be the safer and more efficient way. Firstly, we should download the HTML file containing the links to the CSV files and extract the first two links following the header “Tabellen zur Weiterverwendung” using the tools we already have at our disposal. To understand the construction of the css selector, please have another look into chapter 5.1. Note that this is only one of many possible selectors that can achieve the goal of selecting the two links of interest. website &lt;- &quot;https://www.bundeswahlleiter.de/bundestagswahlen/2017/ergebnisse/repraesentative-wahlstatistik.html&quot; %&gt;% read_html() a_nodes &lt;- website %&gt;% html_nodes(css = &quot;ul[style=&#39;list-style-type:none&#39;] &gt; li:first-child &gt; a, ul[style=&#39;list-style-type:none&#39;] &gt; li:nth-child(2) &gt; a&quot;) a_nodes ## {xml_nodeset (2)} ## [1] &lt;a data-file-extension=&quot;CSV&quot; data-file-size=&quot;40,53 kB&quot; title=&quot;Link zur CS ... ## [2] &lt;a data-file-extension=&quot;CSV&quot; data-file-size=&quot;68,82 kB&quot; title=&quot;Link zur CS ... links &lt;- a_nodes %&gt;% html_attr(name = &quot;href&quot;) links ## [1] &quot;../../../dam/jcr/38972966-dc3d-40fa-91d7-6599d913f5e9/btw17_rws_bw2.csv&quot; ## [2] &quot;../../../dam/jcr/a67208c0-2a2b-41aa-abb8-204b09e73b6b/btw17_rws_bst2.csv&quot; We achieved our goal of extracting the links, but we also see, that these are not complete as they are missing their base URL. This works on the website, as the link is relative. To access the files directly, we need an absolute link – the full path – though. As before (sub section 7.2.2) we can use the function str_c() to construct the complete URLs. If we give str_c() a vector as one of it’s input arguments, the function will repeat the string connection for each element in the vector. For this example we can ignore the “../../../” part of the link as it will work regardless. To remove the part we could apply some of the functions from the stringr package. For this we would need some basic knowledge in string manipulation and regular expressions, which we will gain in chapter 12. So for now, we will just use the links as they come. links &lt;- str_c(&quot;https://www.bundeswahlleiter.de/&quot;, links) links ## [1] &quot;https://www.bundeswahlleiter.de/../../../dam/jcr/38972966-dc3d-40fa-91d7-6599d913f5e9/btw17_rws_bw2.csv&quot; ## [2] &quot;https://www.bundeswahlleiter.de/../../../dam/jcr/a67208c0-2a2b-41aa-abb8-204b09e73b6b/btw17_rws_bst2.csv&quot; Now that we we have complete absolute links, we can download the files to our hard drive. To achieve this, we can use the base R function download.file(). We have to specify an URL as it’s first argument as well as a path and file name for the destfile argument. When no path is given, the file is saved in the directory where your R script is located. download.file(links[1], destfile = &quot;eligible.csv&quot;) download.file(links[2], destfile = &quot;turnout.csv&quot;) We could also have used read_csv() to parse the CSV files directly from the web. Still, sometimes it may be a good idea to download the files to our hard drives, as we only have to do this once and thus decrease traffic for the server and also increase the efficiency of our code. But this is only a benefit, if we run the download once and not every time we rerun our script. So you should maybe comment out the lines where the download occurs after it was successful. In chapter (good_practice) we will discuss a way to do this more elegantly. 8.2.1 Parsing the csv files Now we can parse the downloaded CSV files into R objects. But first let’s have a look at the CSV files. You can open them in a text editor of your choosing or directly in RStudio. Importing them into Excel or a similar spreadsheet software is not a valid option here, as we want to see the raw contents of the file and not a representation of the data. Looking at eligible.csv we notice at least two things. Firstly, the delimiter used here is not a comma but a semicolon. The default CSV style discussed above is common in those countries where a “.” is used as the decimal point. Thus the “,” is available to be used as a delimiter. In countries where the “,” is used as a decimal point, it is not available and thus the “;” is used as a delimiter. The function read_csv2() can be used in this case. Even more general is read_delim() as the delimiting character can be defined as an argument. Secondly, we see that the first 8 lines are neither column names or data but contain comments on the CSV file itself. We have to tell read_csv2() to ignore these lines. We could use the argument skip to tell R to ignore the first 8 lines. But as these comments are neatly introduced with an # we can use the comment argument instead and specify the # as the identifier for comments, which then will be ignored. eligible_tbl &lt;- read_csv2(&quot;eligible.csv&quot;, comment = &quot;#&quot;) ## ℹ Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use `read_delim()` for more control. ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Land = col_character(), ## Geschlecht = col_character(), ## Geburtsjahresgruppe = col_character(), ## Wahlberechtigte = col_double(), ## `Wahlberechtigte ohne Wahlscheinvermerk` = col_double(), ## `Wahlberechtigte mit Wahlscheinvermerk` = col_double(), ## `Wähler/-innen` = col_double(), ## `Wähler/-innen ohne Wahlschein` = col_double(), ## `Wähler/-innen mit Wahlschein` = col_double(), ## Wahlbeteiligung = col_double() ## ) eligible_tbl ## # A tibble: 627 x 10 ## Land Geschlecht Geburtsjahresgr… Wahlberechtigte `Wahlberechtigt… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bund Summe Summe 61688485 47595800 ## 2 Bund Summe 1997 - 1999 2045501 1709333 ## 3 Bund Summe 1993 - 1996 2902998 2295723 ## 4 Bund Summe 1988 - 1992 4206014 3311417 ## 5 Bund Summe 1983 - 1987 4302277 3455795 ## 6 Bund Summe 1978 - 1982 4264347 3490155 ## 7 Bund Summe 1973 - 1977 3953481 3273575 ## 8 Bund Summe 1968 - 1972 5230865 4232089 ## 9 Bund Summe 1958 - 1967 12396299 9581842 ## 10 Bund Summe 1948 - 1957 9487267 6950185 ## # … with 617 more rows, and 5 more variables: `Wahlberechtigte mit ## # Wahlscheinvermerk` &lt;dbl&gt;, `Wähler/-innen` &lt;dbl&gt;, `Wähler/-innen ohne ## # Wahlschein` &lt;dbl&gt;, `Wähler/-innen mit Wahlschein` &lt;dbl&gt;, ## # Wahlbeteiligung &lt;dbl&gt; turnout_tbl &lt;- read_csv2(&quot;turnout.csv&quot;, comment = &quot;#&quot;) ## ℹ Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use `read_delim()` for more control. ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Land = col_character(), ## `Erst-/Zweitstimme` = col_double(), ## Geschlecht = col_character(), ## Geburtsjahresgruppe = col_character(), ## Summe = col_double(), ## Ungültig = col_double(), ## CDU = col_double(), ## SPD = col_double(), ## `DIE LINKE` = col_double(), ## GRÜNE = col_double(), ## CSU = col_double(), ## FDP = col_double(), ## AfD = col_double(), ## Sonstige = col_double(), ## `dar. NPD` = col_double(), ## `dar. FREIE WÄHLER` = col_double() ## ) turnout_tbl ## # A tibble: 798 x 16 ## Land `Erst-/Zweitsti… Geschlecht Geburtsjahresgr… Summe Ungültig CDU ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bund 1 Summe Summe 4.70e7 586726 1.40e7 ## 2 Bund 1 Summe 1993 - 1999 3.37e6 37704 8.19e5 ## 3 Bund 1 Summe 1983 - 1992 6.04e6 58687 1.51e6 ## 4 Bund 1 Summe 1973 - 1982 6.23e6 62068 1.74e6 ## 5 Bund 1 Summe 1958 - 1972 1.39e7 146749 3.81e6 ## 6 Bund 1 Summe 1948 - 1957 7.69e6 103820 2.31e6 ## 7 Bund 1 Summe 1947 und früher 9.73e6 177697 3.84e6 ## 8 Bund 1 m Summe 2.28e7 274037 6.30e6 ## 9 Bund 1 m 1993 - 1999 1.69e6 20330 4.02e5 ## 10 Bund 1 m 1983 - 1992 3.00e6 29800 7.07e5 ## # … with 788 more rows, and 9 more variables: SPD &lt;dbl&gt;, `DIE LINKE` &lt;dbl&gt;, ## # GRÜNE &lt;dbl&gt;, CSU &lt;dbl&gt;, FDP &lt;dbl&gt;, AfD &lt;dbl&gt;, Sonstige &lt;dbl&gt;, `dar. ## # NPD` &lt;dbl&gt;, `dar. FREIE WÄHLER` &lt;dbl&gt; 8.3 Saving data files In my opinion, the best approach to data analysis in terms of reproducibility is to have one script in which you load your raw data, do all your data cleaning, transform your data, do all statistical and graphical analysis and also all exports of tables and graphics. This ensures that anyone who has access to your script and the raw data can reproduce every step you undertook in your analysis, one of the cornerstones of transparency and reproducibility of scientific works. From this angle, we seldom need to save the results of our data analysis in a file. The script – the .R file – is enough. Nevertheless, there are situations in which saving results becomes reasonable or even necessary. One of these is Web Scraping, for at least two reasons. Firstly, we should try to minimize the traffic we create on the websites we scrape (see @ref(good_practice)). If we re-run the read_html() function every time we run our script, we also re-download all of the HTML data in each re-run. This puts unnecessary load on the servers and also slows down our script, which becomes more relevant the more data you are scraping from the web. Secondly, websites change all the time. The data you scraped today me be changed or even gone tomorrow. Also if the structure of the site changes, you CSS selectors and URLs may not work anymore. So for both reasons, it may be appropriate to save the all of our downloaded HTML data locally, so we can reload it for re-runs of our script and to archive the point in time we downloaded the data we used for our analysis. The most straightforward approach could be to use the download.file() function directly on the URL and thus copy the original HTML data exactly. download.file(&quot;https://webscraping-tures.github.io/hello_world.html&quot;, &quot;hello_world_local.html&quot;) We could run this line once and after this just use read_html() on the local file. hello_world &lt;- read_html(&quot;hello_world_local.html&quot;) In most situations we don’t need to save the results of our transformations and analysis. Re-running the script on the original data we have now saved locally is enough to ensure that the work can be reproduced. But if we are still interested in saving the results of our analysis to a local file, we could use write_csv() from stringr or save the data in the native R format .RData. Let’s create an example object we want to save first: data &lt;- tibble( name = c(&quot;Peter&quot;, &quot;Paul&quot;, &quot;Mary&quot;), age = c(42, 84, 24), size = c(1.68, 1.82, 1.74), retired = c(FALSE, TRUE, FALSE) ) write_csv() is an appropriate format for saving tibbles or data frames and has the advantage of being readable by most software that deals with data, not just by R. It takes the object to be saved as it’s first argument and the path and file name used for saving as its second. write_csv(data, &quot;peter_paul_mary.csv&quot;) The resulting file looks like this: name,age,size,retired Peter,42,1.68,FALSE Paul,84,1.82,TRUE Mary,24,1.74,FALSE If we want semicolons as a delimiter we could also use write_csv2(). To load the resulting file we would just use read_csv() or read_csv2(). The downside to using CSV files is that the definition of column types – e.g. character, numeric or integer – is lost and may have to be redefined when reloading the data. Also more complex objects like lists, can not be saved as a CSV. So we may use the data format native to R, .RData. The save() function also takes the name of the object – or multiple objects – to be saved as its first argument. We also have to specify a file name (and possibly path) in the file = argument. save(data, file = &quot;peter_paul_mary.RData&quot;) We can load objects stored as .RData files by using load() on this file. load(&quot;peter_paul_mary.RData&quot;) Scarping code should still be part of the script, commented out "],["good-practice.html", "9 Good practice 9.1 Legality? 9.2 Responsibility! 9.3 Good practice", " 9 Good practice WIP! Content is due to change! 9.1 Legality? its grey german case? 9.2 Responsibility! its a privilege and we have to treat it this way to remain a privilege robots.txt when in doubt, contact the admins or get legal advice is possible think about what you really need test your code on a small scale 9.3 Good practice follow robots.txt download only what you need and only once (if possible) example for checking if file is already present set waiting times "],["dplyr.html", "10 Transformation with dplyr 10.1 filter() 10.2 select() 10.3 rename() 10.4 mutate() 10.5 summarise() &amp; group_by()", " 10 Transformation with dplyr WIP! Content is due to change! At this point you should have a look at both tibbles either by clicking on them in the environment tab or by writing View(eligible_tbl) and View(turnout_tbl). A quick sidenote: You will notice, that most of the values we will compute in this chapter are already present in eligible.csv. We will soon drop those and compute them ourselves for the sake of having a good introductory example into data transformation. In a real world scenario we would most probably just use the values for turnout percentages etc. already present in the data. In the following, we will use several functions from the tidyverse package dplyr to filter observations, select columns and add new columns as computations of existing ones. 10.1 filter() As a first step, we should limit our tibbles to the observations we actually need. For this example we should limit ourselves to an analysis on the federal level. Thus we can drop all observations referring to the individual states. We will also drop the totals for the gender and year of birth columns and compute them ourselves later on. To filter on the observations we need and drop the remainder, we can use the function filter(). It takes the tibble or data frame to be filtered as it’s first argument – which we don’t need to specify when using the pipe – and one or multiple expressions that specify which observations are to be filtered. To filter only those observations that have the value “Bund” for the column “Land”, we can write: eligible_tbl %&gt;% filter(Land == &quot;Bund&quot;) Thus only the observations for “Bund” remain in the tibble. Or in other words, those observations for whom the expression Land == \"Bund\" is returned as TRUE. Besides ==, all other comparison operators can be used and we can chain multiple expressions to be filtered upon in one call of filter(). So, to filter on those observations who refer to the “Bund” and don’t have a sum for the gender and year of birth columns, we can write: eligible_tbl %&gt;% filter(Land == &quot;Bund&quot;, Geschlecht != &quot;Summe&quot;, Geburtsjahresgruppe != &quot;Summe&quot;) We can also use logical operators – &amp; for “and”, | for “or” – when combining expressions to be filtered upon. So instead of Geschlecht = \"Summe\" another approach would be to filter those that have “m” or “w” as values. To demonstrate this, let us filter the object turnout_tblas well. Here we have an additional column we have to filter for called “Erst-/Zweitstimme”. I propose we only analyse the party votes. turnout_tbl %&gt;% filter(Land == &quot;Bund&quot;, `Erst-/Zweitstimme` == 2, Geschlecht == &quot;m&quot; | Geschlecht == &quot;w&quot;, Geburtsjahresgruppe != &quot;Summe&quot;) When we create rather long function calls with many arguments, it is often a good idea to insert some line breaks to keep your code easily readable, as seen above. Let’s have another look at this line: `Erst-/Zweitstimme` == 2,. You will notice, that the name of the column is enclosed by backticks `. In general, object and variable (ie. column) names in R can only contain letters, numbers, points and underscores and have to start with a letter. It is possible to have names that violate those rules, but these have to enclosed in backticks. As you can see, “Erst-/Zweitstimme” contains two characters that are not allowed in R names. turnout_tbl also contains the column “DIE LINKE”. The usage of whitespace in names is also not allowed. So to address this column in our code, we would have to write DIE LINKE. 10.2 select() We should also limit our tibbles to the set of columns we will actually use. Looking at eligible_tbl, we will only need the first four columns. We can use the function select() to “select” the columns we need by name. eligible_tbl %&gt;% select(Land, Geschlecht, Geburtsjahresgruppe, Wahlberechtigte) If, as in this case, the columns we want so select follow each other in their positions in the tibble, we can also use a shorter “from:to” notation, that saves us somy typing. eligible_tbl %&gt;% select(Land:Wahlberechtigte) At this point we can combine the filtering of rows and selection of columns in the pipe and assign the result to a new object. But first, let us think about again about which columns we will actually need. Above we selected the first four columns from turnout_tbl. But we do not actually need the column “Land” after we filtered for the rows containing the federal state level as it contains no useful information anymore. So we can drop the column, if we filter first. Order matters! For turnout_tbl this is also true but extends to the column “Erst-/Zweitstimme” which also only contains one value after filtering. eligible_tbl &lt;- eligible_tbl %&gt;% filter(Land == &quot;Bund&quot;, Geschlecht != &quot;Summe&quot;, Geburtsjahresgruppe != &quot;Summe&quot;) %&gt;% select(Geschlecht:Wahlberechtigte) turnout_tbl &lt;- turnout_tbl %&gt;% filter(Land == &quot;Bund&quot;, `Erst-/Zweitstimme` == 2, Geschlecht == &quot;m&quot; | Geschlecht == &quot;w&quot;, Geburtsjahresgruppe != &quot;Summe&quot;) %&gt;% select(Geschlecht:Summe) 10.3 rename() As we are working in english, it makes sense to also rename the columns using english wordings. Also, we have to write down the column names several time during analysis, so we should use names that are short but concise to limit unnecessary typing while maintaining recognisability. Imagine writing “Geburtsjahresgruppe” several times in your code, and you will know why renaming is approriate here. Also the column “Summe” in turnout_tbl does not describe what the values stored in it refer to. We should change this. One approach to renaming is using the function rename() from dplyr. The function takes the data to applied to as its first argument – passed by the pipe in this case – followed by one or more arguments in the form new_name = old_name. eligible_tbl &lt;- eligible_tbl %&gt;% rename(gender = Geschlecht, ybirth = Geburtsjahresgruppe, eligible = Wahlberechtigte) eligible_tbl turnout_tbl &lt;- turnout_tbl %&gt;% rename(gender = Geschlecht, ybirth = Geburtsjahresgruppe, turnout = Summe) turnout_tbl 10.4 mutate() We will now leave our running example for a short time and look at how to compute new variables in a tibble from existing ones, using mutate() from dplyr. For this purpose, let us create a new simple tibble on voter turnout. Note that the entered data is purely illustrational and has no meaning. We will return to the real data shortly. exmpl_tbl &lt;- tibble( gender = c(&quot;m&quot;, &quot;m&quot;, &quot;w&quot;, &quot;w&quot;), ybirth = c(&quot;1993 - 1999&quot;, &quot;1983 - 1992&quot;, &quot;1993 - 1999&quot;, &quot;1983 - 1992&quot;), eligible = c(100000, 100000, 100000, 100000), turnout_person = c(40000, 50000, 60000, 70000), turnout_mail = c (35000, 30000, 25000, 20000) ) exmpl_tbl In this hypothetical data, we have different columns for people who voted in person and who voted by mail. We do not care about this difference in voting method in this example and want one column that combines both. We can achieve this by using mutate(). The function takes the data to be manipulated as its first argument, followed by one or multiple arguments defining the new columns to be created. We can create this new columns as computations involving the columns already present. To calculate the total turnout we could write: exmpl_tbl %&gt;% mutate(turnout = turnout_person + turnout_mail) We can also immediately start calculating with new columns in the same pipe. So to calculate the turnout percentage: exmpl_tbl %&gt;% mutate( turnout = turnout_person + turnout_mail, turnout_pct = turnout / eligible ) As we no longer need turnout_person and turnout_mail we could drop them by using select() in a last step. Instead of telling select() which columns we want to keep, we can also tell it which noot to keep by adding a - before the column names. exmpl_tbl &lt;- exmpl_tbl %&gt;% mutate( turnout = turnout_person + turnout_mail, turnout_pct = turnout / eligible ) %&gt;% select(-turnout_person, -turnout_mail) exmpl_tbl Note that mutate() is not limited to basic arithmetic operations. Many functions can be applied within mutate(), for example sum() or mean() to only name a few examples we already know. 10.5 summarise() &amp; group_by() summarise() can be used to calculate summary statistics for a whole tibble. The syntax is similar to mutate(). Let us compute the total number of persons eligible to vote: exmpl_tbl %&gt;% summarise(eligible_total = sum(eligible)) As you can see, the result is a new tibble, containing only the summary statistics we requested. We also can compute multiple summary statistics in one step. Please note, that the overall mean for turnout percentage can be computed as the mean of the turnout percentages per group in this case only because all four groups are exactly of the same size (\\(n = 100000\\)). We will later look into computing the same measure with our real data. exmpl_tbl %&gt;% summarise( eligible_total = sum(eligible), turnout_total = sum(turnout), turnout_pct_mean = mean(turnout_pct) ) Often we are interested in summaries grouped by the value of one or more other variables. We might be interested in computing these summary statistics not for the complete tibble but by gender. For this purpose, we can group the data by the gender column using group_by() and then compute the summary statistics separately for each group. As always with dplyr functions, group_by() needs the data that shall be grouped as the first argument, followed by one or multiple variables to group by. exmpl_tbl %&gt;% group_by(gender) %&gt;% summarise( eligible_total = sum(eligible), turnout_total = sum(turnout), turnout_pct_mean = mean(turnout_pct) ) Let us create a final example, with two variables to group by. Let us assume we have the turnout percentages by gender and age group for two elections. We also again assume the same group size, i.e. the same “n”, for all observations. exmpl_tbl_2 &lt;- tibble( election = c(1, 1, 1, 1, 2, 2, 2, 2), gender = c(&quot;m&quot;, &quot;m&quot;, &quot;w&quot;, &quot;w&quot;, &quot;m&quot;, &quot;m&quot;, &quot;w&quot;, &quot;w&quot;), ybirth = c(&quot;1993 - 1999&quot;, &quot;1983 - 1992&quot;, &quot;1993 - 1999&quot;, &quot;1983 - 1992&quot;, &quot;1993 - 1999&quot;, &quot;1983 - 1992&quot;, &quot;1993 - 1999&quot;, &quot;1983 - 1992&quot;), turnout_pct = c(0.6, 0.55, 0.75, 0.625, 0.7, 0.65, 0.85, 0.725) ) exmpl_tbl_2 To compute the mean turnout percentages by gender for each election separately, we can group by election and gender. exmpl_tbl_2 %&gt;% group_by(election, gender) %&gt;% summarise( turnout_pct_mean = mean(turnout_pct) ) There is a peculiarity to the way group_by() works with summarise() that can cause headaches, if we are not aware of it. In general any summarise() function following a group_by() will calculate the summary statistic and then remove one level of grouping. In the examples where we only had one level of grouping, this essentially meant, that the data was ungrouped after summarise(). In the last example we had two levels of grouping. So after the computation of the mean turnout percentage by election and gender, the grouping by gender was removed, but the grouping by election remained in effect. We can see this in the output, where R informs us about the column by which the data is grouped and the number of groups in the output: ## # Groups: election [2]. Another summarise() function would compute the statistic by election and then remove this level also. We can also use ungroup() to remove all grouping from a tibble. In the case of this example, this does not make a practical difference as we only compute the summary and move on. If we assign the results of a summary to an object for later use in data analysis, we have to think about removing groups though. "],["ggplot.html", "11 Graphical analysis with ggplot 11.1 ggplot syntax 11.2 Aesthtetics 11.3 Tuning the output", " 11 Graphical analysis with ggplot WIP! Content is due to change! 11.1 ggplot syntax 11.2 Aesthtetics 11.2.1 Continuous 11.2.2 Continuous x Continuous 11.2.3 Discrete x Continuous 11.3 Tuning the output ggplot(data = data) + geom_point(aes(x = eligible, y = turnout)) + geom_line(aes(x = eligible, y = turnout)) + geom_smooth(aes(x = eligible, y = turnout)) data %&gt;% group_by(gender) %&gt;% summarise( eligible_total = sum(eligible), turnout_total = sum(turnout), turnout_pct_mean = weighted.mean(turnout_pct, w = eligible) ) %&gt;% ggplot() + geom_col(aes(x = gender, y = turnout_pct_mean)) data %&gt;% group_by(ybirth) %&gt;% summarise( eligible_total = sum(eligible), turnout_total = sum(turnout), turnout_pct_mean = weighted.mean(turnout_pct, w = eligible) ) %&gt;% ggplot() + geom_col(aes(x = ybirth, y = turnout_pct_mean)) overall_turnout_mean &lt;- data %&gt;% summarise( turnout_pct_mean = weighted.mean(turnout_pct, w = eligible) ) %&gt;% unlist() data %&gt;% group_by(ybirth) %&gt;% summarise( turnout_pct_mean = weighted.mean(turnout_pct, w = eligible) ) %&gt;% ggplot() + geom_col(aes(x = ybirth, y = turnout_pct_mean)) + geom_hline(yintercept = overall_turnout_mean) turnout_pct_mean = data %&gt;% ggplot() + geom_col(aes(x = ybirth, y = turnout_pct, fill = gender), position = &quot;dodge&quot;) + geom_hline(yintercept = overall_turnout_mean) data %&gt;% ggplot() + geom_col(aes(x = ybirth, y = turnout_pct, fill = gender, colour = eligible), position = &quot;dodge&quot;) + geom_hline(yintercept = overall_turnout_mean) + geom_text(aes(x = ybirth, y = turnout_pct, fill = gender, label = eligible), position = position_dodge(width = 1), hjust = -2, angle = -90) data %&gt;% ggplot(aes(x = ybirth, y = turnout_pct, fill = gender)) + geom_col(aes(colour = eligible), position = &quot;dodge&quot;) + geom_hline(yintercept = overall_turnout_mean) + geom_text(aes(label = eligible), position = position_dodge(width = 1), hjust = -2, angle = -90) + scale_y_continuous(limits = c(0, 1)) syntax aes() important types onedimensional barcounts density twodimensional line smoothed line ? by groups colour facets pattern of gender differences by age group why are m and w more or less equal overall? -&gt; large group of old women with low turnout? "],["regex.html", "12 Regular expressions 12.1 str_detect() 12.2 Basic pattern matching 12.3 Practical application", " 12 Regular expressions WIP! Content is due to change! If you want to further follow the road of Web Scraping, you absolutely should acquire this knowledge at some point, but it lies outside the scope of this introduction. A good starting point could be the chapter on string from “R for Data Science” by Wickham and Grolemund: https://r4ds.had.co.nz/strings.html 12.1 str_detect() 12.2 Basic pattern matching 12.2.1 Literal characters 12.2.2 Escaping metacharacters 12.2.3 Character classes 12.2.4 Anchors 12.2.5 Quantifiers 12.2.6 Groups 12.3 Practical application 12.3.1 str_count() 12.3.2 str_extract() &amp; str_match() 12.3.3 str_replace() "]]
