[["index.html", "Web Scraping mit R Einführung Was ist Web Scraping? Was lernen wir, was nicht? Technische Voraussetzungen 0.1 Conventions", " Web Scraping mit R Jakob Tures 2021-04-07 Einführung WIP! Was ist Web Scraping? Ziele Zugang zu Daten die wir auf klassischem Wege nicht bekommen könnten Zugang zu neuen Formen von Daten die nur im Web denkbar sind (bspw. Kundenreviews, Nutzerverhalten) Was ist möglich: Beispiele Kriesel 3C Methoden Identifikation für unsere Fragestellung relevanter Webinhalte Zugriff auf Webinhalte aus einer Programmiersprache heraus (R, Python, etc.) Extraktion der für unsere Fragestellung relevanten Teilinhalte der Seite Säuberung und Überführung in ein für die Datenanalyse geeignetes Format -&gt; Datenanalyse Ethik Grundregeln, später Sitzung dazu Was lernen wir, was nicht? Grundstruktur des Web HTML, XML Überblick zu anderen Bausteinen die wir in der Wildnis treffen, ohne zu tief hineinzugehen Einführung in R Basics in R Studio Tidyverse tidy data dplyr ggplot rvest evtl. stringr Scraping Scrapen ganzer Seiten mit rvest CSS Selectors und Developement Tools zur Identifikation des Pfades einzelner Inhalte Extraktion dieser Pfade mit rvest Evtl. gezieltes Auslesen von XML Inhalten Was nicht: Umgang mit dynamischen Websites, javascript, Datenbankzugriffen usw. (hier nur grober Überblick was es alles gibt und das es Wege gibt damit umzugehen -&gt; Fortgeschrittene) Umgang mit Strings &amp; regular expressions (Hinweis dass sie nicht darum herumkommen wenn sie den Weg weitergehen möchten, kurzer Überblick dazu wenn Zeit reicht) Scrapen mit anderen packages oder Sprachen als R + rvest (evtl. Hinweis auf weitere verbreitete Möglichkeiten) Technische Voraussetzungen R + R Studio ordentlicher Text Editor Windows: Notepad ++ Mac: ? Linux: Atom Browser Chrome/Chromium (wegen der Developer Tools?) Notwendige Einstellungen? i.e. Darstellung voller URLs 0.1 Conventions Inline code and Code blocks Code is copyable but use sparely (writing is important) Acknowledgement Colophon? sessioninfo::session_info() but need to load all packages here to make them show up? "],["html.html", "1 HTML as a cornerstone of the internet 1.1 HTML-Tags 1.2 Attributes 1.3 Entities", " 1 HTML as a cornerstone of the internet What happens when we call up a URL such as https://webscraping-tures.github.io in a browser? We will get a visualisation of the page in our browser window. From the perspective of the user of a website, this is everything we need to know. Our goal – calling up the website – has already been reached at this point. From the perspective of a web scraper, we need to understand however, what is happening behind the scenes. The link https://webscraping-tures.github.io does nothing but call up an HTML-file in which the content of the website is recorded in form of a specific code. This code is then interpreted by your browser and translated into a visual representation, which you are finding yourselves in front of now. Try it yourself. With a right-click into this area of the text and a further click on “View page source”, the HTML-code that the website is based on, is shown. At this point it is completely legitimate to be overwhelmed by the flood of unfamiliar symbols and terminology. Who would have thought that a relatively simple website such as this one, can be so complex and complicated on the backend? But the good news is that we do not need to be able to understand every word and every symbol in an HTML file. Our goal is identifying the parts of a website that are relevant for our data collection and extract them precisely from the HTML-code. This can possibly be only a single line of code in an HTML-file with thousands of lines of code. Do we need to understand every single line? No, but we have to be able to understand the structure of an HTML-file to be able to identify that one line that is of interest to us. Until we reach this point, we still have a ways to go. At the end of this first section, you will have a basic understanding of the structure and components of an HTML document and the source code will already seem way less intimidating. 1.1 HTML-Tags The “language” that HTML-files have been written in, is the Hypertext Markup Language, HTML for short. The basics of this language are the so-called Tags or the “vocabulary”. These terms are used to structure the HTML-document, format it, insert links and images or create lists and tables. The browser knows the meaning of these key phrases, can interpret them and present the website visually according to the coded HTML-Tags. As with any language in the IT-world, HTML also follows certain rules or “grammar” or Syntax. Fortunately for us, in the case of HTML this syntax is rather simple. In the following we will take a closer look at the tags and syntax rules that are important to us. Contemplate the following example: &lt;b&gt;Hello World!&lt;/b&gt; &lt;b&gt; is a tag. The b stands for bold. Tags always follow the same pattern. They begin with a &lt;, followed by the name of the tag – b – and end with a &gt;. It is important to note that an opened tag will need to be closed as well, under normal circumstances. To do this, the same tag is written again with a forward slash, &lt;/b&gt; in our case. Everything that is contained within the opening and closing tag in an HTML-document will be interpreted according to the meaning of the tag. With this knowledge we understand what will happen with our example. The tag &lt;b&gt; means bold, and the opening and closing tag &lt;b&gt;…&lt;/b&gt; include the text Hello World!. The text will be interpreted according to the tag, which means in bold: Hello World! 1.1.1 hello_world.html Let us have a look at a full HTML-document now: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; The interpretation of this document by the browser can be seen under https://webscraping-tures.github.io/hello_world.html. Let us take a look at the single elements of the code: The first line, the document type declaration, &lt;!DOCTYPE html&gt;, informs the browser, which version of HTML is being used in the document. &lt;!DOCTYPE html&gt; stands for the current HTML5 standard. This is the place to declare if an older version of HTML was used, to ensure the most accurate visual representation – in spite of possible changes or the omission of certain standards in the current version. From the point of view of a web-scraper the HTML-version does not necessarily play a massive role. Interestingly enough, the tag &lt;!DOCTYPE html&gt; is one of the few exceptions from the rule that a once-opened tag has to be closed again; it is not necessary here. The actual content of the HTML-document only starts in line 3 with the &lt;html&gt; tag. This will only be closed in the last line and thus contains the total contents of the document. So the tag tells the browser that all it encompasses, is HTML. Next is the tag &lt;head&gt;. What we see the tag encompassing here, is not what we see in the browser window yet. What this means in practice is that here we mainly find meta-information, advanced functionality (JavaScript) and definitions of design choices (Cascading Style Sheets – CSS). This should not distract us too much in this introduction into web scraping, but we should be aware that references to .js and .css files can appear in the &lt;head&gt; tag. In our example the &lt;head&gt; tag exclusively contains another tag called &lt;title&gt;, which in turn contains the text Hello World!. The &lt;title&gt; tag determines what we will see in the title bar of our browser. In this case Hello World! Finally, everything that the &lt;body&gt; tag includes, describes the content we can see in the browser window. In this simple example, only one line is included. The already known &lt;b&gt; tag includes the text Hello World!, which it displays in bold for us to see in the browser window. So now you already know the basic structure of any HTML file. While looking at the sample code above, you may have noticed that certain lines are indented to the right. This is not a requirement for functional HTML code, but a convention that makes it easier for reading and understanding. Indented lines represent the different hierarchical levels of the code. &lt;head&gt; is hierarchically subordinate to &lt;html&gt; and therefore single indented. &lt;title&gt;, in turn, is subordinate to &lt;head&gt; and therefore doubly indented. By writing it this way, it is also obvious at a first glance that &lt;body&gt; is subordinate to &lt;html&gt; but not to &lt;title&gt;, since &lt;body&gt; and &lt;title&gt; are each only single indented. You will often – but not always – encounter this convention in “real” HTML documents on the Internet. One more note on the technical side: HTML documents can basically be written by hand in any editor and must be saved with the extension .html. You can test this by starting any text editor yourself, copying the HTML code above, saving the file with the extension .html and opening it in a browser of your choice. However, due to their complexity, websites are not usually written by hand nowadays. A variety of professional tools now offer much more efficient ways to design websites. For example, the page you are looking at was written directly in RStudio using the “bookdown” package, which automates most of the layout decisions. 1.1.2 Important tags We cannot look at all the tags available in HTML at this point, but will initially limit ourselves to those that we encounter very frequently and that will be particularly relevant for our first web scraping projects. 1.1.2.1 Page structure One tag that relates to the structure of the page, we have already met above. The &lt;body&gt; tag communicates that everything encompassed by it is part of the content displayed in the browser window. One way to further structure the content is to use the maximum of six levels of headings that HTML offers. The tags &lt;h1&gt; &lt;h2&gt; ... &lt;h6&gt; allow this in a simple way. The h stands for header. The text encompassed by the tag is automatically numbered and displayed in different font sizes depending on the level of the heading. As an example, you can see the structure of the headings on this page as HTML code: &lt;h1&gt;HTML as a cornerstone of the internet&lt;/h1&gt; &lt;h2&gt;HTML-Tags&lt;/h2&gt; &lt;h3&gt;hello_world.html&lt;/h3&gt; &lt;h3&gt;Important tags&lt;/h3&gt; &lt;h4&gt;Page structure&lt;/h4&gt; &lt;h4&gt;Formatting&lt;/h4&gt; &lt;h4&gt;Lists&lt;/h4&gt; &lt;h4&gt;Tables&lt;/h4&gt; &lt;h2&gt;Attributes&lt;/h3&gt; &lt;h3&gt;Links&lt;/h4&gt; &lt;h3&gt;Images&lt;/h4&gt; &lt;h2&gt;Entities&lt;/h3&gt; Another frequently occurring form of structuring in HTML documents, are the groupings defined via &lt;div&gt; (division) and &lt;span&gt;. Both tags basically work the same way, with &lt;div&gt; referring to one or more lines and &lt;span&gt; referring to one line or part of a line. Neither have any direct effect on the display of the website at first, but they are often applied in combination with classes that are defined in Cascading Style Sheets – CSS, to adjust the visual interpretation. Normally, we do not care about how the CSS classes are defined and how they affect rendering. You will learn later in this seminar why the combination of &lt;div&gt; or &lt;span&gt; and CSS classes are often a very practical starting point for our web scraping endeavours and how we can exploit this in our work. Here is a simplified example of how both tags can appear in HTML code: &lt;div&gt; This sentence is part of the div tag. This sentence is part of the div tag, &lt;span&gt; while this sentence is part of the div and the span Tags. &lt;/span&gt; &lt;/div&gt; This sentence is not part of the div tag. 1.1.2.2 Formatting HTML provides a variety of tags for formatting the displayed text. In the following we will look at some of the most common ones. The &lt;p&gt; tag defines the enclosed text as a paragraph and is accordingly automatically terminated in the display with a line break. &lt;p&gt;This sentence is part of the paragraph. So is this. And this one.&lt;/p&gt; This sentence is not part of the paragraph. This is represented as: This sentence is part of the paragraph. So is this. And this one. Dieser Satz ist nicht mehr Teil des Absatz. The tag &lt;br&gt; introduces a line break. This tag is another exception to the rule that an opened tag must also be closed again. In this special case, using opening and closing tags &lt;br&gt;&lt;/br&gt; stands for two line breaks, so it is not equivalent to &lt;br&gt;. Unlike the line break inserted by &lt;p&gt;...&lt;/p&gt; at the end of the paragraph, no further spacing is inserted after &lt;br&gt;: Here comes some text, which is now broken up in two lines.&lt;br&gt; After the break tag, in contrast to the paragraph tag, no line spacing is inserted.&lt;br&gt; If the break tag is also explicitly closed again, two line breaks are inserted.&lt;br&gt;&lt;/br&gt;. As can be seen here. This is represented as: Here comes some text, which is now broken up in two lines. After the break tag, in contrast to the paragraph tag, no line spacing is inserted. If the break tag is also explicitly closed again, two line breaks are inserted.. As can be seen here. The typeface can be adjusted by tags like the already known &lt;b&gt; (bold) or &lt;i&gt; (italics) similar to the known options in common text editing programs: These tags can be used to render words, sentences and paragraphs &lt;b&gt;bold&lt;/b&gt; or &lt;i&gt;italic&lt;/i&gt;. This is represented as: These tags can be used to render words, sentences and paragraphs bold or italic. 1.1.2.3 Lists We will often encounter lists in HTML documents. The two most common variants being the unordered list, introduced by &lt;ul&gt;, and the ordered list, &lt;ol&gt;. The opening and closing list-tag covers the entire list, while each individual list element is enclosed by a &lt;li&gt; tag in both variants. Here are two short examples: &lt;ul&gt; &lt;li&gt;First unordered list element&lt;/li&gt; &lt;li&gt;Second unordered list element&lt;/li&gt; &lt;li&gt;Third unordered list element&lt;/li&gt; &lt;/ul&gt; This is represented as: First unordered list element Second unordered list element Third unordered list element &lt;ol&gt; &lt;li&gt;First ordered list element&lt;/li&gt; &lt;li&gt;Second ordered list element&lt;/li&gt; &lt;li&gt;Third ordered list element&lt;/li&gt; &lt;/ol&gt; This is represented as: First ordered list element Second ordered list element Third ordered list element 1.1.2.4 Tables HTML can also be used to display tables, without further adjustments of the display via CSS admittedly not very attractive tables. These are opened by a &lt;table&gt; tag and closed accordingly. Within the table, lines are defined by &lt;tr&gt;...&lt;/tr&gt; (table row). Within the line, table headers can be defined by &lt;th&gt; (table header) and cell contents by &lt;td&gt; (table data). Content encompassed by &lt;th&gt;...&lt;/th&gt; and &lt;td&gt;...&lt;/td&gt; are not only formatted differently in their presentation, from the web scraper’s point of view, these tags also allow us to clearly distinguish the table content to be read. Here is a simple example: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; This is displayed as: # Tag Effect 1 “b” bold 2 “i” italics Formatting the HTML code in a kind of “table form” as in the example above is not necessary but increases intuitive readability. In fact, the following manner of writing it, is equivalent in result and actually more common in practice: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effect&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; This will also be displayed as: # Tag Effect 1 “b” bold 1 “i” italics 1.2 Attributes Many HTML tags can be further adapted in their functionality and presentation by using so-called attributes. The basic syntax is: &lt;tag attribute=\"value\"&gt;...&lt;/tag&gt;. In the opening (never in the closing) tag, the name of the tag is followed by the name of the attribute = the value to be assigned, enclosed in single or double inverted commas. In HTML, &lt;tag attribute=\"value\"&gt; is not equal to &lt;tag attribute = \"value\"&gt;. The pair of attribute name and value must be connected with a = without spaces in order to be interpreted correctly. A variety of tags can be modified with a multitude of attributes. Two of the most common and illustrative applications are the inclusion of links and images, two other frequently encountered HTML tags. 1.2.1 Links Links are included using the &lt;a&gt; (anchor) tag. The first intuitive attempt &lt;a&gt;This is a link&lt;/a&gt; is unfortunately unsuccessful: This is (not) a link Although the text is displayed and marked as a link – i.e. blue and underlined – it does not lead to any destination, since it was not defined in the HTML document what this destination should be. This is where the first attribute comes into play. With &lt;a href=\"url\"&gt; the target of the link is defined. href stands for hypertext reference and its assigned value can be, among other things, a website, an email address or even a file. For example, &lt;a href=\"webscraping-tures.github.io/html.html\"&gt;This is a link&lt;/a&gt; links to the page you are viewing. This is a link You may have noticed that you had to scroll to this point again to continue reading. A second attribute can remedy this. With &lt;a href=\"webscraping-tures.github.io/html.html\" target=\"_blank\"&gt;This is a link&lt;/a&gt; we instruct the browser to open the link in a new tab. The assigned value of target here is \"_blank\", which stands for a new tab, but it can also take on a number of other values. This is a link Links are of particular interest to us in web scraping when we collect links to all sub-pages from a parent page in order to scrape them specifically. But more about that later. One more note: the &lt;link&gt; tag is not to be confused with &lt;a&gt; and is used to integrate external files, such as the JavaScript or CSS files already mentioned. 1.2.2 Images Images and graphics are integrated in HTML with &lt;img&gt;, another tag that does not have to be explicitly closed. So that the browser knows which image is to be included, this is specified via the src (source) attribute of the tag. Thus &lt;img src=\"webscraping-tures.github.io/Rlogo.png\"&gt; includes the following image: Using further attributes, it is also possible, for example, to adjust the size of the image in pixels. &lt;img src=\"webscraping-tures.github.io/Rlogo.png\" width=\"100\" height=\"100\"&gt; leads to a resized display of the image. Images can also be combined with links. So &lt;a href=\"https://www.r-project.org/\" target=\"_blank\"&gt;&lt;img src=\"webscraping-tures.github.io/Rlogo.png\"&gt;&lt;/a&gt; defines the image as a link, where a click on the image takes you to the specified link: 1.3 Entities A number of characters are reserved for the HTML code. We have already seen that the characters &lt; &gt; \" are part of the code to define tags and values of attributes. In many cases, more current HTML versions allow for the usage of reserved characters directly in continuous text. For the time being, however, we will regularly encounter so-called entities instead of the actual characters in web scraping. Entities are coded representations of certain characters. They are always introduced with &amp; and ended with ;. Between the two characters is either the name or the number of the entity. For example, &amp;lt; stands for less than, i.e. &lt; and &amp;gt; for greater than, i.e. &gt;. A text with reserved characters like &amp;lt; und &amp;gt; or the so-called &amp;quot;ampersand&amp;quot; &amp;amp;. Is displayed as: A text with reserved characters like &lt; und &gt; or the so-called “ampersand” &amp;. Sidenote: If you are interested in the origin of the term ampersand, I recommend its Wikipedia article, which makes for an interesting read: https://en.wikipedia.org/wiki/Ampersand Another entity we will encounter regularly is &amp;nbsp; (non-breaking space), which can be used instead of a simple space. The advantage of this is that there is never a line break in the browser, and it allows the use of more than one space: Displayed with one space Displayed with four&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;spaces Displayed with one space Displayed with four    spaces An overview of the most common entities can be found here: https://www.w3schools.com/html/html_entities.asp The entities of all Unicode characters can be found here: https://unicode-table.com/ "],["R1.html", "2 R &amp; R Studio 2.1 Installing R 2.2 RStudio 2.3 Hello World! 2.4 Objects 2.5 Vectors 2.6 Functions", " 2 R &amp; R Studio 2.1 Installing R R is a freely available programming language used predominantly for data science and statistical computations. For more information on the language and to access the documentation, visit: https://www.r-project.org/. From there you can also follow the link to CRAN, the Comprehensive R Archive Network, or access it directly by visiting https://cran.r-project.org/. The latest versions of R will always be hosted at CRAN. At the top of the landing page, you will find links to the installers for each operating system. If you are using Windows, please choose “base” after following the link and then download the offered file. In the case of Mac OS download the first file listed under “Latest release”. In both cases, execute the file ans install R to a directory of your choice. If you are using Linux, the link on CRAN will offer installation advice for some of the more popular distributions. In any case, you can check the package manager of your choice for the latest available release for your system. All examples used on this Website were written and tested with R version 4.0.4 “Lost Library Book”. While it is not be expected, that they will not be usable under newer versions of R, this could be a potential error source, should you run into problems. 2.2 RStudio The basic R installation provides a simple GUI – Graphical User Interface – that could in principle be used to follow the contents of this introduction to Web Scraping. The widely more common approach is to use an external IDE – Integrated Development Environment –, the most popular being RStudio. Using an IDE will dramatically ease your workflow and I would strongly recommend using RStudio or this purpose. RStudio is also freely available and can be found at https://www.rstudio.com/. Following the “Download” link and scrolling down (ignoring the different versions offered for professional usage) you will find the latest installers for several operating systems offered as downloads. In most cases, simply installing RStudio after R has been installed, will work “out of the box”. 2.2.1 Overview The RStudio interface consists of four subareas. The bottom-left shows the “Console” as well as additional riders which you will rarely need in the beginning. The console can be used to enter R code, complete it by pressing “Enter” and receiving the output created by your code. We will begin working with the console soon, so this will make more sense to you in a bit. The top-left shows opened files, like R scripts. This is where you will actually spend most of your time, as this introduction processes from using one-time commands in the console to writing your code in savable scripts, that can be re-opened, re-run and shared. The top-right has several riders, of whom “Environment” should be our main concern at this point. Here you will see all data objects created in your RStudio session. More on this later. Finally, the bottom-right amongst other things shows us the “Files” in a selected folder, graphical output under “Plots” and requested “Help” on packages and functions. 2.3 Hello World! So, let’s begin with putting R &amp; RStudio to use. For now, we will write our commands directly into the console. You will notice a &gt; sign at the beginning of the last line in the console. This is a prompt, as in “write commands here”. Try writing this and executing the command with the “Enter” key: print(&quot;Hello World!&quot;) ## [1] &quot;Hello World!&quot; You just entered your first R command, received your first output and also used your first function. We will concern ourselves with functions in more detail later, for now it is enough to know that the command print() prints everything that is enclosed in its parentheses to the output. The output begins with [1], indicating that this is the first, and in this case the only, element of the output generated by the executed command. Please note, that RStudio will not print ## before the output. In the shown code segments on this website, ## is inserted before the output to allow copying the code directly to RStudio, as one or mutliple # indicate that a line is a comment, and thus is not evaluated as a command by R. 2.3.1 Calculating with R R understands the basic arithmetic symbols + - * / and thus the console can be used as a calculator. Many functions for more involved calculations, e.g.  sqrt() for taking a square root of the content enclosed in the parenthesis, are available. x^y can be used to write x to the power of y. For now, you should write the code below line for line into the r console and execute each line with the “Enter” key. 17 + 25 ## [1] 42 99 - 57 ## [1] 42 4 * 10.5 ## [1] 42 84 / 2 ## [1] 42 sqrt(1764) ## [1] 42 6.480741 ^ 2 ## [1] 42 2.3.2 Comparison operators We can use comparison operators to compare two values and receive the test result as output. To test if two values are equal, we write ==. To test if they are not equal, we can use != 42 == 42 ## [1] TRUE 42 != 42 ## [1] FALSE We can also compare if the first value is less &lt;, less or equal &lt;=, larger &gt; or larger or equal &gt;=, compared to the second value. 10 &lt; 42 ## [1] TRUE 42 &lt;= 42 ## [1] TRUE 10 &gt; 42 ## [1] FALSE 90 &gt;= 42 ## [1] TRUE 2.4 Objects Some of the power of using a language like R for computation, comes from the ability to store data or results for later use and further analysis. In R, all types of data are stored in objects. On a basic level, an object is a name that we define that has some form of data assigned to it. To assign data to a name, we use the assignment operator &lt;-. the_answer &lt;- 42 A handy keyboard shortcut for writing the assignment operator is pressing the “Alt” and “-” keys simultaneously. Learning this shortcut early, will safe you on a lot of typing and keyboard gymnastics. After we assigned a value to an object, we can recall that value, by writing the object’s name. the_answer ## [1] 42 We can also use defined objects in calculations and function calls (more on these later). Note, that if we assign a value to a already defined object, the stored value is overwritten with by the new one. the_answer &lt;- the_answer / 2 the_answer ## [1] 21 a &lt;- 17 b &lt;- 4 the_answer &lt;- (a + b) * 2 the_answer ## [1] 42 All objects we define, are listed in the “Environment” tab, seen in the upper right of RStudio. If we ever want to remove objects from the environment, we can use the rm() function. In general, this is not necessary, but it can help with keeping the list from getting cluttered. rm(the_answer) 2.5 Vectors When we assigned a number to an object, we actually created a vector. A vector is a one-dimensional data structure that can contain multiple elements. The number of elements determine the length of the vector. So a vector with only one element is still a vector, but with a length of 1. To assign multiple elements to a vector, we use the combine function c(). All values inside the parentheses, separated by ,, are combined as elements to form the vector. v &lt;- c(7, 8, 9) v ## [1] 7 8 9 2.5.1 Subsetting If we want to access certain elements of a vector, we have to use subsetting. This is achieved by adding square brackets to the objects name, containing the position of the element in its vector. So, to access the first or third element, we can write: v[1] ## [1] 7 v[3] ## [1] 9 We can also access multiple elements at once, using c() inside the brackets or by defining a range of positions using :. v[c(1, 3)] ## [1] 7 9 v[2:3] ## [1] 8 9 2.5.2 Types of vectors Observing the vector v we created in the environment, we notice, that RStudio writes num [1:3] before listing the values of the elements. The second part, indicates the length of 3, while the first part shows the type of the vector we created. In this case the type is numeric. Numeric vectors, as you might have guessed, contain numbers, as seen in the example above. We can also use str() to receive info on type, length and content of a vector. str(v) ## num [1:3] 7 8 9 There are a number of other types of vectors, the two most important – besides numeric vectors – being logical and character vectors. Logical vectors can only contain the values TRUE and FALSE. Strictly speaking, they – as the other types of vectors – can also contain NA, indicating a missing value. We will talk more about NAs later on. Logical vectors are often created when we test for something. For example, we can test, if the elements in a numerical vector are larger or equal to 5 and receive a logical vectors containing the test results. x &lt;- c(1, 7, 3, 5) x &gt;= 5 ## [1] FALSE TRUE FALSE TRUE Character vectors contain strings of characters. When assigning strings, they have to be enclosed in quotation marks. char_v &lt;- c(&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;character&quot;, &quot;vector!&quot;) We can compare character vectors only for (non)equality, not for being smaller or larger. &quot;same&quot; == &quot;same&quot; ## [1] TRUE &quot;same&quot; == &quot;not the same&quot; ## [1] FALSE &quot;same&quot; != &quot;not the same&quot; ## [1] TRUE Character vectors also can not used to calculate. This can get problematic, if numbers are stored as characters, which arises frequently when Web Scraping. a &lt;- c(1, 2, 3) b &lt;- c(&quot;7&quot;, &quot;8&quot;, &quot;9&quot;) str(a) ## num [1:3] 1 2 3 str(b) ## chr [1:3] &quot;7&quot; &quot;8&quot; &quot;9&quot; a + b ## Error in a + b: non-numeric argument to binary operator As we enclosed the elements of vector b in quotation marks, R interprets the data as characters instead of numbers. As characters can not be used for calculations, we received an error message. But we can force R to interpret the characters as numbers by using as.numeric(). a + as.numeric(b) ## [1] 8 10 12 2.5.3 A brief look on lists Note that a vector of a certain type, can only contain elements of that type. So we can not mix data types in the same vector. If we want to mix data types, we could use lists instead of vectors. l &lt;-list(1, TRUE, &quot;Hello World!&quot;) str(l) ## List of 3 ## $ : num 1 ## $ : logi TRUE ## $ : chr &quot;Hello World!&quot; Lists can also contain other lists to represent hierarchical data structures. We will see lists “in action” later on in this course. 2.6 Functions Functions provide an easy and concise way of performing more or less complex tasks, using predefined bits of R code that are provided in “base R” – i.e.  that come with the basic R installation – or in the various additional packages that are available for installation. We have already used a number of functions up to this point, e.g. print(). To “call” a function, we write its name, followed by parentheses. Inside the parentheses additional arguments are provided to R. In most cases, some data has to be entered as the first argument. For example, print() writes the text provided as argument to the output. More complex functions often allow for more than one argument. Sometimes these are required, but more often these additional arguments are optional and can be used to change some options from the default value to the one desired. 2.6.1 Help But how do we know which arguments can or have to be provide to use a function and what their effects are? We can check the documentation on CRAN or use Google to find additional information. Another often more convenient way, is to use the help functionality build into R. By writing ? in front of the function name into the console and executing the line by pressing “Enter”, the help file is opened in the lower right of the RStudio window. Let’s try this for the function rnorm(). ?rnorm() From the help file we can learn several things. rnorm() is part of a family of functions that are related to the normal distribution, each providing a distinct functionality. The functionality of rnorm() being the generation of random numbers stemming from the normal disribution. We also learn, that three arguments can be provided. n, the number of observations to be generated, as well as mean and sd, the mean and the standard deviation of the normal distribution to be drawn from. We also see, that mean and sd are provided with the standard values 0 and 1 respectively, indicated by the =. We also see that n has no standard value. So we have to provide a value for n but not for mean and sd. Just writing rnorm() will result in an error. rnorm() ## Error in rnorm(): argument &quot;n&quot; is missing, with no default To provide an argument to a function, we write the name of the argument, followed by = and the value to be provided. Note that as rnorm() draws random numbers, your output will differ from the output presented here. rnorm(n = 10) ## [1] 1.0142646 -0.1689748 0.5068803 -0.5069829 0.9669013 1.0442285 ## [7] -0.3898110 -0.6025732 0.1892501 0.2142304 In the same vein, additional arguments that are allowed by the function can be defined, instead of using their default values. rnorm(n = 10, mean = 10, sd = 0.5) ## [1] 9.401038 9.725171 10.034771 10.070750 10.174030 11.065934 10.047441 ## [8] 9.505244 10.110123 10.504773 We can also skip writing the names of arguments in many cases. As the n argument is the first listed in the function’s parentheses, R also understands the call, if we just provide the value to be used as the first argument. You will often encounter the convention, that the first argument is written without its name, and any further arguments are written in full. rnorm(10, mean = 10, sd = 0.5) ## [1] 10.691547 10.031205 10.091163 9.250863 10.261373 9.770908 9.738884 ## [8] 10.310251 10.425828 11.603318 2.6.2 Examples: Basic statistical functions Base R provides us with some basic statistical functions that are used for data analysis. We should start with defining a numerical vector that contains some data to be analysed. data &lt;- c(4, 8, 15, 16, 23, 42) We could be interested in describing this data by its arithmetic mean, median and standard deviation. For this purpose we can use the functions mean(), median() and sd() provided by base R. All three do not require additional arguments besides the data to be analysed, which we can provide using the object data we created beforehand. mean(data) ## [1] 18 median(data) ## [1] 15.5 sd(data) ## [1] 13.49074 "],["tidyverse.html", "3 Tidyverse 3.1 RStudio Workflow 3.2 Settings? 3.3 Packages in general 3.4 Philosophie 3.5 Tidy data 3.6 Pipe", " 3 Tidyverse 3.1 RStudio Workflow 3.2 Settings? 3.3 Packages in general 3.3.1 namespaces 3.3.2 help/vignettes 3.4 Philosophie 3.5 Tidy data 3.6 Pipe cheat sheets "],["rvest1.html", "4 First scraping with rvest 4.1 The rvest package 4.2 hello_world.html 4.3 Countries of the World", " 4 First scraping with rvest With the knowledge of how an HTML file is constructed and how R and RStudio work in basic terms, we are equipped with the necessary tools to take our first steps in web scraping. In this session we will learn how to use the R package rvest to read HTML source code into RStudio, extract targeted content we are interested in, and transfer the collected data into an R object for further analysis in the future. 4.1 The rvest package Part of the tidyverse is a package called rvest, which provides us with all the basic functions for a variety of typical web scraping tasks. This package was included in the installation of the tidyverse package, but it is not part of the core tidyverse and thus is not loaded into the current R session with library(tidyverse). Therefore, we have to do this explicitly: library(rvest) ## Loading required package: xml2 The output in the RStudio console informs us that besides rvest, the package xml2 has also been loaded, on which rvest is partly based and whose function for reading HTML files, we will need in the following. 4.2 hello_world.html As a first exercise, it is a good idea to scrape the Hello World example already described in chapter 1. As a reminder, here is the HTML source code: &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello World!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;Hello World!&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; 4.2.1 read_html() The first step in web scraping is to convert the page we are interested in into an R object. This is made possible by the function read_html() from the xml2 package. read_html() “parses” the website, i.e. it reads the HTML, understands its source code and transforms it into a representation R can understand. This function needs the URL, i.e. the address of the website we want to read in, as its first argument. The URL must be given as a string, so we have to enclose it in \". The function also allows you to specify other options. In most cases, however, the default settings are sufficient. So we read in the hello_world.html file, assign it to a new R object at the same time and have this object put out in the next step: hello_world &lt;- read_html(&quot;https://webscraping-tures.github.io/hello_world.html&quot;) hello_world ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; As we can see in the output, the R object hello_world is a list with two entries. The first entry contains everything enclosed by the &lt;head&gt; tag, the second entry everything enclosed by the &lt;body&gt; tag. The opening and closing &lt;html&gt; tag is not part of the object. Remembering that HTML code is hierarchically structured, the list is thus organised based on the highest remaining levels – &lt;title&gt; and &lt;body&gt;. We have thus successfully created a representation of the website in an R object. But what do we do with it now? In the case of this simple example, we might be interested in extracting the title of the website or the text displayed on the page. 4.2.2 html_nodes() The function html_nodes() from the rvest package allows us to extract individual elements of the HTML code. To do this, it needs the object to be extracted from, as the first argument and a selector as well. In this introduction, we will concentrate exclusively on the so-called CSS Selectors. The alternative XPath is a bit more flexible, but CSS Selectors are sufficient in most cases and have a shorter and more intuitive syntax, which clearly makes them the tool of choice here. We will discuss the possibilities offered by CSS Selectors in more detail in chapter 5.1 and will limit ourselves to the basics for now. A selector in the form \"tag\", selects all HTML tags of the specified name. If we want to extract the &lt;title&gt; tag, we can do so in this way: node_title &lt;- html_nodes(hello_world, css = &quot;title&quot;) node_title ## {xml_nodeset (1)} ## [1] &lt;title&gt;Hello World!&lt;/title&gt; If we want to extract the text Hello World! shown on the website, one possibility would be to select the complete &lt;body&gt; tag, since in this case no other text is displayed on the page. node_body &lt;- html_nodes(hello_world, css = &quot;body&quot;) node_body ## {xml_nodeset (1)} ## [1] &lt;body&gt;\\n &lt;b&gt;Hello World!&lt;/b&gt;\\n &lt;/body&gt; This works in principle, but we also extracted the &lt;b&gt; tags as well as multiple new lines (\\n), which we do not need both. It would be more efficient to directly select the &lt;b&gt; tag enclosing the text. node_b &lt;- html_nodes(hello_world, css = &quot;b&quot;) node_b ## {xml_nodeset (1)} ## [1] &lt;b&gt;Hello World!&lt;/b&gt; 4.2.3 html_text() In this case, we are interested in the text in the title and on the website, i.e. the content of the tags. We can extract this from the selected HTML elements in an additional step. This is made possible by the rvest function html_text(). This requires the previously extracted HTML element as the only argument. html_text(node_title) ## [1] &quot;Hello World!&quot; html_text(node_b) ## [1] &quot;Hello World!&quot; With this, we have successfully completed our first web scraping goal, the extraction of the title and the text displayed on the page. One more thing about the application of html_text() to elements that themselves contain further tags: Further above we extracted the object node_body, which contains the tags as well as several line breaks in addition to the displayed text. Here, too, we can extract the pure text. html_text(node_body) ## [1] &quot;\\n Hello World!\\n &quot; We see that the function has conveniently removed the &lt;b&gt; tags we were not interested in, for us. However, the line breaks and several spaces, so-called whitespace, remain. Both can be removed with the additional argument trim = TRUE. html_text(node_body, trim = TRUE) ## [1] &quot;Hello World!&quot; 4.3 Countries of the World Let us now look at a somewhat more realistic application. The website https://scrapethissite.com/pages/simple/ lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further. Before we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.6 ✓ dplyr 1.0.4 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x readr::guess_encoding() masks rvest::guess_encoding() ## x dplyr::lag() masks stats::lag() ## x purrr::pluck() masks rvest::pluck() library(rvest) website &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) To understand the structure of the HTML file, the first step is to look at the source code. As always, we can open it by right-clicking in the browser window and then clicking on “View page source”. The first 100 or so lines of HTML code mainly contain information on the design of the website, which should not distract us further at this point. We are purely interested in the data of the countries. The first country listed on the website is Andorra. It therefore makes sense to search the source code specifically for “Andorra”. The key combination CTRL+F opens the search mask in your browser. We find what we are looking for in line 128. Since this source code, designed for practice purposes, is formatted in a very structured way, we quickly realise that lines 125-135 are code blocks related to Andorra. Let’s look at these more closely: &lt;div class=&quot;col-md-4 country&quot;&gt; &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; &lt;/div&gt;&lt;!--.col--&gt; All the information about Andorra is enclosed in a &lt;div&gt; tag. As a reminder, a &lt;div&gt; defines a grouping of code across multiple lines. In web design practice, these groupings are mainly used to assign a certain CSS style to the following code via the argument class=, for example to define the typeface. From a web scraping perspective, we generally don’t care how the styles are defined. We just need to know that we can exploit these CSS assignments of classes for our purposes. At the next level down, we find two blocks, one containing, among other things, the name of the country and another containing information about that country. Let’s look at the first block first. 4.3.1 Country names &lt;h3 class=&quot;country-name&quot;&gt; &lt;i class=&quot;flag-icon flag-icon-ad&quot;&gt;&lt;/i&gt; Andorra &lt;/h3&gt; The name “Andorra” is enclosed in an &lt;h3&gt; tag, i.e., a third-level heading. In addition to the name, we also find another tag within the tag that includes the image of the flag. Since we are not interested in the graphics here, we can ignore this. On this website, all &lt;h3&gt; tags are used exclusively to display the names of the countries. Thus, we can use the &lt;h3&gt; tag as a CSS selector to read out the enclosed text analogous to the first example. node_country &lt;- html_nodes(website, css = &quot;h3&quot;) text_country &lt;- html_text(node_country, trim = TRUE) head(text_country, n = 10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; The result looks promising. Since the structure of the code block is the same for each country, the vector text_country was created in this way with 250 entries, exactly the number of countries listed on the website. For reasons of clarity, it often makes sense not to put out the complete and often very long vectors, data frames or tibbles, but to use the function head() to list the number of entries specified by the argument n, starting with the first. 4.3.1.1 The pipe %&gt;% At this point, we should think again about the readability and structure of our R code. Let us consider the preceding code block: node_country &lt;- html_nodes(website, css = &quot;h3&quot;) text_country &lt;- html_text(node_country, trim = TRUE) As we have seen, this achieves our goal. However, we have also created the node_country object to temporarily save the first step – reading the &lt;h3&gt; tags. We will never need this object again. If we use the already introduced pipe %&gt;% instead, the need to cache partial results is eliminated and we write code that is more intuitive and easier to understand at the same time. country &lt;- website %&gt;% html_nodes(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE) head(country, n = 10) ## [1] &quot;Andorra&quot; &quot;United Arab Emirates&quot; &quot;Afghanistan&quot; ## [4] &quot;Antigua and Barbuda&quot; &quot;Anguilla&quot; &quot;Albania&quot; ## [7] &quot;Armenia&quot; &quot;Angola&quot; &quot;Antarctica&quot; ## [10] &quot;Argentina&quot; As a reminder, the pipe passes the result of a work step along to the next function, which in the tidyverse as well as in many other R-functions (but not ALL!) takes data as the first argument, which we then do not have to define explicitly. For a better understanding, let’s look at the above example in detail. The first line passes the object website along to the function html_nodes(). So we don’t have to tell html_nodes() which object to apply to, because we already passed it along to the function with the pipe. The function is applied to the object website with all other defined arguments – here css – and the result is passed along again to the next line, where the html_text() function is applied to it. Here the pipe ends, and the final result is assigned to the object country. We now need three instead of two lines to get the same result, but the actual typing work has been reduced – especially if you create the pipe with the key combination CTRL+Shift+M – and we have created code that can be read and understood more intuitively with a little practice. So should we always connect all steps with the pipe? No. In many cases it makes sense to save intermediate results in an object, namely whenever we will access it multiple times. In our example, we could also integrate the import of the website into the pipe: country &lt;- read_html(&quot;https://scrapethissite.com/pages/simple/&quot;) %&gt;% html_nodes(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE) Overall, this saves us even more typing. However, since we still have to access the selected website multiple times later on, this would also mean that the parsing process has to be repeated each time. On the one hand, this can have a noticeable impact on the computing time for larger amounts of data. On the other hand, it also means accessing the website’s servers and downloading the data again each time. However, we should avoid data traffic generated without good reasons as part of a good practice of web scraping. So it makes perfect sense to save the result of the read_html() function in an R object so that it can be reused multiple times later. 4.3.2 Capitals, population and area Let us now turn to the further information for each country. These are located in the second block of the HTML code considered above: &lt;div class=&quot;country-info&quot;&gt; &lt;strong&gt;Capital:&lt;/strong&gt; &lt;span class=&quot;country-capital&quot;&gt;Andorra la Vella&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Population:&lt;/strong&gt; &lt;span class=&quot;country-population&quot;&gt;84000&lt;/span&gt;&lt;br&gt; &lt;strong&gt;Area (km&lt;sup&gt;2&lt;/sup&gt;):&lt;/strong&gt; &lt;span class=&quot;country-area&quot;&gt;468.0&lt;/span&gt;&lt;br&gt; &lt;/div&gt; As we can see, both the name of the capital, the population of the country, and its size in square kilometers are enclosed by a &lt;span&gt; tag in lines 2–4 respectively. Like &lt;div&gt;, &lt;span&gt; defines groupings, but not across multiple lines but for one, or as here, part of a line. So let’s try to read the names of the capitals, using the &lt;span&gt; tag as a selector. website %&gt;% html_nodes(css = &quot;span&quot;) %&gt;% html_text() %&gt;% head(n = 10) ## [1] &quot;Andorra la Vella&quot; &quot;84000&quot; &quot;468.0&quot; &quot;Abu Dhabi&quot; ## [5] &quot;4975593&quot; &quot;82880.0&quot; &quot;Kabul&quot; &quot;29121286&quot; ## [9] &quot;647500.0&quot; &quot;St. John&#39;s&quot; So we get the names of the capitals, but also the population and the size of the country. span was too unspecific as a selector. Since all three types of country data are enclosed with &lt;span&gt; tags, all three are also selected. So we have to tell html_nodes() more precisely which &lt;span&gt; we are interested in. This is where the CSS classes we mentioned earlier come into play. These differ between the three countries’ information. For example, the &lt;span&gt; that includes the name of the capital city is assigned the class \"country-capital\". We can target this class with our CSS selector. To select a class, we can use the syntax .class-name. So, to select all &lt;span&gt; that have the class \"country-capital\", we can do as follows: capital &lt;- website %&gt;% html_nodes(css = &quot;span.country-capital&quot;) %&gt;% html_text() head(capital, n = 10) ## [1] &quot;Andorra la Vella&quot; &quot;Abu Dhabi&quot; &quot;Kabul&quot; &quot;St. John&#39;s&quot; ## [5] &quot;The Valley&quot; &quot;Tirana&quot; &quot;Yerevan&quot; &quot;Luanda&quot; ## [9] &quot;None&quot; &quot;Buenos Aires&quot; We can repeat this in an analogue manner for the number of inhabitants with the class \"country-population\". population &lt;- website %&gt;% html_nodes(css = &quot;span.country-population&quot;) %&gt;% html_text() head(population, n = 10) ## [1] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ## [7] &quot;2968000&quot; &quot;13068161&quot; &quot;0&quot; &quot;41343201&quot; If we take a closer look at the vector created in this way, we see that it is a character vector. For inspection we can use the function str(), which gives us the structure of an R object, including the data type used. str(population) ## chr [1:250] &quot;84000&quot; &quot;4975593&quot; &quot;29121286&quot; &quot;86754&quot; &quot;13254&quot; &quot;2986952&quot; ... So the numbers were not read out as numbers but as strings. Among other things, this does not allow for calculation with the numbers. population[1] selects the first element of the vector. population[1] / 2 ## Error in population[1]/2: non-numeric argument to binary operator As you remember, one way to tell R to interpret the “text” read from the HTML code as numbers is to use the function as.numeric(). population &lt;- website %&gt;% html_nodes(css = &quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric() str(population) ## num [1:250] 84000 4975593 29121286 86754 13254 ... population[1] / 2 ## [1] 42000 In the same way, the size in square kilometers can be read with the class \"country-area\". area &lt;- website %&gt;% html_nodes(css = &quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() str(area) ## num [1:250] 468 82880 647500 443 102 ... 4.3.3 Merge into one tibble We have now created four vectors, which respectively contain the information about the name of the country, the associated capital, the number of population and the size of the country. For Andorra: country[1] ## [1] &quot;Andorra&quot; capital[1] ## [1] &quot;Andorra la Vella&quot; population[1] ## [1] 84000 area[1] ## [1] 468 We could already continue working with this, but for many applications it is more practical if we compile the data in tabular form. In the tidyverse, the form of the tibble is suitable for this purpose. countries &lt;- tibble( Land = country, Hauptstadt = capital, Bevoelkerung = population, Flaeche = area ) countries ## # A tibble: 250 x 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # … with 240 more rows This is not only more readable but also facilitates all further potential analysis steps. If we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines. website &lt;- &quot;https://scrapethissite.com/pages/simple/&quot; %&gt;% read_html() countries_2 &lt;- tibble( Land = website %&gt;% html_nodes(css = &quot;h3&quot;) %&gt;% html_text(trim = TRUE), Hauptstadt = website %&gt;% html_nodes(css = &quot;span.country-capital&quot;) %&gt;% html_text(), Bevoelkerung = website %&gt;% html_nodes(css = &quot;span.country-population&quot;) %&gt;% html_text() %&gt;% as.numeric(), Flaeche = website %&gt;% html_nodes(css = &quot;span.country-area&quot;) %&gt;% html_text() %&gt;% as.numeric() ) countries_2 ## # A tibble: 250 x 4 ## Land Hauptstadt Bevoelkerung Flaeche ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Andorra Andorra la Vella 84000 468 ## 2 United Arab Emirates Abu Dhabi 4975593 82880 ## 3 Afghanistan Kabul 29121286 647500 ## 4 Antigua and Barbuda St. John&#39;s 86754 443 ## 5 Anguilla The Valley 13254 102 ## 6 Albania Tirana 2986952 28748 ## 7 Armenia Yerevan 2968000 29800 ## 8 Angola Luanda 13068161 1246700 ## 9 Antarctica None 0 14000000 ## 10 Argentina Buenos Aires 41343201 2766890 ## # … with 240 more rows "],["css-selectors-developer-tools.html", "5 CSS selectors &amp; Developer Tools 5.1 CSS selectors 5.2 Developer Tools", " 5 CSS selectors &amp; Developer Tools 5.1 CSS selectors In the previous section, you already got to know the first CSS Selectors. These are actually used in web design to select individual elements of a website and apply a CSS style to them, i.e. to define the display of the elements. So they were not developed with web scraping applications in mind, yet we can still make use of them, because we also want to select individual elements of a website in order to extract them. CSS Selectors are used in rvest as an argument of the read_html() function. As a second argument – the first one determines which data the function should be applied to –we specify a selector in the form css = \"selector\". This determines which elements of the HTML code we want to extract. It is important here that the entire selector – regardless of how many individual parts it consists of – is always passed along as a string to the argument. In the following, the CSS Selectors are applied to the website https://webscraping-tures.github.io/wahlbeteiligung_2.html for illustration. You can view the source code the usual way. Firstly, we load the rvest package and parse the website. library(rvest) website &lt;- &quot;https://webscraping-tures.github.io/wahlbeteiligung_2.html&quot; %&gt;% read_html() We have already learned about the simplest selector. \"Tag\" selects all occurrences of the specified HTML tag. For example, we can select the title of the website – in the &lt;title&gt; tag – or the heading displayed in the browser window – in &lt;h3&gt;. website %&gt;% html_nodes(css = &quot;title&quot;) ## {xml_nodeset (1)} ## [1] &lt;title&gt;Wahlbeteiligung Landtagswahlen&lt;/title&gt;\\n website %&gt;% html_nodes(css = &quot;h3&quot;) ## {xml_nodeset (1)} ## [1] &lt;h3&gt;Wahlbeteiligung bei der letzten Landtagswahl nach Bundesland:&lt;/h3&gt; 5.1.1 Classes We also got to know the selector for the argument class – \".class\" – in the previous section. We can select all &lt;span&gt; tags of the class \"bundesland-name\" as follows: website %&gt;% html_nodes(css = &quot;.bundesland-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;span.bundesland-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; The first variation of the selector says, select all elements of the class \"bundesland-name\". The second variation says, select all elements of the class \"bundesland-name\". For this website, both variations are equivalent in result, since all elements assigned the class \"bundesland-name\" are also &lt;span&gt; tags. However, this does not always have to be the case. Different tags with different content may very well have the same class in practice. Basically, constructing CSS selectors is always a balancing act between functionality and readability. The selector should select exactly only the elements we are interested in, and at the same time be understandable. The latter is especially important when others – or you yourself a few weeks later – read the code. Readability also means achieving a balance between length and clarity of the selector. To illustrate: website %&gt;% html_nodes(css = &quot;body &gt; div#daten &gt; div[class^=bundesland-] &gt; span.bundesland-name&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; This is the full path through the hierarchical HTML structure, which leads us to the result already seen above. You don’t need to fully understand this one yet. But you can see that the selector can get long and complicated, and here we are dealing with a very simply designed website. For me, the selector \"span.bundesland-name\" is a good choice, because it allows us to reach our goal and has a balance between brevity and readability that is pleasant for me. But you must decide this for yourself in each individual case. 5.1.2 IDs Another common attribute in HTML elements, is the id. These identify individual HTML elements with a unique assigned name. Among other things, these are used for design purposes, as part of scripts for the dynamic design of websites or in HTML forms. We can also use them to extract individual elements specifically. In our example, the &lt;span&gt; tag, which includes the name of each state, has an id that identifies the state with a two-character abbreviation: &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; The selector for IDs is being written as \"#id\". website %&gt;% html_nodes(css = &quot;#bw&quot;) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; In this case, however, we have selected more than we wanted. Because the &lt;img&gt; tag, which represents the flags of the federal states, has the attribute id=\"bw\" as well. Strictly speaking, this is against the HTML rules, but this is also a reality. We can’t rely on the creators of a website to always write clean HTML code, so we have to be able to deal with “rule violations” and unexpected structures. The solution at this point is the combination of ID and tag in the selector, in the form \"tag#id\". website %&gt;% html_nodes(css = &quot;span#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; A combination with the class attribute is also possible. website %&gt;% html_nodes(css = &quot;span.bundesland-name#bw&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; In general, it should be noted that the CSS selectors are case-sensitive. This means that we also need to be case sensitive. In the entry for Hamburg, the name of the id is capitalized: &lt;span class=&quot;bundesland-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; If we want to extract this element, we have to use this way of writing in the selector as well, \"#hh\" will not work. website %&gt;% html_nodes(css = &quot;span#hh&quot;) ## {xml_nodeset (0)} website %&gt;% html_nodes(css = &quot;span#HH&quot;) ## {xml_nodeset (1)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;HH&quot;&gt;Hamburg&lt;/span&gt; 5.1.3 Attributes ID and Class are attributes of HTML tags. Since both are particularly relevant in web design, the shortcuts presented above exist to select them quickly. However, we can use all attributes occurring in an HTML code to select elements. The corresponding CSS selector is written as tag[attribute]. At the bottom of our example HTML code, there are three &lt;a&gt; tags. These have, in addition to href=\"url\" – the linked page – the attribute target_=\"\". This specifies how the link should open. The value \"_blank\" opens the link in a new browser tab, \"_self\" in the active tab. For the second link no target=\"\" attribute is set. In our example, to select all &lt;a&gt; tags that have a target attribute, i.e. the first and third, we could proceed as follows: website %&gt;% html_nodes(css = &quot;a[target]&quot;) ## {xml_nodeset (2)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... ## [2] &lt;a href=&quot;https://webscraping-tures.github.io/css-selectors-developer-tool ... With \"element[attribute='value']\" it is possible to select only attributes with a certain value. Note that the value is enclosed in single quotes. The manner of writing \"element[attribute=\"value\"]\" would be split into the two strings \"element[attribute=\" and \"]\" and the R object value, which cannot be interpreted by R in this combination. Instead, we use single quotes to define the value of the attribute. This is a convention used in many programming contexts. Inside “ we use ‘, inside ‘ we use “ to be able to realize multiple levels of quotes. If we want to select only the link that opens in a new tab: website %&gt;% html_nodes(css = &quot;a[target=&#39;_blank&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... This can be further modified by specifying that the value of an attribute should have a certain beginning – \"element[attribute^='beginning']\" – a certain end – \"element[attribute$='end']\" – or contain a certain partial term – \"element[attribute*='partial term']\". Thus, we could select the links based on the beginning, the end, or any part of the URL assigned to the href=\"\" attribute. website %&gt;% html_nodes(css = &quot;a[href^=&#39;https://www&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://www.wahlrecht.de/ergebnisse/index.htm&quot; target=&quot;_blank&quot;&gt;\\ ... website %&gt;% html_nodes(css = &quot;a[href$=&#39;Deutschland&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://de.wikipedia.org/wiki/Flaggen_und_Wappen_der_L%C3%A4nder ... website %&gt;% html_nodes(css = &quot;a[href*=&#39;webscraping&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;https://webscraping-tures.github.io/css-selectors-developer-tool ... 5.1.4 Hierarchy levels We can think of the hierarchy of an HTML structure as analogous to a family tree. A simplified representation of our example page as a family tree could look like this: By clicking on the elements of the family tree, you can expand and collapse the hierarchy levels. Click once through the different levels and compare this with the HTML source code of the page. When you’re done with that, I suggest collapsing &lt;head&gt; and expanding only &lt;div id=\"daten\"&gt; in &lt;body&gt; and the first &lt;div&gt; at the next level down. With that, you should see everything we need below. With the metaphor of the family tree and the associated terms “descendant”, “child/parent”, and “sibling”, we should have an easier time understanding the slightly more advanced Selector concepts that follow. 5.1.4.1 Descendant If element A is a “descendant” of element B, this means that A “descended” from B over any number of generations. This can be one generation – i.e. a direct child-parent relationship; it can also be any number of generations – i.e. grandchildren and grandparents with any number of “grand” prefixes. In the selector, we write this as \"B A\". For example, if we want to select all &lt;span&gt; tags that descend from the &lt;div id=\"daten\"&gt; tag – that is, a grandchild-grandparent relationship: website %&gt;% html_nodes(css = &quot;div#daten span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; It should be noted that we can only select younger generations with CSS Selectors. So, for example, we can’t select the grandparents of the grandchildren by “flipping” the selector around. To select the grandparents – i.e. &lt;div id=\"daten\"&gt;, we would need to look at which element they descended from, for example &lt;body&gt;. CSS selectors are not limited to mapping the relationship of two generations. A longer selector containing four generations and leading to the same result could look like this: website %&gt;% html_nodes(css = &quot;body div#daten div span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; 5.1.4.2 Child/Parent If we want to define direct child-parent relationships instead of lineages across any number of generations, we can do this with selectors in the form \"B &gt; A\". This means A is a direct child of B. Since the &lt;span&gt; tags selected earlier are not direct children of &lt;div id=\"daten\"&gt;, \"div#daten &gt; span\" would not achieve the desired goal here. However, the &lt;span&gt; tags are direct children of &lt;div&gt; with classes \"bundesland-odd\" and \"bundesland-even\" respectively. Since there is no other direct child-parent relationship of &lt;span&gt; and &lt;div&gt; tags in our example, the selector \"div &gt; span\" would already be sufficient. However, we could become more explicit to write less error-prone code. Both &lt;div&gt; tags are similar in that their classes start with \"state\" and we already know how to exploit this: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; If we do not want to select all children, but only certain ones, there are a number of options. \":first-child\" selects the first child of an element: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; :first-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... However, these are not the &lt;span&gt; but the &lt;img&gt; tags, as a quick look at the family tree reminds us again. With \":nth-child(n)\" we can again specify more precisely which child – counted in the order in which they appear in the HTML code – we are interested in. To make it clearer again that we are only interested in &lt;span&gt; children, we combine \"span\" and \"nth-child(n)\": website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:nth-child(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; So we want to select all &lt;span&gt; tags that are the second child of the &lt;div&gt; tags whose classes start with \"state\". If we would like to select only the last child, we could do this with :last-child: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:last-child&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; 5.1.4.3 Sibling(s) The common children of a parent element can be seen as siblings within the metaphor of the family tree. On the lowest hierarchical level, we thus have four siblings per federal state in our example. One &lt;img&gt; and three &lt;span&gt; tags. The selector \"Element-A ~ Element-B\" selects all siblings B that follow sibling A: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; website %&gt;% html_nodes(css = &quot;img ~ span&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; The result for both notations is identical. If we had &lt;span&gt; tags that follow &lt;img&gt; tags in other places in the HTML code in our example, the second variant would no longer be explicit enough. The selector \"Element-A + Element-B\" works in the same way, but only selects the sibling B that follows directly after sibling A: website %&gt;% html_nodes(css = &quot;img + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;span.bundesland-name + span&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; The selection of siblings can be further restricted with \"element:first-of-type\", \"element:nth-of-type(n)\" and \"element:last-of-type\". In this way, the first, nth and last sibling of a certain type can be selected. The siblings are thus differentiated according to the type of element, with which our family tree metaphor is being somewhat overused. website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:first-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:nth-of-type(2)&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span:last-of-type&quot;) %&gt;% head(n = 2) ## {xml_nodeset (2)} ## [1] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [2] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; 5.1.5 Further selectors The wildcard \"*\" selects all elements. For example, we can also select all children of an element in the following way: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; *&quot;) %&gt;% head(n = 8) ## {xml_nodeset (8)} ## [1] &lt;img id=&quot;bw&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/6/ ... ## [2] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [3] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [4] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [5] &lt;img id=&quot;by&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/ ... ## [6] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [7] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [8] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; If we want to exclude certain elements from the selection, this is possible with \":not(selector)\". Here, all elements are selected, except those that we have explicitly excluded. For example, all children except the &lt;img&gt; elements: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; :not(img)&quot;) %&gt;% head(n = 6) ## {xml_nodeset (6)} ## [1] &lt;span class=&quot;bundesland-name&quot; id=&quot;bw&quot;&gt;Baden-Württemberg&lt;/span&gt; ## [2] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [3] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [4] &lt;span class=&quot;bundesland-name&quot; id=&quot;by&quot;&gt;Bayern&lt;/span&gt; ## [5] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [6] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; With \"selector-A, selector-B\", several selectors can be linked with “and/or”. This makes it possible, for example, to select two &lt;span&gt; classes in one step: website %&gt;% html_nodes(css = &quot;div[class^=&#39;bundesland&#39;] &gt; span.wahljahr, div[class^=&#39;bundesland&#39;] &gt; span.wahlbeteiligung&quot;) %&gt;% head(n = 4) ## {xml_nodeset (4)} ## [1] &lt;span class=&quot;wahljahr&quot;&gt;2016&lt;/span&gt; ## [2] &lt;span class=&quot;wahlbeteiligung&quot;&gt;70.4&lt;/span&gt; ## [3] &lt;span class=&quot;wahljahr&quot;&gt;2018&lt;/span&gt; ## [4] &lt;span class=&quot;wahlbeteiligung&quot;&gt;72.4&lt;/span&gt; You now know the selectors most commonly used in web scraping. An overview of these and all other CSS selectors can be found on the W3 pages: https://www.w3schools.com/cssref/css_selectors.asp In addition to CSS selectors, there is another way to select elements of an HTML page: XPath. This method is even more flexible and allows, among other things, the selection of “ancestors”, i.e. higher hierarchical levels, starting from a lower one. The price for the higher flexibility, however, is an often longer and somewhat more complicated syntax. In most cases, the CSS selectors will suffice. And if at some point you reach a point where they are no longer sufficient to achieve your scraping goal, you will already be so proficient in using the CSS selectors that you will find it very easy to switch to XPath. On the pages of the W3, you will find a suitable introduction.: https://www.w3schools.com/xml/xpath_intro.asp 5.2 Developer Tools Recognising the HTML structure and identifying functional selectors can be very difficult on complex websites, especially if the HTML code is not as clearly formatted as in our example. This is where it can be helpful to use the web developer tools built into modern browsers. On the following screenshots, you can see the application in Chromium for Linux on our running example page. However, the procedure is almost identical in other browsers such as Chrome, Firefox or Edge. In all cases, open the Developer Tools by right-clicking in the browser window and then selecting “Inspect Element”. Depending on the setting, the Developer Tools open in a horizontally or vertically separated area of the browser. We see a variety of tabs here, but for this introduction we will concentrate purely on the “Elements” tab. This shows us the HTML code in its hierarchical structure and allows us to expand and c ollapse individual elements. If we select an element in “Elements”, it will also be marked in the display in the browser window. We can also activate the “Inspector” and select elements directly in the browser window by clicking on it. This in turn selects the corresponding entry in the “Elements” tab. The bottom of the “elements” tab, shows us the full CSS selector belonging to the selected element. By right-clicking on an element and selecting “Copy” -&gt; “Copy selector”, we can also get a CSS selector in the clipboard, which we can then paste into RStudio with CTRL+V. For the element selected above, i.e. the state name for Baden-Württemberg, we get the selector \"#bw\". In this case, this short selector is sufficient to uniquely identify the element. However, as explained above, I recommend the construction of more unique and comprehensible selectors. However, the selector created by the Developer Tools, and especially the display of the full path in the “Elements” tab, can be very helpful in constructing these and allow us to quickly grasp the structure of a website. Also, although we learned how to select the name for Baden-Württemberg, we did not learn, for example, how to select all the names of the federal states. But here, too, the developer tools can be helpful. One approach would be to select several of the name elements one after the other and compare how the full CSS selector changes. If we identify similarities and differences between the name elements, this can give us starting points for formulating our own selector. Another possibility would be to use the SelectorGadget extension instead of the integrated developer tools. Among other things, this enables the selection of several elements at the same time. You can find information on how to use it at:https://selectorgadget.com/ "],["rvest2.html", "6 Scraping von Tabellen &amp; dynamischen Websites 6.1 Scraping von Tabellen 6.2 Dynamische Websites", " 6 Scraping von Tabellen &amp; dynamischen Websites 6.1 Scraping von Tabellen Im Web Scraping werden wir häufig das Ziel verfolgen, die extrahierten Daten in einen Tibble oder Data Frame zu überführen, um diese dann weiter analysieren zu können. Besonders dankbar ist es da, wenn die Daten an denen wir interessiert sind bereits in einer HTML-Tabelle gespeichert sind. Denn rvest erlaubt es uns mit der Funktion html_table() komplette Tabellen schnell und einfach auszulesen. Zur Erinnerung, Der HTML-Code für Tabellen ist in der Grundstruktur so gestaltet: &lt;table&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;Tag&lt;/th&gt; &lt;th&gt;Effekt&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&quot;b&quot;&lt;/td&gt; &lt;td&gt;bold&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&quot;i&quot;&lt;/td&gt; &lt;td&gt;italics&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; Der &lt;table&gt; Tag umfasst die gesamte Tabelle. Zeilen werden durch &lt;tr&gt; definiert, Spaltenüberschriften mit &lt;th&gt; und Zellen mit &lt;td&gt;. Bevor wir mit dem Scraping beginnen, laden wir wie immer die nötigen packages: library(tidyverse) library(rvest) 6.1.1 Tabelle mit CSS Selectors aus Wikipedia Auf der Wikipedia Seite zu “CSS”, findet sich auch eine Tabelle mit CSS Selectors. Diese ist unser Scraping Ziel. Lesen wir zunächst die Website ein: website &lt;- &quot;https://en.wikipedia.org/wiki/CSS&quot; %&gt;% read_html() Betrachten wir den Quellcode und suchen – STRG+F – nach “&lt;table”, erkennen wir, dass diese Seite eine Vielzahl von HTML-Tabellen enthält. Dazu gehören nicht ausschließlich die auf den ersten Blick als “klassische” Tabellen zu erkennenden Elemente, sondern unter anderem auch die “Infoboxen” am rechten oberen Rand des Artikels oder die ausklappbaren Auflistungen weiterführender Links am unteren Ende. Wenn Sie dies genauer betrachten möchten, können die Web Developer Tools hier sehr hilfreich sein. Statt einfach alle &lt;table&gt; nodes der Seite zu selektieren, könnte eine Strategie sein, mit Hilfe der WDT einen CSS Selector für diese spezifische Tabelle zu erzeugen: \"table.wikitable:nth-child(28)\". Wir selektieren so, die Tabelle der Klasse \"wikitable\" die das 28te Kind der übergeordneten Hierarchiebene ist – hier &lt;div class=\"mw-parser-output\"&gt;. Wenn wir nur ein einzelnes HTML-Element auswählen möchten, kann es hilfreich sein, statt html_nodes() die Funktion html_node() zu nutzen. node &lt;- website %&gt;% html_node(css = &quot;table.wikitable:nth-child(28)&quot;) nodes &lt;- website %&gt;% html_nodes(css = &quot;table.wikitable:nth-child(28)&quot;) nodes ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th ... node ## {html_node} ## &lt;table class=&quot;wikitable&quot;&gt; ## [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Pattern&lt;/th&gt;\\n&lt;th&gt;Matches&lt;/th&gt;\\n&lt;th&gt;First defined&lt;br&gt;i ... Der Unterschied besteht vor allem in der Ausgabe der Funktion. Dies ist erkennbar an dem Eintrag innerhalb der geschweiften Klammern im Output. Im ersten Fall, bekommen wir eine Liste von HTML Elementen – ein “xml_nodeset” –, auch wenn diese Liste, wie hier, nur aus einem Eintrag besteht. html_node() gibt uns direkt das HTML Element – “html_node” – als Ergebnis der Funktion. Warum ist dies relevant? In vielen Fällen kann es einfacher sein direkt mit dem HTML-Element statt einer Liste von HTML Elementen zu arbeiten, beispielsweise bei der Überführung von Tabellen in Data Frames und Tibbles, aber dazu später mehr. Um nun die so ausgewählte Tabelle auszulesen, müssen wir nur noch die Funktion html_table() auf unser Element anwenden. css_table_df &lt;- node %&gt;% html_table() css_table_df %&gt;% head(n = 4) ## Pattern ## 1 E ## 2 E:link ## 3 E:active ## 4 E::first-line ## Matches ## 1 an element of type E ## 2 an E element is the source anchor of a hyperlink of which the target is not yet visited (:link) or already visited (:visited) ## 3 an E element during certain user actions ## 4 the first formatted line of an E element ## First definedin CSS level ## 1 1 ## 2 1 ## 3 1 ## 4 1 Das Ergebnis ist ein Data Frame, der die gescrapten Inhalte der HTML-Tabelle enthält und dabei die in den &lt;th&gt; Tags hinterlegten Spaltennamen für die Spalten des Data Frames übernimmt. Durch die sehr langen Zellen in der Spalte “Matches”, ist der Output der R Studio Console leider nicht besonders hilfreich. Ein weiterer Vorteil der Nutzung von Tibbles statt Data Frames, ist es, dass lange Zelleninhalte in der Darstellung im Output automatisch abgekürzt werden. Um einen Data Frame in einen Tibble umzuwandeln, können wir die Funktion as_tibble() nutzen. css_table_tbl &lt;- node %&gt;% html_table() %&gt;% as_tibble() css_table_tbl %&gt;% head(n = 4) ## # A tibble: 4 x 3 ## Pattern Matches `First definedin CSS… ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 E an element of type E 1 ## 2 E:link an E element is the source anchor of a hype… 1 ## 3 E:active an E element during certain user actions 1 ## 4 E::first-l… the first formatted line of an E element 1 6.1.2 Scrapen mehrerer Tabellen Es könnte auch unser Scraping Ziel sein, nicht nur die erste, sondern alle vier inhaltlichen Tabellen des Wikipedia Artikels zu scrapen. Betrachten wir die vier Tabellen im Quellcode und/oder den WDTs, zeigt sich, dass sie alle die Klasse \"wikitable\" tragen. Damit können wir sie leicht selektieren. Hierbei bitte beachten, dass wieder die Funktion html_nodes() genutzt werden muss, da wir nicht mehr nur ein Element, sondern wieder eine Liste mehrerer selektierter Elemente benötigen. tables &lt;- website %&gt;% html_nodes(css = &quot;table.wikitable&quot;) %&gt;% html_table() Das Ergebnis ist eine Liste von vier Data Frames, welche jeweils eine der vier Tabellen enthalten. Möchten wir einen Einzelnen der Data Frames aus der Liste auswählen um ihn beispielsweise in ein neues Objekt zu überführen, sind wir auf subsetting angewiesen, welches wir bereits aus der Einführung in R kennen. Zur Erinnerung: es bestehen in R grundsätzlich zwei Möglichkeiten des subsettings von Listen. liste[#] und liste[[#]]. Der für uns relevanteste Unterschied besteht darin, was für eine Art Objekt R an uns zurückgibt. Im ersten Fall wird uns immer eine Liste zurückgegeben, auch wenn diese Möglicherweise nur aus einem Element besteht. Die Nutzung doppelter eckiger Klammern, gibt uns hingegen ein einzelnes Element direkt zurück. Der Unterschied ist also ähnlich zu dem zwischen html_nodes() und html_node(). Haben wir das Ziel beispielsweise den dritten Data Frame aus der Liste mit vier Data Frames auszuwählen, welches subsetting müssten wir nutzen? tables[3] %&gt;% str() ## List of 1 ## $ :&#39;data.frame&#39;: 7 obs. of 2 variables: ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... tables[[3]] %&gt;% str() ## &#39;data.frame&#39;: 7 obs. of 2 variables: ## $ Selectors : chr &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## $ Specificity: chr &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... Im ersten Fall sehen wir, dass wir eine Liste der Länge 1 haben, welche einen Data Frame mit 7 Zeilen und 2 Variablen enthält, sowie weitere Informationen zu diesen Variablen. Im zweiten Fall bekommen wir den Data Frame direkt, also nicht mehr als Element einer Liste. Wir müssen also liste[[]] nutzen um direkt einen einzelnen Data Frame aus einer Liste von Data Frames auswählen zu können. Sind wir stattdessen an einer Auswahl mehrerer Elemente aus einer Liste interessiert, ist dies nur mit liste[] möglich. Statt ein Element mit einer einzelnen Zahl auszuwählen, können wir mehrere mit einem Vektor von Zahlen in einem Schritt selektieren. tables[c(1, 3)] %&gt;% str() ## List of 2 ## $ :&#39;data.frame&#39;: 42 obs. of 3 variables: ## ..$ Pattern : chr [1:42] &quot;E&quot; &quot;E:link&quot; &quot;E:active&quot; &quot;E::first-line&quot; ... ## ..$ Matches : chr [1:42] &quot;an element of type E&quot; &quot;an E element is the source anchor of a hyperlink of which the target is not yet visited (:link) or already visited (:visited)&quot; &quot;an E element during certain user actions&quot; &quot;the first formatted line of an E element&quot; ... ## ..$ First definedin CSS level: int [1:42] 1 1 1 1 1 1 1 1 1 1 ... ## $ :&#39;data.frame&#39;: 7 obs. of 2 variables: ## ..$ Selectors : chr [1:7] &quot;h1 {color: white;}&quot; &quot;p em {color: green;}&quot; &quot;.grape {color: red;}&quot; &quot;p.bright {color: blue;}&quot; ... ## ..$ Specificity: chr [1:7] &quot;0, 0, 0, 1&quot; &quot;0, 0, 0, 2&quot; &quot;0, 0, 1, 0&quot; &quot;0, 0, 1, 1&quot; ... Wir bekommen als Ergebnis erneut eine Liste, welche die beiden hier ausgewählten Elemente enthält. 6.1.3 Tabellen mit NAs Was passiert, wenn wir versuchen eine Tabelle mit fehlenden Werten auszulesen? Betrachten Sie dazu folgendes Beispiel: https://webscraping-tures.github.io/table_na.html Auf den ersten Blick ist bereits ersichtlich, dass hier mehrere Zellen der Tabelle unbesetzt sind. Es fehlen also Werte. Versuchen wir die Tabelle trotzdem einmal einzulesen. table_na &lt;- &quot;https://webscraping-tures.github.io/table_na.html&quot; %&gt;% read_html %&gt;% html_node(css = &quot;table&quot;) table_na %&gt;% html_table() ## Error: Table has inconsistent number of columns. Do you want fill = TRUE? Wir bekommen eine hilfreiche Fehlermeldung die darüber informiert, dass die Anzahl der Spalten nicht über die gesamte Tabelle hinweg konstant ist. Netterweise wird uns auch direkt eine mögliche Lösung angeboten. Die Funktion html_table() kann mit dem Argument fill = TRUE dazu angewiesen werden, Zeilen mit abweichender Spaltenanzahl automatisch mit NA aufzufüllen. Dies steht für “Not Available” und repräsentiert fehlende Werte in R. wahlbet &lt;- table_na %&gt;% html_table(fill = TRUE) wahlbet %&gt;% head(n = 4) ## Bundesland Wahljahr Wahlbeteiligung ## 1 Baden-Württemberg 2016.0 NA ## 2 Bayern 2018.0 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg 61.3 NA Wie wir sehen, konnte html_table die vier Zellen mit fehlenden Werten mit NA füllen und die Tabelle trotz der Probleme einlesen. Allerdings, bestehen in dem HTML-Quellcode zwei unterschiedliche Arten von Problemen, welche die automatische Reparatur unterschiedlich gut handhaben kann. Betrachten wir dazu zunächst den Quellcode der ersten beiden Zeilen: &lt;tr&gt; &lt;td&gt;Baden-Württemberg&lt;/td&gt; &lt;td&gt;2016&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Bayern&lt;/td&gt; &lt;td&gt;2018&lt;/td&gt; &lt;/tr&gt; Für Baden-Württemberg sehen wir, dass die dritte Spalte zwar im Quellcode angelegt ist, sich aber kein Inhalt in dieser Zelle befindet. Dies hätte html_table() auch ohne fill = TRUE einlesen können und die Zelle mit einem NA gefüllt. Im Gegensatz dazu fehlt für Bayern die Zelle komplett. Das bedeutet, die zweite Zeile der Tabelle besteht nur aus zwei Spalten, während der Rest der Tabelle drei Spalten hat. Dies ist das Problem, auf welches uns die Fehlermeldung hingewiesen hat. Im Ergebnis konnte R in beiden Fällen den richtigen Schluss ziehen und die Zelle in beiden Zeilen mit einem NA füllen. Betrachten wir aber auch die dritte und vierte Zeile im Quellcode: &lt;tr&gt; &lt;td&gt;Berlin&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;66.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Brandenburg&lt;/td&gt; &lt;td&gt;61.3&lt;/td&gt; &lt;/tr&gt; Hier fehlt in beiden Fällen die zweite Spalte. Im ersten Fall ist sie angelegt aber leer, im zweiten existiert sie nicht. Mit dem ersten Fall kann html_table() erneut problemlos umgehen. Für Brandenburg stößt die Funktion aber an ihre Grenzen. Wir als menschliche Betrachter erkennen schnell, dass die letzte Landtagswahl in Brandenburg nicht im Jahr 61.3 stattfand und dies also die Wahlbeteiligung sein muss. R kann dies nicht so einfach unterscheiden und nimmt 61.3 als Wert für die Spalte “Wahljahr” und setzt für “Wahlbeteiligung” ein NA ein. Was tun? Zunächst einmal, sollten wir uns darüber bewusst sein, dass solche Probleme existieren. Wenn html_table()diese Fehlermeldung ausgibt, sollten wir also nicht einfach fill = TRUE setzen, sondern versuchen herauszufinden, warum das Problem besteht und ob die Option, es automatisch beheben zu lassen, uns auch tatsächlich zum Ziel führt. Ist dies nicht der Fall, könnte ein Lösungsansatz sein, eine eigene “Extraktorfunktion” zu schreiben, welche die Probleme direkt während des Scrapings behebt. Dies ist aber eine eher fortgeschrittene Methode und außerhalb des in dieser Einführung machbaren. Wir können aber immerhin die entstandenen Probleme im Nachhinein korrigieren. Unser Problem liegt ausschließlich in Zeile Vier. Deren zweite Spalte muss in die dritte verschoben werden und die zweite dann selbst als NA gesetzt werden. Dazu benötigen wir erneut subsetting. Im Falle eines Data Frames müssen wir dabei die Zeile und die Spalte in der Form df[zeile, spalte] angeben, um eine Zelle auszuwählen. So können wir R sagen: “Schreibe in Zelle Drei den Inhalt von Zelle Zwei, und dann schreibe in Zelle Zwei NA” wahlbet[4, 3] &lt;- wahlbet[4, 2] wahlbet[4, 2] &lt;- NA wahlbet %&gt;% head(n = 4) ## Bundesland Wahljahr Wahlbeteiligung ## 1 Baden-Württemberg 2016 NA ## 2 Bayern 2018 NA ## 3 Berlin NA 66.9 ## 4 Brandenburg NA 61.3 6.2 Dynamische Websites In der “Realität” des modernen Internets werden wir immer häufiger auf Websites stoßen, die nicht mehr ausschließlich auf statischen HTML-Dateien basieren, sondern Inhalte dynamisch generieren. Dies kennen Sie beispielsweise in Form von Timelines in Social Media Angeboten die dynamisch auf Basis Ihres Nutzerprofils generiert werden. Andere Websites generieren die dargestellten Inhalte möglicherweise mit JavaScript Funktionen oder als Reaktion auf Eingaben in HTML-Formularen. In vielen dieser Fälle reicht es aus Web Scraping Perspektive nicht mehr aus, eine HTML-Seite einzulesen und die gesuchten Daten zu extrahieren, da diese im HTML-Quellcode oft nicht enthalten sind, sondern dynamisch im Hintergrund geladen werden. Die gute Nachricht ist, dass es auch hier meist Möglichkeiten gibt, die Informationen trotzdem scrapen zu können. Möglicherweise bietet der Betreiber einer Seite oder eines Services eine API (Application Programming Interface) an. In diesem Fall können wir uns für einen Zugang zu dieser Schnittstelle registrieren und erhalten dann Zugang zu den Daten von Interesse. Dies ist beispielsweise bei Twitter möglich. In anderen Fällen können wir eventuell in den eingebetteten Scripts identifizieren, wie und aus welcher Datenbank die Informationen geladen werden und diese direkt ansteuern. Oder wir nutzen den Selenium WebDriver um ein Browserfenster “fernzusteuern” und das zu Scrapen, was der Browser “sieht”. Bei all diesen Ansätzen handelt es sich allerdings um fortgeschrittene Methoden, die den Umfang dieser Einführung übersteigen. In den Fällen, in denen auf Basis der Eingaben in ein Formular eine HTML-Datei dynamisch erzeugt wird, können wir diese aber mit den bereits bekannten Methoden auslesen. 6.2.1 HTML-Formulare und HTML-Queries Lassen Sie uns als Beispiel den OPAC Katalog der Universitätsbibliothek Potsdam https://opac.ub.uni-potsdam.de/ zunächst im Browser betrachten. Geben wir in das Suchfeld den Begriff “test” ein und klicken auf Suche, zeigt uns das Browserfenster die Ergebnisse der Suchanfrage. Was uns hier aber eigentlich interessiert ist die Adressleiste des Browsers. Statt der URL “https://opac.ub.uni-potsdam.de/”, steht dort nun eine deutlich längere URL in der Form: “https://opac.ub.uni-potsdam.de/DB=1/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test”. Der erste Teil ist offensichtlich weiterhin die URL der aufgerufenen Website, nennen wir dies die Base-URL. An das Ende der URL wurde aber der Teil “CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” angehangen. Dies ist der HTML-Query, an dem wir hier interessiert sind. Zwischen Base-URL und Query befinden sich noch ein oder mehrere Bestandteile, die sich in diesem Fall auch je nach ihrem Browser unterscheiden können. Diese sind für die eigentliche Suchanfrage aber auch irrelevant. Die verkürzte URL: “https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test” führt zu dem selben Ergebnis. Ein Query ist eine Anfrage, in der Daten aus einem HTML-Formular an den Server gesendet werden. Dieser generiert als Reaktion eine neue Website, welche an den Nutzer zurückgesendet und im Browser dargestellt wird. Ausgelöst wurde die Anfrage hier durch den Klick auf den “Suchen” Button. Wenn wir verstehen, was die Bestandteile des Querys bewirken, könnten wir diesen manipulieren und gezielt Nutzen um uns eine Website von Interesse erstellen zu lassen und diese auszulesen. 6.2.2 HTML-Formulare Dazu müssen wir zunächst einen Blick in den HTML-Code des Suchformulars werfen. Um dies nachvollziehen zu können, sollten Sie sich den Quellcode der Seite anzeigen lassen und nach “&lt;form” suchen oder die WDTs nutzen, um das Formular und seine Bestandteile zu betrachten. &lt;form action=&quot;CMD&quot; class=&quot;form&quot; name=&quot;SearchForm&quot; method=&quot;GET&quot;&gt; ... &lt;/form&gt; HTML-Formulare werden durch den &lt;form&gt; Tag umfasst. Innerhalb des Tags können ein oder mehrere Formular-Elemente wie Texteingabefelder, Drop-Down Optionslisten, Buttons usw. platziert werden. &lt;form&gt; selbst trägt in diesem Beispiel eine Reihe von Attributen. Interessant für uns ist als erstes das Attribut method=\"GET\". Dies legt die Methode der Datenübermittlung zwischen Client und Server fest. Wichtig ist dabei, dass die Methode “GET” für die Übermittlung von Daten Queries in der URL nutzt und die Methode “POST” nicht. Wir können Queries somit nur manipulieren, wenn die “GET” Methode genutzt wird. Ist keine Methode im &lt;form&gt; Tag festgelegt, wird als Standard ebenfalls “GET” genutzt. Das zweite für uns interessante Attribut, ist action=\"CMD\". Dies legt fest, welche Aktion ausgelöst werden soll nachdem das Formular übermittelt wurde. Oft ist der Wert von action= der Name einer Datei auf dem Server, an welche die Daten gesendet werden sollen und welche dann eine dynamisch erzeugte HTML-Seite zurück an den Nutzer sendet. Betrachten wir nun die Elemente des Formulars. Hierzu kann die rvest Funktion html_form() hilfreich sein. &quot;https://opac.ub.uni-potsdam.de/&quot; %&gt;% read_html() %&gt;% html_node(css = &quot;form&quot;) %&gt;% html_form() ## &lt;form&gt; &#39;SearchForm&#39; (GET CMD) ## &lt;select&gt; &#39;ACT&#39; [1/3] ## &lt;select&gt; &#39;IKT&#39; [0/13] ## &lt;select&gt; &#39;SRT&#39; [0/4] ## &lt;input checkbox&gt; &#39;FUZZY&#39;: Y ## &lt;input text&gt; &#39;TRM&#39;: ## &lt;input submit&gt; &#39;&#39;: Suchen Der Output zeigt uns in der ersten Zeile nochmals die Werte für method= und action= sowie den Namen des Formulars. Die weiteren sechs Zeilen zeigen, dass das Formular aus drei &lt;select&gt; sowie drei &lt;input&gt; Elementen besteht. Wir sehen außerdem die Namen dieser Elemente sowie den Standardwert, der beim Übermitteln des Formulars gesendet wird, solange kein anderer Wert ausgewählt oder eingegeben wurde. Schauen wir uns einige dieser Elemente an. &lt;select&gt; Elemente sind Drop-Down Listen mit Optionen, die ausgewählt werden können. Dies ist der Quellcode für das erste &lt;select&gt; Element unseres Beispiels: &lt;select name=&quot;ACT&quot;&gt; &lt;OPTION VALUE=&quot;SRCH&quot;&gt;suchen [oder] &lt;OPTION VALUE=&quot;SRCHA&quot; SELECTED&gt;suchen [und] &lt;OPTION value=&quot;BRWS&quot;&gt;Index bl&amp;auml;ttern &lt;/select&gt; Das Attribut name=\"ACT\" legt den Namen des Elements fest. Dieser wird bei der Übermittlung der Formulardaten über den Query genutzt. Über die ’Tags, werden die auswählbaren Optionen, also die Drop-Down Liste, definiert.&lt;value=\"\"&gt;steht dabei für den von Formular übermittelten Wert. Den Nutzer:innen wird der auf den Tag folgende Text angezeigt. Als Standardwert wird entweder der erste Wert der Liste genutzt oder wie hier eine Option mit dem Attributselected` explizit als Standard festgelegt. Die drei weiteren Elemente sind &lt;input&gt; Tags. Eingabefelder, deren spezifischer Typ über das Attributtype=\"\" festgelegt wird. Dies können beispielsweise Textboxen (type=\"text\") oder Checkboxes (input=\"checkbox\") sein, es stehen aber weit mehr Optionen zur Verfügung. Eine umfassende Liste finden sie unter: https://www.w3schools.com/html/html_form_input_types.asp. Hier der Quellcode zu Zwei der Drei ’` Elemente auf der Beispielseite: &lt;input type=&quot;text&quot; name=&quot;TRM&quot; value=&quot;&quot; size=&quot;50&quot;&gt; &lt;input type=&quot;submit&quot; class=&quot;button&quot; value=&quot; Suchen &quot;&gt; Der erste Tag ist vom Typ “text”, also ein Textfeld, genau genommen das Textfeld in welches der Suchbegriff eingegeben wird. Neben dem Namen des Elements wird über value=\"\" ein Standardwert des Feldes festgelegt. In diesem Fall ist der Standardwert ein leeres Feld. Der zweite Tag ist vom Typ “submit”. Dies ist der Button “Suchen”, welcher durch einen Klick die Übermittlung der Formulardaten über den Query auslöst. 6.2.3 Der Query Doch was wird jetzt genau übermittelt? Betrachten wir nochmals den Beispiels-Query von weiter oben: CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test Der Wert des action=\"\" Attributes bildet den ersten Bestandteil des Queries und wird nach der URL angehangen. Der Wert des Attributs teilt dem Server mit, was dieser mit den weiteren übermittelten Daten anfangen soll. Darauf folgt ein “?”, welches die zu übermittelnden Daten als mehrere Paare von name=\"\" und value=\"\" Attributen der einzelnen Elemente einleitet. Die Paare sind mit &amp; verbunden. “ACT=SRCHA” steht also dafür, dass im Element mit dem Namen “ACT” die Option “SRCHA” ausgewählt wurde. Wofür die Werte der beiden weiteren &lt;select&gt; Elemente “IKT” und “SRT” stehen, können sie selber mit einem Blick in den Quellcode oder die WDTs nachvollziehen. Als Wert des &lt;input type=\"text\"&gt; mit dem Namen “TRM” wird der in das Feld eingegebene Text übermittelt. Hier “test”. Der Server empfängt so die Formulardaten, kann auf Basis des action=\"\" Attributes, hier “CMD”, nachvollziehen, wie die Daten verarbeitet werden sollen und konstruiert dementsprechend die Website, die er an uns zurücksendet und die in unserem Browser dargestellt wird. 6.2.4 Manipulation des Queries und Scrapen des Ergebnis Da wir jetzt wissen, was die Bestandteile des Queries bedeuten, können wir diese auch gezielt manipulieren. Statt nun Queries per Hand zu schreiben, sollten wir R Nutzen um diese für uns zusammenzusetzen. Die Technik, URLs direkt im R-Code zu manipulieren, wird uns außerdem noch häufiger begegnen. Wir sollten sie also frühzeitig erlernen. Die Funktion str_c() setzt die als Argumente aufgelisteten Strings, also Folgen von Buchstaben, zu einem einzelnen String zusammen. Dabei können auch in anderen R Objekten gespeicherte Strings eingebunden werden. Haben wir das Ziel sowohl die Suchmethode als auch den Suchbgegriff zu manipulieren, könnten wir dies so erreichen: base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;test&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=test&quot; Verändern wir nun die in den Objekten method und term gespeicherten Strings und generieren die komplette URL erneut, werden diese Bestandteile des Queries entsprechend manipuliert. method &lt;- &quot;SRCH&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCH&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; Die Suchmethode wurde auf den Wert “SRCH” festgelegt, also eine “ODER” Suche, der Suchbegriff auf “web scraping”. Dabei ist wichtig zu beachten, dass im Query keine Leerzeichen auftauchen dürfen und diese beim Absenden des Formulars durch “+” ersetzt werden. Statt “web scraping” müssen wir also den String “web+scraping” nutzen. Als Beispielanwendung können wir nun dem Server eine “UND” Suche zu dem Begriff “web scraping” durchführen lassen, die vom Server generierte HTML-Seite auslesen und die dargestellten 10 Titel extrahieren. base_url &lt;- &quot;https://opac.ub.uni-potsdam.de/&quot; method &lt;- &quot;SRCHA&quot; term &lt;- &quot;web+scraping&quot; url &lt;- str_c(base_url, &quot;CMD?ACT=&quot;, method, &quot;&amp;IKT=1016&amp;SRT=YOP&amp;TRM=&quot;, term) url ## [1] &quot;https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&amp;IKT=1016&amp;SRT=YOP&amp;TRM=web+scraping&quot; website &lt;- url %&gt;% read_html() Die Suchergebnisse werden in der generierten HTML-Datei als Tabellen dargestellt. Der &lt;table&gt; Tag hat dabei die Attribut-Wert Kombination summary=\"hitlist\", was wir für unseren CSS Selector nutzen können: hits &lt;- website %&gt;% html_node(css = &quot;table[summary=&#39;hitlist&#39;]&quot;) %&gt;% html_table() %&gt;% as_tibble() hits %&gt;% head(n=10) ## # A tibble: 10 x 4 ## X1 X2 X3 X4 ## &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 NA NA &quot;&quot; NA ## 2 NA 1 &quot;Introduction to Data Systems : Building from Python/ Bres… NA ## 3 NA NA &quot;&quot; NA ## 4 NA 2 &quot;An Introduction to Data Analysis in R : Hands-on Coding, … NA ## 5 NA NA &quot;&quot; NA ## 6 NA 3 &quot;Quantitative portfolio management : with applications in … NA ## 7 NA NA &quot;&quot; NA ## 8 NA 4 &quot;Automate the boring stuff with Python : practical program… NA ## 9 NA NA &quot;&quot; NA ## 10 NA 5 &quot;Spotify teardown : inside the black box of streaming musi… NA Dies hat zwar funktioniert, wir sehen aber, dass die Tabelle vor allem aus leeren Zeilen und Zellen besteht. Diese sind auf der Website unsichtbar, werden aber zur Formatierung der Darstellung genutzt. Statt die Tabelle nun nachträglich zu reparieren, macht es mehr Sinn, gezielt nur die Zellen zu extrahieren, in welchen die von uns gesuchte Information enthalten sind. Dies sind die &lt;td&gt; Tags mit class=\"hit\" und der Attribut-Wert Kombination align=\"left\". Auf dieser Basis können wir einen eindeutigen CSS Selector konstruieren. hits &lt;- website %&gt;% html_nodes(css = &quot;td.hit[align=&#39;left&#39;]&quot;) %&gt;% html_text(trim = TRUE) hits %&gt;% head(n = 5) ## [1] &quot;Introduction to Data Systems : Building from Python/ Bressoud, Thomas. - 1st ed. 2020. - Cham : Springer International Publishing, 2020&quot; ## [2] &quot;An Introduction to Data Analysis in R : Hands-on Coding, Data Mining, Visualization and Statistics from Scratch/ Zamora Saiz, Alfonso. - 1st ed. 2020. - Cham : Springer International Publishing, 2020&quot; ## [3] &quot;Quantitative portfolio management : with applications in Python/ Brugière, Pierre. - Cham : Springer, [2020]&quot; ## [4] &quot;Automate the boring stuff with Python : practical programming for total beginners/ Sweigart, Albert. - 2nd edition. - San Francisco : No Starch Press, [2020]&quot; ## [5] &quot;Spotify teardown : inside the black box of streaming music/ Eriksson, Maria. - Cambridge, Massachusetts : The MIT Press, [2019]&quot; 6.2.5 Weiterführende Quellen Um diese Informationen weiter zu verarbeiten und beispielsweise in Daten zu Autor, Titel, Jahr usw. zu trennen, sind erweiterte Kenntnisse im Umgang mit Strings notwendig, die leider über diese Einführung hinausgehen. Eine gute erste Übersicht finden sie im Kapitel “Strings” aus “R for Data Science” von Wickham und Grolemund: https://r4ds.had.co.nz/strings.html Dazu sei auch der passende “Cheat Sheet” empfohlen: https://raw.githubusercontent.com/rstudio/cheatsheets/master/strings.pdf "],["rvest3.html", "7 Scraping mehrseitiger Websites 7.1 Indexseiten 7.2 Pagination", " 7 Scraping mehrseitiger Websites In vielen Fällen, möchten wir nicht den Inhalt einer einzelnen Website scrapen, sondern mehrere Unterseiten in einem Arbeitsschritt. In dieser Sitzung betrachten wir zwei häufig auftretende Varianten. Indexseiten und Pagination. 7.1 Indexseiten Eine Indexseite meint in diesem Verständnis, eine Website auf der Links zu den diversen Unterseiten aufgelistet sind. Dies können wir uns als eine Art Inhaltsverzeichnis vorstellen. Als Beispiel soll die Website zu den tidyverse packages dienen: https://www.tidyverse.org/packages/. Unter dem Punkt “Core tidyverse” sind die acht packages gelistet, welche in R mit library(tidyverse) geladen werden. Neben Namen und Icon ist jeweils eine kurze Beschreibung des package sowie ein weiterführender Link Teil der Liste. Betrachten wir eine der Unterseiten zu den core packages. Da diese alle gleich aufgebaut sind, können Sie als Beispiel ein beliebiges package wählen. Es könnte unser Scraping-Ziel sein, eine Tabelle mit den namen der core packages, den aktuellen Versionsnummer, und den Links zu CRAN und dem passenden Kapitel in “R for Data Science” von Hadley und Wickham zu erstellen. Inzwischen haben wir alle Werkzeuge um diese Daten aus den Websiten zu extrahieren. Wir könnten nun also “per Hand” die einzelnen Unterseiten scrapen und die Daten zusammenführen. Praktischer wäre es aber doch, könnten wir ausgehend von der Indexseite alle acht Unterseiten und die jeweils enthaltenen Daten von Interesse in einem Arbeitsschritt scrapen. Genau dies, betrachten wir im Weiteren. Noch eine kurze Anmerkung zu CRAN: Es handelt sich um das “Comprehensive R Archive Network”. Dies haben Sie schon bei der Installation von R kennengelernt, es werden aber auf dieser Website auch die diversen R packages, die zugehörige Dokumentation sowie diverse Informationen “gehostet”. Wenn Sie ein package direkt aus RStudio installieren, greift dieses auf CRAN zurück um das package und die zugehörigen Dateien zu findne und herunterzuladen. 7.1.1 Scrapen des Index library(tidyverse) library(rvest) Als ersten Schritt, müssen wir die Links zu den Unterseiten aus dem Quellcode der Indexseite extrahieren. Dazu laden wir wie immer, die Website herunter und lesen sie ein. website &lt;- &quot;https://www.tidyverse.org/packages/&quot; %&gt;% read_html() Die Links sind in diesem Fall zweifach im Quellcode hinterlegt. Einmal ist das Bild des Icons verlinkt, einmal der Name des Packages. Sie können dies inzwischen im Quellcode und/oder mit den WDTs selbst nachvollziehen. Wir benötigen jeden einzelnen Link jedoch auch nur einmal. Eine Möglichkeit von mehreren, diese zu selektieren, könnte es sein die &lt;a&gt; Tags auszuwhäen die direkt auf die einzelnen &lt;div class=\"package\"&gt; Tags folgen. a_nodes &lt;- website %&gt;% html_nodes(css = &quot;div.package &gt; a&quot;) a_nodes ## {xml_nodeset (8)} ## [1] &lt;a href=&quot;https://ggplot2.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... ## [2] &lt;a href=&quot;https://dplyr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [3] &lt;a href=&quot;https://tidyr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [4] &lt;a href=&quot;https://readr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [5] &lt;a href=&quot;https://purrr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class=&quot; ... ## [6] &lt;a href=&quot;https://tibble.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class= ... ## [7] &lt;a href=&quot;https://stringr.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... ## [8] &lt;a href=&quot;https://forcats.tidyverse.org/&quot; target=&quot;_blank&quot;&gt;\\n &lt;img class ... Da wir die reinen URLs benötigen um im Folgenden die Unterseiten auslesen zu können, sollten wir nun die Werte der href=\"\" Attribute extrahieren. links &lt;- a_nodes %&gt;% html_attr(name = &quot;href&quot;) links ## [1] &quot;https://ggplot2.tidyverse.org/&quot; &quot;https://dplyr.tidyverse.org/&quot; ## [3] &quot;https://tidyr.tidyverse.org/&quot; &quot;https://readr.tidyverse.org/&quot; ## [5] &quot;https://purrr.tidyverse.org/&quot; &quot;https://tibble.tidyverse.org/&quot; ## [7] &quot;https://stringr.tidyverse.org/&quot; &quot;https://forcats.tidyverse.org/&quot; 7.1.2 Iteration mit map() Bevor mit dem Einlesen der Unterseiten und dem Etxrahieren beginnen, müssen wir uns Gedanken dazu machen, wie wir R dazu bringen können diese Arbeitsschritte automatisch nacheinander auf mehrere URLs anzuwenden. Eine Möglichkeit aus base R wäre es, einen “For Loop” anzuwenden. Ich möchte Ihnen hier jedoch die map() Funktion aus dem tidyverse package purrr vorstellen. Diese folgen der grundsätzlichen Logik des tidyverse, lassen sich problemlos in Pipes einbinden und haben eine kurze und intuitiv verständliche Syntax. Die Funktion map() nimmt einen Vektor oder eine Liste als Input, wendet eine im zweiten Argument spezifizierte Funktion auf jedes der Elemente des Input an und gibt uns eine Liste mit den Ergebnissen der angewandten Funktion zurück. x &lt;- c(1.28, 1.46, 1.64, 1.82) map(x, round) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1 ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] 2 Für jedes Element des numerischen Vektors x, wendet map() einzeln die Funktion round() an. round() macht das, was der Name vermuten lässt, und rundet den Input auf oder ab, je nach Zahlenwert. Als Ergebnis, gibt map() eine Liste aus. Möchten wir, einen Vektor als Output haben, können wir je nach gewünschtem Typ – Logical, Integer, Double oder Character – spezifische Varianten der Map Funktionen anwenden. Dazu ein Zitat aus der Hilfe zu ?map: “map_lgl(), map_int(), map_dbl() and map_chr() return an atomic vector of the indicated type (or die trying).” Möchten wir als beispielsweise für das obige Beispiel einen numerischen Vektor statt einer Liste als Output haben, können wir map_dbl() nutzen: x &lt;- c(1.28, 1.46, 1.64, 1.82) map_dbl(x, round) ## [1] 1 1 2 2 Oder für einen Character-Vektor, map_chr(). Die hier angewandte Funktion toupper() gibt den Input als Goßbuchstaben aus. x &lt;- c(&quot;abc&quot;, &quot;def&quot;, &quot;gah&quot;) map_chr(x, toupper) ## [1] &quot;ABC&quot; &quot;DEF&quot; &quot;GAH&quot; Möchten wir weitere Argumente der angewandten Funktion verändern, werden diese nach dem Namen der Funktion aufgezählt. Hier wird die Anzahl der Nachkommastellen auf die gerundet werden soll von dem Standardwert 0 auf 1 gesetzt. x &lt;- c(1.28, 1.46, 1.64, 1.82) map_dbl(x, round, digits = 1) ## [1] 1.3 1.5 1.6 1.8 Damit haben wir einen Überblick zur Iteration mit map() dies kann aber notwendigerweise nur eine erste Einführung sein. Für eine weitergehende Einführung zu For Loops und den Map Funktionen empfiehlt sich das Kapitel zu “Iteration” aus “R for Data Science”: https://r4ds.had.co.nz/iteration.html 7.1.3 Scrapen der Unterseiten Wir können nun, map() nutzen um alle Unterseiten in einem Schritt einzulesen. Als Input geben wir den Character Vector, welche die URLs der Unterseiten enthält an und als anzuwendende Funktion die bekannte read_html(). Für jede der acht URLs wird also nacheinander die Funktion auf die jeweilige URL angewandt. Als Output bekommen wir eine Liste der acht ausgelesenen Unterseiten. pages &lt;- links %&gt;% map(read_html) Betrachten wir die Unterseiten im Browser, können wir feststellen, dass die HTML-Struktur im Bezug auf die uns interessierenden Informationen – Name, Versionsnummer und CRAN sowie “R for Data Science” Links – für jede Unterseite identisch ist. Wir können also für jede dieser, die Daten über die selben CSS Selectors extrahieren. pages %&gt;% map(html_node, css = &quot;a.navbar-brand&quot;) %&gt;% map_chr(html_text) ## [1] &quot;ggplot2&quot; &quot;dplyr&quot; &quot;tidyr&quot; &quot;readr&quot; &quot;purrr&quot; &quot;tibble&quot; &quot;stringr&quot; ## [8] &quot;forcats&quot; Der Name des packages wird in der Menüleiste im oberen Abschnitt der Seiten dargestellt. Dieser wird von einem &lt;a&gt; Tag umschlossen. Für https://ggplot2.tidyverse.org/ ist dies beispielsweise: &lt;a class=\"navbar-brand\" href=\"index.html\"&gt;ggplot2&lt;/a&gt;. Wir können also auf einfachem Weg einen Selector konstruieren, eine Möglichkeit ist der hier genutzte. Was passiert nun im Detail in dem dargestellten Code? Der Input ist die zuvor erstellte Liste mit den acht eingelesenen Websites. In der zweiten Zeile wird durch die Nutzung von map() auf jede der eingelesenen Seiten die Funktion html_node() mit dem Argument css = \"a.navbar-brand\" angewandt. Für jede der acht Seiten, wird also nacheinander der entsprechende HTML-node ausgewählt. Diese werden durch die Pipe in die dritte Zeile weitergegebe, in der erneut über jeden node iteriert wird, diesmal die Ihnen bekannt Funktion html_text(). Für jeden der acht selektierten Nodes wird also der zwischen Start- und End-Tag befindliche Text extrahiert. Da hier map_chr() genutzt wird, wird ein Character Vector als Output zurückgegeben. pages %&gt;% map(html_node, css = &quot;span.version.version-default&quot;) %&gt;% map_chr(html_text) ## [1] &quot;3.3.2&quot; &quot;1.0.5&quot; &quot;1.1.3&quot; &quot;1.4.0&quot; &quot;0.3.4&quot; ## [6] &quot;3.1.0&quot; &quot;1.4.0.9000&quot; &quot;0.5.1&quot; Die Extration der aktuellen Versionsnummer der packages, funktioniert auf dem selben Weg. Diese sind für ggplot2 in folgendem tag enthalten: &lt;span class=\"version version-default\" data-toggle=\"tooltip\" data-placement=\"bottom\" title=\"Released version\"&gt;3.3.2&lt;/span&gt;. Auch dieser lässt sich einfach selektieren, wir sehen aber ein interessantes Detail. Und zwar enthält der Klassenname hier ein Leerzeichen. Dies steht dafür, dass der &lt;span&gt; Tag sowohl die Klasse version als auch version-default trägt. Dies können wir selektieren indem wir beide Klassennamen im Selector mit einem . an span anhängen. Streng genommen, müssen wir dies hier aber nicht tun. Beide Klassennamen für sich, reichen hier zur Selektion aus, da sie an keiner weiteren Stelle auf der Website auftauchen. Im Sinne von möglichst expliziten CSS Selectors, würde ich trotzdem beide Klassennamen nutzen, dies ist aber auch “Geschmackssache”. pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(1) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ## [1] &quot;https://cloud.r-project.org/package=ggplot2&quot; ## [2] &quot;https://cloud.r-project.org/package=dplyr&quot; ## [3] &quot;https://cloud.r-project.org/package=tidyr&quot; ## [4] &quot;https://cloud.r-project.org/package=readr&quot; ## [5] &quot;https://cloud.r-project.org/package=purrr&quot; ## [6] &quot;https://cloud.r-project.org/package=tibble&quot; ## [7] &quot;https://cloud.r-project.org/package=stringr&quot; ## [8] &quot;https://cloud.r-project.org/package=forcats&quot; pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(4) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ## [1] &quot;https://r4ds.had.co.nz/data-visualisation.html&quot; ## [2] &quot;http://r4ds.had.co.nz/transform.html&quot; ## [3] &quot;https://r4ds.had.co.nz/tidy-data.html&quot; ## [4] &quot;http://r4ds.had.co.nz/data-import.html&quot; ## [5] &quot;http://r4ds.had.co.nz/iteration.html&quot; ## [6] &quot;https://r4ds.had.co.nz/tibbles.html&quot; ## [7] &quot;http://r4ds.had.co.nz/strings.html&quot; ## [8] &quot;http://r4ds.had.co.nz/factors.html&quot; Auch die Extraktion der Links, erfolgt nach dem selben Grundprinzip. Der Selector wird etwas komplizierter, lässt sich unter Nutzung der WDTs aber gut nachvollziehen. Wir selektieren die &lt;a&gt; Tags des ersten beziehungsweise vierten &lt;li&gt; Kinds der ungeordneten Liste der Klasse list-unstyled. Hier wenden wir jeweils die Funktion html_attr() mit dem Argument name = \"href\" auf die acht selektierten Nodes an um die Daten von Interesse zu erhalten, die URL der Links. Sind wir nur an dem Endergebnis interessiert, können wir die Extraktion der Daten der Unterseiten auch direkt während der Erstellung eines Tibbles durchführen: tibble( name = pages %&gt;% map(html_node, css = &quot;a.navbar-brand&quot;) %&gt;% map_chr(html_text), version = pages %&gt;% map(html_node, css = &quot;span.version.version-default&quot;) %&gt;% map_chr(html_text), CRAN = pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(1) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;), Learn = pages %&gt;% map(html_node, css = &quot;ul.list-unstyled &gt; li:nth-child(4) &gt; a&quot;) %&gt;% map_chr(html_attr, name = &quot;href&quot;) ) ## # A tibble: 8 x 4 ## name version CRAN Learn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ggplot2 3.3.2 https://cloud.r-project.org/… https://r4ds.had.co.nz/data-v… ## 2 dplyr 1.0.5 https://cloud.r-project.org/… http://r4ds.had.co.nz/transfo… ## 3 tidyr 1.1.3 https://cloud.r-project.org/… https://r4ds.had.co.nz/tidy-d… ## 4 readr 1.4.0 https://cloud.r-project.org/… http://r4ds.had.co.nz/data-im… ## 5 purrr 0.3.4 https://cloud.r-project.org/… http://r4ds.had.co.nz/iterati… ## 6 tibble 3.1.0 https://cloud.r-project.org/… https://r4ds.had.co.nz/tibble… ## 7 stringr 1.4.0.90… https://cloud.r-project.org/… http://r4ds.had.co.nz/strings… ## 8 forcats 0.5.1 https://cloud.r-project.org/… http://r4ds.had.co.nz/factors… 7.2 Pagination Eine weitere häufig auftretende Form der Unterteilung einer Website in mehrere Unterseiten, ist Pagination. Dies kennen Sie alle aus dem Alltag im Internet. Wir geben einen Suchbegriff bei Google ein und bekommen Resultate die über mehrere Seiten aufgeteilt sind. Diese sind über die numerierten Links sowie die Vorwärts-/Rückwärtspfeile am unteren Ende des Browserinhalts zugänglich und navigierbar. Dies ist “Pagination in action” und begegnet uns in vergleichbaren Varianten auf vielen Websites. Im Pressemitteilungsarchiv der Website des Landtags Brandenburg wird ebenfalls Pagination genutzt um die Mitteilungen über mehrere Unterseiten zu verteilen. Zur Veranschaulichung des Scrapings einer solchen Website, können wir beispielsweise das Ziel verfolgen, das Datum, den Titel und den weiterführenden Link für alle Pressemitteilungen aus 2020 zu Scrapen und in einem Tibble zusammenzufassen. Die Website finden Sie unter: https://www.landtag.brandenburg.de/de/aktuelles/presse/pressemitteilungsarchiv/archiv_pressemitteilungen_2020/980521 7.2.1 Der Query Betrachten wir die Seite zunächst im Browser und untersuchen was passiert, wenn wir uns durch die Unterseiten klicken. Wählen wir die zweite Unterseite aus, verändert sich die URL im Browserfenster. Wir sehen, dass das Ende der URL von “980521” zu “980521?skip=15” erweitert wird. Es wird also ein Query an den Server geschickt und die entsprechende Unterseite zurückgesendet und dargestellt. Auf der dritten Unterseite ändert sich der Query wiederum zu “980521?skip=30”. Was könnte nun “skip=15” bedeuten? Betrachten wir die Auflistung der Pressemitteilungen, sehen wir, dass auf jeder Unterseite genau 15 Mitteilungen dargestellt werden. Wir können also davon ausgehen, dass “skip=15” dem Server die Anweisung gibt, die ersten 15 Mitteilungen zu überspringen und somit die Einträge 16-30 darzustellen. “skip=30” überspringt dann die ersten 30 und so weiter. Das Prinzip lässt sich weiter bestätigen, indem wir testen was “skip=0” auslöst. So wird wieder die erste Unterseite dargestellt. “980521” ist also eigentlich äquivalent zu “980521?skip=0”. Damit wissen wir bereits, dass wir dazu in der Lage sein werden, die URLs direkt aus Rstudio zu manipulieren und so die Unterseiten zu Scrapen. 7.2.2 Scrapen der Unterseiten Bevor wir damit beginnen, alle Pressemitteilungen zu scrapen, müssen wir zunächst herausfinden, wie viele Unterseiten vorhanden sind. Der einfachste Weg wäre es, dies per Auge zu machen. Wir sehen ja im Browser, dass die höchste auswählbare Unterseite die “12” ist. Wir können dies aber auch im Scraping Prozess selbst herausfinden. Dies hat mehrere Vorteile. Eventuell möchten wir ja nicht nur die Mitteilungen aus 2020 scrapen sondern die mehrerer oder aller Jahre. Dazu müssten wir für jedes Jahr im Browser prüfen, wie viele Unterseiten vorhanden sind und dies entsprechend anpassen. Extrahieren wir die Seitenzahl im R Code, lässt sich dieser einfach auf andere Jahre mir abweichenden Seitenzahlen generalisieren. Wäre es hingegen das Ziel die Mitteilungen aus 2021 zu scrapen, müssten wir bei jeder Wiederholung des Vorgangs im Laufe des Jahres erneut prüfen, ob sich die Seitenzahl verändert hat. Durch die Extraktion im Code, entfällt dieser Schritt und wir können das Script regelmäßig mit minimalem Aufwand ausführen und dabei sicher sein, dass es funktional bleibt. Die Links zu den Unterseiten, sind im HTML-Code in einer ungeordneten Liste (&lt;ul&gt;) enthalten. Das vorletzte Listenelement &lt;li&gt; enthält dabei die Seitenzahl der letzten Seite, hier “12”. Bitte beachten Sie, dass das letzte Listenelement der “Vorwärts” Button ist. Mit diesen Informationen können wir einen Selector konstruieren und die Seitenzahl extrahieren. website &lt;- &quot;https://www.landtag.brandenburg.de/de/aktuelles/presse/pressemitteilungsarchiv/archiv_pressemitteilungen_2020/980521&quot; %&gt;% read_html() max &lt;- website %&gt;% html_node(css = &quot;ul.pagination.pagination-sm &gt; li:nth-last-child(2)&quot;) %&gt;% html_text() %&gt;% as.numeric() In der letzten Zeile des oben stehenden Codes, sehen sie die Funktion as.numeric(). html_text() gibt uns stets einen Character Vector zurück. Da wir aber den Wert als Zahl benötigen, um in R damit rechnen zu können, müssen wir diesen noch in eine Zahl umwandeln. Dies erledigt as.numeric(). Nun können wir damit beginnen, die Links zu allen Unterseiten direkt in unserem R Script zu konstruieren. Dazu benötigen wir zwei Komponenten die wir im Anschluss zu den URLs kombinieren können. Zunächst müssen wir den unveränderlichen Teil der URL definieren, die base URL. Dies ist in diesem Fall die komplette URL bis “?skip=” inklusive. Zusätzlich benötigen wir die Werte die nach “?skip=” eingefügt werden. Diese können wir einfach berechen. Jede Unterseite enthält 15 Pressemitteilungen. Wir können also die Nummer der Unterseite mit 15 multiplizieren, müssen dann aber nochmals 15 subtrahieren, da die 15 auf dieser Seite dargestellten Mittelungen nicht geskipped werden sollen. Für Seite 1 berechnen wir also: \\(1 * 15 - 15 = 0\\), für Seite 2: \\(2 * 15 - 15 = 15\\) und so weiter. Um dies in einem Schritt für alle Unterseiten zu erledigen, können wir mit 1:max * 15 - 15 R anweisen, die Berechnung für alle Zahlen von 1 bis zum Maximalwert – den wir zuvor in dem Objekt max gespeichert haben – zu wiederholen. : steht dabei für “von-bis”. So erhalten wir einen numerischen Vektor mit den Werten für “?skip=”. Im dritten Schritt können wir die base URL und die berechneten Werte mit str_c() zu vollständigen URLs kombinieren und diese im vierten Schritt mit map() auslesen. base_url &lt;- &quot;https://www.landtag.brandenburg.de/de/aktuelles/presse/pressemitteilungsarchiv/archiv_pressemitteilungen_2020/980521?skip=&quot; skips &lt;- 1:max * 15 - 15 skips ## [1] 0 15 30 45 60 75 90 105 120 135 150 165 links &lt;- str_c(base_url, skips) pages &lt;- links %&gt;% map(read_html) Nun können wir die uns interessierenden Daten extrahieren. Beginnen wir mit dem Datum der Pressemitteilung. Dieses wird durch einen &lt;p&gt; Tag der Klasse date umschlossen. Mit map() selektieren wir zunächst die entsprechenden Nodes und im nächsten Schritt den Text dieser Nodes. Da das Resultat an diesem Punkt eine Liste von Listen ist, und dies in der weiteren Bearbeitung unnötig kompliziert wäre, können wir die Liste durch unlist() auflösen und erhalten einen Character Vektor als Output. pages %&gt;% map(html_nodes, css = &quot;p.date&quot;) %&gt;% map(html_text) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;30.12.2020&quot; &quot;17.12.2020&quot; &quot;11.12.2020&quot; &quot;10.12.2020&quot; &quot;04.12.2020&quot; Wir können aber noch einen Schritt weitergehen und die Daten als Vektor des Typs “Date” speichern. Dies könnte für potentielle weitere Analysen von Vorteil sein. Das tidyverse package lubridate ermöglicht es auf einfache Weise Datumsangaben aus Character oder numerischen Vektoren in das “Date” Format umzuwandeln. Das package ist nicht Teil des core tidyverse und muss dementsprechend explizit geladen werden. Unter anderem bietet es eine Reihe von Funktionen in der Form dmy(), mdy, ymd() und so weiter an. d steht dabei für “day”, m für “month” und y für “year”. Mit der Reihenfolge in der die Buchstaben im Funktionsnamen auftauchen, teilen wir R mit, welches Format die Daten haben, die wir das “Date” Format umwandeln möchten. Auf der Website des Landtags, sind die Datumsangaben in der in Deutschland typischen Form Tag.Monat.Jahr verfasst. Wir nutzen also die Funktion dmy(). Wären sie beispielsweise in der in den USA typischen Form Monat.Tag.Jahr abgelegt, müssten wir entsprechend mdy() nutzen. Dabei ist es irrelevant ob die Bestandteile des Datums mit “.”, “/”, “-” oder Leerzeichen getrennt sind. Selbst ausgeschriebene oder abgekürzte Monatsnamen kann lubridate verarbeiten. library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union test &lt;- pages %&gt;% map(html_nodes, css = &quot;p.date&quot;) %&gt;% map(html_text) %&gt;% unlist() %&gt;% dmy() %&gt;% head(n = 5) Mehr zum Umgang mit Datums- und Zeitangaben in R sowie den weiteren Möglichkeiten die lubridate eröffnet, finden sie im entsprechenden Kapitel in “R for Data Science”: https://r4ds.had.co.nz/dates-and-times.html Als nächstes können wir die Titel der Pressemitteilungen extrahieren. Inzwischen werden Sie dazu in der Lage sein den Code und den CSS Selector dazu selbst nachzuvollziehen. pages %&gt;% map(html_nodes, css = &quot;p.result-name&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;Zum Tode von Paul-Heinz Dittrich&quot; ## [2] &quot;Symbol für Zusammenhalt auch in schwierigen Zeiten: Parlament und Regierung erhalten Fotocollage zu Erntekronen&quot; ## [3] &quot;Termine des Landtages Brandenburg in der Zeit vom 12. bis 20. Dezember 2020&quot; ## [4] &quot;Hinweise für Medien zu den Plenarsitzungen des Landtages vom 15. bis 18. Dezember 2020&quot; ## [5] &quot;Termine des Landtages Brandenburg in der Zeit vom 7. bis 13. Dezember 2020&quot; Als letztes folgen die Links zu den einzelnen Mitteilungen. Auch hier passiert zunächst nichts überraschendes mehr. pages %&gt;% map(html_nodes, css = &quot;li.ce.ce-teaser &gt; a&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% head(n = 5) ## [1] &quot;/de/meldungenzum_tode_von_paul-heinz_dittrich/980480?_referer=980521&quot; ## [2] &quot;/de/meldungensymbol_fuer_zusammenhalt_auch_in_schwierigen_zeiten:_parlament_und_regierung_erhalten_fotocollage_zu_erntekronen/979730?_referer=980521&quot; ## [3] &quot;/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_12._bis_20._dezember_2020/978690?_referer=980521&quot; ## [4] &quot;/de/meldungenhinweise_fuer_medien_zu_den_plenarsitzungen_des_landtages_vom_15._bis_18._dezember_2020/976366?_referer=980521&quot; ## [5] &quot;/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_7._bis_13._dezember_2020/974725?_referer=980521&quot; Aber, die im HTML-Code hinterlegten Links, beschreiben nur einen Teil der vollständigen URL. Wir könnten nun also erneut mit str_c() die vollständigen URLs konstruieren. Dazu benötigen wir jedoch noch ein neues Konzept. Die Pipe gibt das Ergebnis eines Arbeitsschritts an die nächste Zeile weiter. Nutzen wir str_c() innerhalb der Pipe, bekommt diese also den extrahierten Endteil der URLs als erstes Argument weitergegeben. str_c(\"https://www.landtag.brandenburg.de\") würde also dazu führen, das der Endteil der URL, vor “https://www.landtag.brandenburg.de” angehangen wird. Wie möchten aber logischerweise, dass dies anders herum geschieht. Dazu müssen wir str_c() mitteilen, dass die durch die Pipe weitergegebenen Daten als zweites Argument genutzt werden. Dies können wir durch . erreichen. . steht für die durch die Pipe weitergegebenen Daten. So können wir die URLs korrekt kombinieren: pages %&gt;% map(html_nodes, css = &quot;li.ce.ce-teaser &gt; a&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% str_c(&quot;https://www.landtag.brandenburg.de&quot;, .) %&gt;% head(n = 5) ## [1] &quot;https://www.landtag.brandenburg.de/de/meldungenzum_tode_von_paul-heinz_dittrich/980480?_referer=980521&quot; ## [2] &quot;https://www.landtag.brandenburg.de/de/meldungensymbol_fuer_zusammenhalt_auch_in_schwierigen_zeiten:_parlament_und_regierung_erhalten_fotocollage_zu_erntekronen/979730?_referer=980521&quot; ## [3] &quot;https://www.landtag.brandenburg.de/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_12._bis_20._dezember_2020/978690?_referer=980521&quot; ## [4] &quot;https://www.landtag.brandenburg.de/de/meldungenhinweise_fuer_medien_zu_den_plenarsitzungen_des_landtages_vom_15._bis_18._dezember_2020/976366?_referer=980521&quot; ## [5] &quot;https://www.landtag.brandenburg.de/de/meldungentermine_des_landtages_brandenburg_in_der_zeit_vom_7._bis_13._dezember_2020/974725?_referer=980521&quot; Wie immer, können wir die komplette Extraktion der Daten auch während der Konstruktion eines Tibbles durchführen: tibble( date = pages %&gt;% map(html_nodes, css = &quot;p.date&quot;) %&gt;% map(html_text) %&gt;% unlist() %&gt;% dmy(), name = pages %&gt;% map(html_nodes, css = &quot;p.result-name&quot;) %&gt;% map(html_text, trim = TRUE) %&gt;% unlist(), link = pages %&gt;% map(html_nodes, css = &quot;li.ce.ce-teaser &gt; a&quot;) %&gt;% map(html_attr, name = &quot;href&quot;) %&gt;% unlist() %&gt;% str_c(&quot;https://www.landtag.brandenburg.de&quot;, .) ) ## # A tibble: 175 x 3 ## date name link ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2020-12-30 Zum Tode von Paul-Heinz Dittrich https://www.landtag.brandenburg… ## 2 2020-12-17 Symbol für Zusammenhalt auch in … https://www.landtag.brandenburg… ## 3 2020-12-11 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## 4 2020-12-10 Hinweise für Medien zu den Plena… https://www.landtag.brandenburg… ## 5 2020-12-04 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## 6 2020-11-27 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## 7 2020-11-25 UN-Women-Flagge weht im Innenhof… https://www.landtag.brandenburg… ## 8 2020-11-23 Hinweise für Medien zur Sondersi… https://www.landtag.brandenburg… ## 9 2020-11-23 Attikafiguren von Perseus und An… https://www.landtag.brandenburg… ## 10 2020-11-20 Termine des Landtages Brandenbur… https://www.landtag.brandenburg… ## # … with 165 more rows "],["R3.html", "8 Transformation und Visualisierung im Tidyverse 8.1 Transformation mit dplyr 8.2 Visualisierung mit ggplot", " 8 Transformation und Visualisierung im Tidyverse 8.1 Transformation mit dplyr 8.2 Visualisierung mit ggplot "],["ethik.html", "9 Ethik &amp; good practice", " 9 Ethik &amp; good practice "],["projekt1.html", "10 Beispielprojekt 1", " 10 Beispielprojekt 1 "],["projekt2.html", "11 Beispielprojekt 2", " 11 Beispielprojekt 2 "]]
