# Graphical analysis with ggplot {#ggplot}


``` {r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(cache.path = 'cache/')
```


After we have cleaned our example data in the last chapter, we will now apply
some graphical analysis to it, using the ggplot2 package.

To follow most of the examples, we need the object `reports_fyears` from chapter
\@ref(dplyr). Either you can re-run the relevant code blocks from the last
chapter, or the condensed block below. We also will need to load the tidyverse
package -- which includes ggplot2 -- and lubridate.

``` {r message = FALSE}
library(tidyverse)
library(lubridate)
```

``` {r}
load("reports.RData")

reports_fyears <- reports %>% 
  mutate(District = substr(District, 14, length(District))) %>% 
  mutate(date_cmpl = dmy_hm(Date)) %>% 
  mutate(year = year(date_cmpl)) %>% 
  mutate(month = month(date_cmpl, label = TRUE)) %>% 
  mutate(day = wday(date_cmpl, label = TRUE)) %>% 
  mutate(time = 
           substr(Date, 12, length(Date)) %>% 
           hm()
         ) %>% 
  select(-c(Date, Report)) %>% 
  rename(district = District) %>% 
  filter(year %in% 2015:2020) %>% 
  mutate(district = str_remove_all(district, pattern = " "))
```


## ggplot2 syntax

The basic principle of ggplot2 is, that we initiate an empty plot using
`ggplot()` -- note that while the package is calles ggplot2, the funtion is
written without the "2" -- and than add one or multiple *geoms*, the graphical
elements that shall be plotted -- e.g. points, lines, bars and so on.

`ggplot()` has to be provided the name of the object that contains the data we
want to plot as its first argument. 

``` {r}
ggplot(data = reports_fyears)
```

As you can see in the "Plots" tab located in the lower right of RStudio, a plot
was created but it is still empty, as we have not added any geoms yet. We can do
this by writing a `+` after `ggplot()` and adding the geom -- we will look at
some examples of those soon -- in a new line. Additional geoms can be added in
the same manner. So the basic syntax -- note that this is not runnable code --
looks like this:

``` {r eval = FALSE}
ggplot(data = ...) +
  geom_1() +
  geom_2() +
  ...
```


## Geoms and aesthetics

Geoms represent the graphical objects we actually want to plot. All geom
functions start with `geom_` and end in a word describing the type of geom, e.g.
`geom_point()` for scatter plots, `geom_line()` for lines or `geom_col()` for
bar plots. To get an overview of the available geoms, I highly recommend the R
cheat sheet for ggplot2, accessible here:
<https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-visualization-2.1.pdf>{target="_black"}

*Aesthetics* are used to define the variables to be used in plotting. There are
several aesthetics that can be used to affect the visual display of the plotted
elements by the value of a variable. We will examine examples for this later and
first focus on assigning variables to both axis, as these are the only
aesthetics that have to be provided, everything else being optional. 

To define the aesthetics we use `aes()` as an argument of the geom function and
define the x variable -- refering to the horizontal x-axis of the plot -- and
the y variable -- referring to the vertical y-axis -- as arguments of `aes()` in
the way shown below. Again, this code is purely illustrational and wont run.

``` {r eval = FALSE}
ggplot(data = ...) +
  geom_1(aes(x = x_variable, y = y_variable)) +
  geom_2(aes(x = x_variable, y = y_variable)) +
  ...
```

If we are using the same x and y variables for all geoms, we can define them
directly in the call of `ggplot()`.

``` {r eval = FALSE}
ggplot(data = ..., aes(x = x_variable, y = y_variable)) +
  geom_1() +
  geom_2() +
  ...
```


### Continuous x, continuous y

Variables are continuous, when they can take on every value -- maybe limited by
a maximum and minimum. They are also always numerical. Common examples in the
social sciences are income or monetary values in general, if they are measured
exactly and not in a number of broad categories. The latter would be an example
of a categorical variable, as the variable can only take on a number of defined
categories. Income groupd would be an example for this, as would be gender or
the party voted in the last election.

In our example data, we mostly find categorical variables like `district` or
`day`, while `date_cmpl` and `time` are continuous variables. As all of the
continuous variables are referring to the same concept -- the moment in time
when the report occurred -- it makes no sense plotting them against each other.
For this reason and because plots for continuous by continuous variables are
important types of plots, we will construct a simple example dataset using
random values.

Let us create fictional data for the size and the price of apartments in a city.
Let us assume, that mean for size in square meters is $70$ and for the price in
Euro per square meter is $10$. The actual values should be evenly distributed
around those means. Also there should be about $20%$ of premium apartments that
for some reason -- maybe they lie in a exceptionally expansive district -- cost
$10$ Euros more per square meter. The following code block -- which can not be
explained in detail here -- will generate the data just described. `set.seed()`
assures that you will get the same *random number generator* results every time.

``` {r}
set.seed(08072021)

rent <- tibble(
  size = rnorm(1000, mean = 70, sd = 15),
  premium = as.logical(rbinom(n = 1000, size = 1, prob = 0.2)),
  price = size * (10 + rnorm(1000, mean = 0, sd = 3) + (10 * premium)) 
)

rent %>% 
  head(n = 5)
```

We can now plot this data. The price of an apartment depends on its size, thus
price should be plotted on the y-axis which is usually used for the dependent
variable and the independent variable size on the x-axis. Our assumption -- and
we know it is true as we created the data this way -- is that the size of an
apartment explains its price. We will use the `geom_point()` to build a
scatterplot, which each point representing one of the 1000 combinations of size
and price in the data.

``` {r}
ggplot(data = rent, aes(x = size, y = price)) +
  geom_point()
```

Looking at the data points, we clearly see a relationship. Overall, the higher
the size of an apartment, the higher its price point. We know this is true in
our data and the relationship is very clearly visible, but we can confirm this
assumption by also plotting a regression line over the points. This will
visualise the linear relationship between both variables in the way that best
fits the data using a straight line. We can use `geom_smooth()` for this
purpose, specifying that we want to use `method = 'lm'`, which is a linear
model. The second option `se = FALSE` means that we do not want to plot the
confidence intervals around the regression line, which is a measure of the
uncertainty of the estimated relationship.

``` {r}
ggplot(data = rent, aes(x = size, y = price)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

The angle of the regression line shows us the relationship, with greater size,
prices also rise. Also we can use it to estimate the mean value of y for every
given x. An apartment of 50 square meters in average costs about 600 Euros, for
example.

Remember that we added a third variable to the dataset, indicating if it is a
premium apartment or not, by use of a logical variable. We can use this as an
aesthetic to group the data in the plot by its values and in this way generate
different regression lines per group using the aesthetic `group = premium` or we
can use `colour = premium` to simultaneously group the data and colour the
plotted objects by their group membership.

``` {r}
ggplot(data = rent, aes(x = size, y = price, colour = premium)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

we can clearly see, that the steepness of both lines differs. For premium
apartments (light blue) the relationship between square meters and price is
stronger than for non-premium apartments

Sidenote: Here the relationship between the three variables was known
beforehand, as was have designed the data in this way. In real data, we would
not know the relationship. Based on theoretical considerations we can assume a
relationship between two variables and assess its presence in our actual data
visually using a plot of this kind.


### Graphical analysis of Berlin police reports

We will now return to our scraped data on police reports in Berlin and follow
up the summary statistics with additional graphical analysis, broadening our
view on the patterns in the data.

#### Categorical x variables

In the last chapter we saw that there seems to be a rise of police reports
during the week, culminating on Fridays If we want to visualise the number of
police reports per day, one common approach could be to use a barplot.
`geom_bar()`just needs to be supplied with the variable holding the
categories -- here `day` -- and will then count the number of occurences of the
category which are plotted as bars with corresponding height.

``` {r}
ggplot(data = reports_fyears, aes(x = day)) +
  geom_bar()
```

We see that there is an increase over the course of the week, but that the
relative differences between the days are minor. Combined with the summary
statistics for the number of reports by year and month seen in chapter
\@ref(dplyr), we can conclude, that there seem to be no systematic differences
in the count of police reports by weekday, month or year where it occured.
As we have established, we do not know how the data is created. That the number
of reports is rather constant over time could point towards a quota of reports
that are released by the Berlin police per day, not depending on if much of 
interest happened on a specific day.

Let us plot the reports county by the district, using the same approach. Let us
plot the categories on the y-axis to make the long district names more readable.

``` {r}
ggplot(data = reports_fyears, aes(y = district)) +
  geom_bar()
```

This is nice, but it would be more readily interpretable, if the bars were
ordered by their height. Also we have the category "NA" which has no
interpretable meaning and should be removed before plotting. We can do all of
this using the pipe. `drop_na()` will remove all observations where the variable
specified as its argument has a value of `NA`. We then group by `district` and
calculate the group counts using `summarize()` with `n()`, as we have already
done in chapter \@ref(dplyr). The result is passed to `ggplot()` through the
pipe. Here we use `geom_col()` which basically is a bar plot where the
occurrences of each category are not counted automatically but are already
present as a number in the data set. In our case we have summarised the number
of reports in the column `reports` which becomes assigned to the x-axis in our
plot. The y-axis variable is arranged in descending order using `reorder()`; the
function orders the variable that is specified in the first argument by the
values of the variable defined in the second argument -- here ordering the
categories of `district` by the values of `reports`.


``` {r}
reports_fyears %>% 
  drop_na(district) %>% 
  group_by(district) %>% 
  summarize(reports = n()) %>% 
  ggplot(aes(x = reports, y = reorder(district, reports))) +
    geom_col()
```

The plot underlines what we have already seen in chapter \@ref(dplyr), that some
districts, especially "Mitte", are referred to by considerably more police
reports than others.

#### Continuous x variables

The time of day when a report occurred is a continuous variable; it can take all
values between 00:00 and 23:59. If we want to visualise how its values are
distributed across this range, we can plot a histogram. In a histogram the range
of a variable is divided into *bins*; being an interval in the data. If we have
30 bins, which is the standard in `geom_histogram()`, the data is divided into
30 intervals of the same length. For each interval the number of actual values
that fall into it are counted and displayed on the y-axis, which is therefore
represented by the height of the bar. To be able to plot the values of `time`,
we again have to convert the representation in hours, minutes and seconds to its
numerical values -- being seconds since midnight -- by using `as.numeric()` or
`period_to_seconds()` (also see chapter \ref@(dplyr)).


``` {r}
ggplot(data = reports_fyears, aes(x = period_to_seconds(time))) +
  geom_histogram()
```

We immediately see, that the number of reports is not evenly distributed
throughout the day. It is also not normally distributed, as the distribution is
right-skewed, with high counts for low to medium values of `time` and lower
counts for higher values that are spread out far to the right. We also see, that
there is a period on the left- and right-hand side of `time` where the counts
are $0$, meaning no reports are filed in the very early and very late hours.

But the plot remains hard to interpret as the values that are shown on the
x-axis do not immediately make sense to us. What is the time in hours and
minutes when $25000$ seconds have passed since midnight? As most humans -- 
including myself -- will not have an immediate answer to this question, our
graphic is communicating the wrong measure.

We can use `scale_x_continuous()` to set options for the display of the x-axis,
in this case to choose more appropriate labels. The argument `breaks =`
determines at which values the axis should be labelled and with `labels =` we
can further specify what the labels shall display. In the next code block, the
values supplied to `breaks =` are calculations of the seconds since midnight for
the hours of the day I chose, and for `labels =` character strings using the
familiar display of hours and minutes.

While we are at it, we can tune the graphical output some more. With `labs()` we
can set labels for several parts of the graphic; here we display nice and clear
labels on both axis -- instead of using the name of the variables -- and a title
for the graphic. In chapter \@ref(dplyr) we calculated the mean time of day over
all police reports. We can include this in the plot by adding a vertical line
using `geom_vline()`; with the aesthetic `xintercept =` specifying the x-value
at which we want to draw the line, here the mean we calculated and assigned to
an object. We also use the `colour =` argument outside of `aes()` to set a
constant colour for the line -- instead of using it as an aesthetic and thus
colouring the data by the values of a third variable.

Sidenote: For an overview of the available colours, have a look at:
<http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf>{target="_blank"}.
  
``` {r}
mean_time <- reports_fyears$time %>% 
        period_to_seconds() %>% 
        mean()

ggplot(data = reports_fyears, aes(x = period_to_seconds(time))) +
  geom_histogram() +
  scale_x_continuous(breaks = c(6 * 60 * 60, 9 * 60 * 60, 12 * 60 * 60,
                                15 * 60 * 60, 18 * 60 * 60),
                     labels = c("6:00", "9:00", "12:00", "15:00", "18:00")
                     ) +
  labs(x = "Time of Day",
       y = "Reports",
       title = "Distribution of police reports over time of day") +
  geom_vline(aes(xintercept = mean_time), colour = "red")
```

With readable labels and the mean line as reference points, the distribution of
the time of day becomes more apparent. We see, that there are no reports filed
beginning from the late evening to the early morning. This is another indication
that the times we scraped do not actually represent the times when the event
occurred but rather when the reports was officially filed or uploaded to the
website. From this data, we can not conclude that it is safest to enter the
streets of Berlin at nighttime, but rather the the working hours of the people
responsible for communicating the police reports work from about 08:00 to 
19:00-20:00.

We can also see that the mean we calculated in chapter \@ref(dplyr) -- 11:55 --
is misleading. As the many reports are filed in the late afternoon to early
evening, the mean is biased to the right. 11:55 is not the time when most
reports occur; actually half of all reports are filed before 11:00, as indicated
by the median, which in general is more robust to skewed distributions. There
seems to be a short period of high activity in the early to late morning hours.
Again, we can infer more about the working context of the police PR team than
about crimes in Berlin from this. It seems reasonable, that reports on the
events that occurred in the night pile up and are then quickly published in the
morning, when office hours begin. Following this, the number of reports gets
considerably lower as events are reported at the rate they occur.

We can also ask, if the distribution of the time of day when reports are
published has changed over time. To compare plots grouped by the values of
another variable -- here `year` --, we can use *facets*. As the first argument
of `facet_wrap()` we specify the variable by which we want subgroups to be
formed, using the notation `~ variable`. The argument `nrows =` determines the
number of rows to be displayed.

``` {r}
ggplot(data = reports_fyears, aes(x = period_to_seconds(time))) +
  geom_histogram() +
  facet_wrap(~ year, nrow = 2) +
  scale_x_continuous(breaks = c(6 * 60 * 60, 12 * 60 * 60, 18 * 60 * 60),
                     labels = c("6:00", "12:00", "18:00")
                     ) +
  labs(x = "Time of Day",
       y = "Reports",
       title = "Distribution of police reports over time of day by year")
```

Looking at the high numbers of very early reports and their maximum, we can see
a gradual and moderate shift from the early to the late morning, which is
especially apparent in 2019 and 2020. As there are still reports beginning from
about 08:00, we can not necessarily infer that the working hours have changed,
but maybe that the working process has. Maybe the writers started favouring the
quality of the written reports overt the quantity of early reports. Maybe new
rules have been implemented that require reports to be approved by some central
authority, which might slow down the rate. Also the Corona pandemic started in
early 2020, so the small shift to the right could be an effect of working
realities involving home-office, daycare and home-schooling.

While some of these interpretations seem reasonable, remember that we do not
actually have enough information to draw any robust inferences on either the
number and distribution of crimes in Berlin or the working hours of the PR team. 

