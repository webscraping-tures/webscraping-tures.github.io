# Scraping von Tabellen & dynamischen Websites {#rvest2}

``` {r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)

```

## Scraping von Tabellen

Im Web Scraping werden wir häufig das Ziel verfolgen, die extrahierten Daten in 
einen Tibble oder Data Frame zu überführen, um diese dann weiter analysieren zu 
können. Besonders dankbar ist es da, wenn die Daten an denen wir interessiert
sind bereits in einer HTML-Tabelle gespeichert sind. Denn rvest erlaubt es uns 
mit der Funktion `html_table()` komplette Tabellen schnell und einfach 
auszulesen.

Zur Erinnerung, Der HTML-Code für Tabellen ist in der Grundstruktur so 
gestaltet:

```
<table>
  <tr> <th>#</th> <th>Tag</th> <th>Effekt</th> </tr>
  <tr> <td>1</td> <td>"b"</td> <td>bold</td> </tr>
  <tr> <td>2</td> <td>"i"</td> <td>italics</td> </tr>
</table>
```

Der `<table>` Tag umfasst die gesamte Tabelle.
Zeilen werden durch `<tr>` definiert, Spaltenüberschriften mit `<th>` und
Zellen mit `<td>`.

Bevor wir mit dem Scraping beginnen, laden wir wie immer die nötigen packages:

``` {r}
library(tidyverse)
library(rvest)
```

### Tabelle mit CSS Selectors aus Wikipedia

Auf der [Wikipedia Seite zu "CSS"](https://en.wikipedia.org/wiki/CSS){target="_blank"},
findet sich auch eine Tabelle mit CSS Selectors. Diese ist unser Scraping Ziel.

Lesen wir zunächst die Website ein:

``` {r}
website <- "https://en.wikipedia.org/wiki/CSS" %>% 
  read_html()
```

Betrachten wir den Quellcode und suchen -- STRG+F -- nach "<table", erkennen
wir, dass diese Seite eine Vielzahl von HTML-Tabellen enthält. Dazu gehören
nicht ausschließlich die auf den ersten Blick als "klassische" Tabellen zu 
erkennenden Elemente, sondern unter anderem auch die "Infoboxen" am rechten 
oberen Rand des Artikels oder die ausklappbaren Auflistungen weiterführender 
Links am unteren Ende. Wenn Sie dies genauer betrachten möchten, können die 
Web Developer Tools hier sehr hilfreich sein.

Statt einfach alle `<table>` nodes der Seite zu selektieren, könnte eine 
Strategie sein, mit Hilfe der WDT einen CSS Selector für diese spezifische 
Tabelle zu erzeugen: `"table.wikitable:nth-child(28)"`. Wir selektieren so, die
Tabelle der Klasse `"wikitable"` die das 28te Kind der übergeordneten 
Hierarchiebene ist -- hier `<div class="mw-parser-output">`.

Wenn wir nur ein einzelnes HTML-Element auswählen möchten, kann es hilfreich 
sein, statt `html_nodes()` die Funktion `html_node()` zu nutzen. 

``` {r}
node <- website %>% 
  html_node(css = "table.wikitable:nth-child(28)")
nodes <- website %>% 
  html_nodes(css = "table.wikitable:nth-child(28)")

nodes
node
```

Der Unterschied besteht vor allem in der Ausgabe der Funktion.  Dies ist
erkennbar an dem Eintrag innerhalb der geschweiften Klammern im Output.  Im
ersten Fall, bekommen wir eine Liste von HTML Elementen -- ein "xml_nodeset" --,
auch wenn diese Liste, wie hier, nur aus einem Eintrag besteht.  `html_node()`
gibt uns direkt das HTML Element -- "html_node" -- als Ergebnis der Funktion.
Warum ist dies relevant? In vielen Fällen kann es einfacher sein direkt mit dem
HTML-Element statt einer Liste von HTML Elementen zu arbeiten, beispielsweise
bei der Überführung von Tabellen in Data Frames und Tibbles, aber dazu später
mehr.

Um nun die so ausgewählte Tabelle auszulesen, müssen wir nur noch die Funktion
`html_table()` auf unser Element anwenden.

``` {r}
css_table_df <- node %>% 
  html_table()

css_table_df %>% 
  head(n = 4)
```

Das Ergebnis ist ein Data Frame, der die gescrapten Inhalte der HTML-Tabelle 
enthält und dabei die in den `<th>` Tags hinterlegten Spaltennamen für die
Spalten des Data Frames übernimmt.
Durch die sehr langen Zellen in der Spalte "Matches", ist der Output der R 
Studio Console leider nicht besonders hilfreich. Ein weiterer Vorteil der
Nutzung von Tibbles statt Data Frames, ist es, dass lange Zelleninhalte in der
Darstellung im Output automatisch abgekürzt werden.
Um einen Data Frame in einen Tibble umzuwandeln, können wir die Funktion
`as_tibble()` nutzen.

``` {r}
css_table_tbl <- node %>% 
  html_table() %>% 
  as_tibble()

css_table_tbl %>% 
  head(n = 4)
```

### Scrapen mehrerer Tabellen

Es könnte auch unser Scraping Ziel sein, nicht nur die erste, sondern alle vier 
inhaltlichen Tabellen des Wikipedia Artikels zu scrapen. Betrachten wir die vier
Tabellen im Quellcode und/oder den WDTs, zeigt sich, dass sie alle die Klasse
`"wikitable"` tragen. Damit können wir sie leicht selektieren. Hierbei bitte
beachten, dass wieder die Funktion `html_nodes()` genutzt werden muss, da wir 
nicht mehr nur ein Element, sondern wieder eine Liste mehrerer selektierter
Elemente benötigen.

``` {r}
tables <- website %>% 
  html_nodes(css = "table.wikitable") %>% 
  html_table()
```

Das Ergebnis ist eine Liste von vier Data Frames, welche jeweils eine der vier 
Tabellen enthalten. Möchten wir einen Einzelnen der Data Frames aus der Liste
auswählen um ihn beispielsweise in ein neues Objekt zu überführen, sind wir auf
*subsetting* angewiesen, welches wir bereits aus der Einführung in R kennen.

Zur Erinnerung: es bestehen in R grundsätzlich zwei Möglichkeiten des
subsettings von Listen.  `liste[#]` und `liste[[#]]`. Der für uns
relevanteste Unterschied besteht darin, was für eine Art Objekt R an
uns zurückgibt. Im ersten Fall wird uns immer eine Liste
zurückgegeben, auch wenn diese Möglicherweise nur aus einem Element
besteht. Die Nutzung doppelter eckiger Klammern, gibt uns hingegen ein
einzelnes Element direkt zurück. Der Unterschied ist also ähnlich zu
dem zwischen `html_nodes()` und `html_node()`.

Haben wir das Ziel beispielsweise den dritten Data Frame aus der Liste mit vier
Data Frames auszuwählen, welches subsetting müssten wir nutzen?

``` {r}
tables[3] %>% 
  str()

tables[[3]] %>% 
  str()
```

Im ersten Fall sehen wir, dass wir eine Liste der Länge 1 haben,
welche einen Data Frame mit 7 Zeilen und 2 Variablen enthält, sowie
weitere Informationen zu diesen Variablen.  Im zweiten Fall bekommen
wir den Data Frame direkt, also nicht mehr als Element einer
Liste. Wir müssen also `liste[[]]` nutzen um direkt einen einzelnen
Data Frame aus einer Liste von Data Frames auswählen zu können.

Sind wir stattdessen an einer Auswahl mehrerer Elemente aus einer Liste 
interessiert, ist dies nur mit `liste[]` möglich. Statt ein Element mit einer
einzelnen Zahl auszuwählen, können wir mehrere mit einem Vektor von Zahlen
in einem Schritt selektieren.

``` {r}
tables[c(1, 3)] %>% 
  str()
```

Wir bekommen als Ergebnis erneut eine Liste, welche die beiden hier ausgewählten
Elemente enthält.


### Tabellen mit NAs

Was passiert, wenn wir versuchen eine Tabelle mit fehlenden Werten auszulesen?
Betrachten Sie dazu folgendes Beispiel:
<https://webscraping-tures.github.io/table_na.html>{target="_blank"}

Auf den ersten Blick ist bereits ersichtlich, dass hier mehrere Zellen der
Tabelle unbesetzt sind. Es fehlen also Werte. Versuchen wir die Tabelle 
trotzdem einmal einzulesen.

``` {r error = TRUE}
table_na <- "https://webscraping-tures.github.io/table_na.html" %>% 
  read_html %>% 
  html_node(css = "table")

table_na %>% 
  html_table()
```

Wir bekommen eine hilfreiche Fehlermeldung die darüber informiert, dass die
Anzahl der Spalten nicht über die gesamte Tabelle hinweg konstant ist. 
Netterweise wird uns auch direkt eine mögliche Lösung angeboten.
Die Funktion `html_table()` kann mit dem Argument `fill = TRUE` dazu angewiesen 
werden, Zeilen mit abweichender Spaltenanzahl automatisch mit `NA` aufzufüllen. 
Dies steht für "Not Available" und repräsentiert fehlende Werte in R.

``` {r}
wahlbet <- table_na %>% 
  html_table(fill = TRUE)

wahlbet %>% 
  head(n = 4)
```

Wie wir sehen, konnte `html_table` die vier Zellen mit fehlenden Werten mit `NA`
füllen und die Tabelle trotz der Probleme einlesen. Allerdings, bestehen in dem
HTML-Quellcode zwei unterschiedliche Arten von Problemen, welche die 
automatische Reparatur unterschiedlich gut handhaben kann. Betrachten wir dazu
zunächst den Quellcode der ersten beiden Zeilen:

```
<tr>
  <td>Baden-Württemberg</td>
  <td>2016</td>
  <td></td>
</tr>

<tr>
  <td>Bayern</td>
  <td>2018</td>
</tr>
```

Für Baden-Württemberg sehen wir, dass die dritte Spalte zwar im Quellcode
angelegt ist, sich aber kein Inhalt in dieser Zelle befindet. Dies hätte
`html_table()` auch ohne `fill = TRUE` einlesen können und die Zelle mit einem
`NA` gefüllt.  Im Gegensatz dazu fehlt für Bayern die Zelle komplett. Das
bedeutet, die zweite Zeile der Tabelle besteht nur aus zwei Spalten, während der
Rest der Tabelle drei Spalten hat. Dies ist das Problem, auf welches uns die
Fehlermeldung hingewiesen hat. Im Ergebnis konnte R in beiden Fällen den
richtigen Schluss ziehen und die Zelle in beiden Zeilen mit einem `NA` füllen.

Betrachten wir aber auch die dritte und vierte Zeile im Quellcode:

```
<tr>
  <td>Berlin</td>
  <td></td>
  <td>66.9</td>
</tr>

<tr>
  <td>Brandenburg</td>
  <td>61.3</td>
</tr>
```

Hier fehlt in beiden Fällen die zweite Spalte. Im ersten Fall ist sie angelegt
aber leer, im zweiten existiert sie nicht.  Mit dem ersten Fall kann
`html_table()` erneut problemlos umgehen. Für Brandenburg stößt die Funktion
aber an ihre Grenzen.  Wir als menschliche Betrachter erkennen schnell, dass die
letzte Landtagswahl in Brandenburg nicht im Jahr 61.3 stattfand und dies also
die Wahlbeteiligung sein muss. R kann dies nicht so einfach unterscheiden und
nimmt 61.3 als Wert für die Spalte "Wahljahr" und setzt für "Wahlbeteiligung"
ein `NA` ein.

Was tun? Zunächst einmal, sollten wir uns darüber bewusst sein, dass solche
Probleme existieren. Wenn `html_table()`diese Fehlermeldung ausgibt, sollten wir
also nicht einfach `fill = TRUE` setzen, sondern versuchen herauszufinden, warum
das Problem besteht und ob die Option, es automatisch beheben zu lassen, uns
auch tatsächlich zum Ziel führt. Ist dies nicht der Fall, könnte ein
Lösungsansatz sein, eine eigene "Extraktorfunktion" zu schreiben, welche die
Probleme direkt während des Scrapings behebt. Dies ist aber eine eher
fortgeschrittene Methode und außerhalb des in dieser Einführung machbaren. Wir
können aber immerhin die entstandenen Probleme im Nachhinein korrigieren.

Unser Problem liegt ausschließlich in Zeile Vier. Deren zweite Spalte muss in die
dritte verschoben werden und die zweite dann selbst als `NA` gesetzt werden.
Dazu benötigen wir erneut subsetting. Im Falle eines Data Frames müssen wir 
dabei die Zeile und die Spalte in der Form `df[zeile, spalte]` angeben, um eine
Zelle auszuwählen. So können wir R sagen: "Schreibe in Zelle Drei den Inhalt von
Zelle Zwei, und dann schreibe in Zelle Zwei `NA`"

```{r}
wahlbet[4, 3] <- wahlbet[4, 2]
wahlbet[4, 2] <- NA

wahlbet %>% 
  head(n = 4)
```

## Dynamische Websites

In der "Realität" des modernen Internets werden wir immer häufiger auf
Websites stoßen, die nicht mehr ausschließlich auf statischen HTML-Dateien
basieren, sondern Inhalte dynamisch generieren. Dies kennen Sie 
beispielsweise in Form von Timelines in Social Media Angeboten die dynamisch
auf Basis Ihres Nutzerprofils generiert werden. Andere Websites generieren
die dargestellten Inhalte möglicherweise mit *JavaScript* Funktionen oder als
Reaktion auf Eingaben in HTML-Formularen.

In vielen dieser Fälle reicht es aus Web Scraping Perspektive nicht mehr aus,
eine HTML-Seite einzulesen und die gesuchten Daten zu extrahieren, da diese
im HTML-Quellcode oft nicht enthalten sind, sondern dynamisch im Hintergrund
geladen werden. Die gute Nachricht ist, dass es auch hier meist Möglichkeiten 
gibt, die Informationen trotzdem scrapen zu können. 

Möglicherweise bietet der Betreiber einer Seite oder eines Services eine *API* 
(Application Programming Interface) an. In diesem Fall können wir uns für einen
Zugang zu dieser Schnittstelle registrieren und erhalten dann Zugang zu den 
Daten von Interesse. Dies ist beispielsweise bei Twitter möglich. In anderen
Fällen können wir eventuell in den eingebetteten Scripts identifizieren, wie und
aus welcher Datenbank die Informationen geladen werden und diese direkt 
ansteuern. Oder wir nutzen den *Selenium WebDriver* um ein Browserfenster 
"fernzusteuern" und das zu Scrapen, was der Browser "sieht". 

Bei all diesen Ansätzen handelt es sich allerdings um fortgeschrittene Methoden,
die den Umfang dieser Einführung übersteigen.

In den Fällen, in denen auf Basis der Eingaben in ein Formular eine HTML-Datei
dynamisch erzeugt wird, können wir diese aber mit den bereits bekannten
Methoden auslesen.

### HTML-Formulare und HTML-Queries

Lassen Sie uns als Beispiel den OPAC Katalog der Universitätsbibliothek
Potsdam <https://opac.ub.uni-potsdam.de/>{target="_blank"}
zunächst im Browser betrachten.

Geben wir in das Suchfeld den Begriff "test" ein und klicken auf Suche, zeigt
uns das Browserfenster die Ergebnisse der Suchanfrage. Was uns hier aber
eigentlich interessiert ist die Adressleiste des Browsers. Statt der URL
"https://opac.ub.uni-potsdam.de/", steht dort nun eine deutlich längere URL in
der Form:
"https://opac.ub.uni-potsdam.de/DB=1/CMD?ACT=SRCHA&IKT=1016&SRT=YOP&TRM=test".
Der erste Teil ist offensichtlich weiterhin die URL der aufgerufenen Website,
nennen wir dies die Base-URL. An das Ende der URL wurde aber der Teil
"CMD?ACT=SRCHA&IKT=1016&SRT=YOP&TRM=test" angehangen. Dies ist der *HTML-Query*,
an dem wir hier interessiert sind. Zwischen Base-URL und Query befinden sich
noch ein oder mehrere Bestandteile, die sich in diesem Fall auch je nach ihrem
Browser unterscheiden können. Diese sind für die eigentliche Suchanfrage aber
auch irrelevant. Die verkürzte URL:
"https://opac.ub.uni-potsdam.de/CMD?ACT=SRCHA&IKT=1016&SRT=YOP&TRM=test" führt
zu dem selben Ergebnis.

Ein Query ist eine Anfrage, in der Daten aus einem HTML-Formular an den Server 
gesendet werden. Dieser generiert als Reaktion eine neue Website, welche an den 
Nutzer zurückgesendet und im Browser dargestellt wird.
Ausgelöst wurde die Anfrage hier durch den Klick auf den "Suchen" Button.
Wenn wir verstehen, was die Bestandteile des Querys bewirken, könnten wir
diesen manipulieren und gezielt Nutzen um uns eine Website von Interesse
erstellen zu lassen und diese auszulesen. 


### HTML-Formulare

Dazu müssen wir zunächst einen Blick in den HTML-Code des Suchformulars
werfen. Um dies nachvollziehen zu können, sollten Sie sich den Quellcode der
Seite anzeigen lassen und nach "<form" suchen oder die WDTs nutzen, um das
Formular und seine Bestandteile zu betrachten.

```
<form action="CMD"
      class="form"
      name="SearchForm"
      method="GET">
  ...
</form>
```

HTML-Formulare werden durch den `<form>` Tag umfasst. Innerhalb des Tags können
ein oder mehrere Formular-Elemente wie Texteingabefelder, Drop-Down 
Optionslisten, Buttons usw. platziert werden.

`<form>` selbst trägt in diesem Beispiel eine Reihe von Attributen. Interessant 
für uns ist als erstes das Attribut `method="GET"`. Dies legt die Methode der
Datenübermittlung zwischen Client und Server fest. Wichtig ist dabei, dass die 
Methode "GET" für die Übermittlung von Daten Queries in der URL nutzt und die 
Methode "POST" nicht. Wir können Queries somit nur manipulieren, wenn die "GET" 
Methode genutzt wird. Ist keine Methode im `<form>` Tag festgelegt, wird als 
Standard ebenfalls "GET" genutzt. 

Das zweite für uns interessante Attribut, ist `action="CMD"`. 
Dies legt fest, welche Aktion ausgelöst werden soll 
nachdem das Formular übermittelt wurde. Oft ist der Wert von `action=` der
Name einer Datei auf dem Server, an welche die Daten gesendet werden sollen und
welche dann eine dynamisch erzeugte HTML-Seite zurück an den Nutzer sendet.

Betrachten wir nun die Elemente des Formulars. Hierzu kann die rvest Funktion
`html_form()` hilfreich sein.

``` {r}
"https://opac.ub.uni-potsdam.de/" %>% 
  read_html() %>% 
  html_node(css = "form") %>% 
  html_form()
```

Der Output zeigt uns in der ersten Zeile nochmals die Werte für `method=` und
`action=` sowie den Namen des Formulars.  Die weiteren sechs Zeilen zeigen, dass
das Formular aus drei `<select>` sowie drei `<input>` Elementen besteht. Wir
sehen außerdem die Namen dieser Elemente sowie den Standardwert, der beim
Übermitteln des Formulars gesendet wird, solange kein anderer Wert ausgewählt
oder eingegeben wurde.
  
Schauen wir uns einige dieser Elemente an.
`<select>` Elemente sind Drop-Down Listen mit Optionen, die ausgewählt werden 
können. Dies ist der Quellcode für das erste `<select>` Element unseres
Beispiels:

```
<select name="ACT">
  <OPTION VALUE="SRCH">suchen [oder]
  <OPTION VALUE="SRCHA" SELECTED>suchen [und]
  <OPTION value="BRWS">Index bl&auml;ttern
</select>
```

Das Attribut `name="ACT"` legt den Namen des Elements fest. Dieser wird bei der
Übermittlung der Formulardaten über den Query genutzt. Über die '<option>` Tags, 
werden die auswählbaren Optionen, also die Drop-Down Liste, definiert. 
`<value="">` steht dabei für den von Formular übermittelten Wert. Den Nutzer:innen
wird der auf den Tag folgende Text angezeigt. Als Standardwert wird
entweder der erste Wert der Liste genutzt oder wie hier eine Option mit dem
Attribut `selected` explizit als Standard festgelegt.

Die drei weiteren Elemente sind `<input>` Tags. Eingabefelder, deren
spezifischer Typ über das Attribut`type=""` festgelegt wird. 
Dies können beispielsweise Textboxen (`type="text"`) oder Checkboxes 
(`input="checkbox"`) sein, es stehen 
aber weit mehr Optionen zur Verfügung. Eine umfassende Liste finden sie unter:
<https://www.w3schools.com/html/html_form_input_types.asp>{target="_blank"}. 
Hier der Quellcode zu Zwei der Drei '<input>` Elemente auf der Beispielseite:

```
<input type="text" name="TRM" value="" size="50">

<input type="submit" class="button" value=" Suchen ">
```
  
Der erste Tag ist vom Typ "text", also ein Textfeld, genau genommen das 
Textfeld in welches der Suchbegriff eingegeben wird. Neben dem Namen des 
Elements wird über `value=""` ein Standardwert des Feldes festgelegt. In diesem 
Fall ist der Standardwert ein leeres Feld. Der zweite Tag ist vom Typ "submit". 
Dies ist der Button "Suchen", welcher durch einen Klick die Übermittlung der
Formulardaten über den Query auslöst.


### Der Query

Doch was wird jetzt genau übermittelt? Betrachten wir nochmals den 
Beispiels-Query von weiter oben: 

```
CMD?ACT=SRCHA&IKT=1016&SRT=YOP&TRM=test
```

Der Wert des `action=""` Attributes bildet den ersten Bestandteil des Queries
und wird nach der URL angehangen. Der Wert des Attributs teilt dem Server mit,
was dieser mit den weiteren übermittelten Daten anfangen soll. Darauf folgt ein
"?", welches die zu übermittelnden Daten als mehrere Paare von `name=""` und
`value=""`  Attributen der einzelnen Elemente einleitet. Die Paare sind mit `&`
verbunden. "ACT=SRCHA" steht also dafür, dass im Element mit dem Namen "ACT" die
Option "SRCHA" ausgewählt wurde. Wofür die Werte der beiden weiteren `<select>`
Elemente "IKT" und "SRT" stehen, können sie selber mit einem Blick in den 
Quellcode oder die WDTs nachvollziehen. Als Wert des `<input type="text">` mit 
dem Namen "TRM" wird der in das Feld eingegebene Text übermittelt. Hier "test".

Der Server empfängt so die Formulardaten, kann auf Basis des `action=""` 
Attributes, hier "CMD", nachvollziehen, wie die Daten verarbeitet werden sollen 
und konstruiert dementsprechend die Website, die er an uns zurücksendet und die 
in unserem Browser dargestellt wird.

### Manipulation des Queries und Scrapen des Ergebnis

Da wir jetzt wissen, was die Bestandteile des Queries bedeuten, können wir diese
auch gezielt manipulieren. Statt nun Queries per Hand zu schreiben, sollten wir
R Nutzen um diese für uns zusammenzusetzen. Die Technik, URLs direkt im R-Code
zu manipulieren, wird uns außerdem noch häufiger begegnen. Wir sollten sie also 
frühzeitig erlernen.

Die Funktion `str_c()` setzt die als Argumente aufgelisteten *Strings*, also 
Folgen von Buchstaben, zu einem einzelnen String zusammen. Dabei können auch in 
anderen R Objekten gespeicherte Strings eingebunden werden. Haben wir das Ziel 
sowohl die Suchmethode als auch den Suchbgegriff zu manipulieren, könnten wir
dies so erreichen:

``` {r}
base_url <- "https://opac.ub.uni-potsdam.de/"
method <- "SRCHA"
term <- "test"

url <- str_c(base_url, "CMD?ACT=", method, "&IKT=1016&SRT=YOP&TRM=", term)
url
```

Verändern wir nun die in den Objekten `method` und `term` gespeicherten Strings 
und generieren die komplette URL erneut, werden diese Bestandteile des Queries
entsprechend manipuliert.

``` {r}
method <- "SRCH"
term <- "web+scraping"

url <- str_c(base_url, "CMD?ACT=", method, "&IKT=1016&SRT=YOP&TRM=", term)
url
```

Die Suchmethode wurde auf den Wert "SRCH" festgelegt, also eine "ODER" Suche, 
der Suchbegriff auf "web scraping". Dabei ist wichtig zu beachten, dass im Query
keine Leerzeichen auftauchen dürfen und diese beim Absenden des Formulars durch 
"+" ersetzt werden. Statt "web scraping" müssen wir also den String 
"web+scraping" nutzen.

Als Beispielanwendung können wir nun dem Server eine "UND" Suche zu dem Begriff
"web scraping" durchführen lassen, die vom Server generierte HTML-Seite auslesen
und die dargestellten 10 Titel extrahieren.

``` {r}
base_url <- "https://opac.ub.uni-potsdam.de/"
method <- "SRCHA"
term <- "web+scraping"

url <- str_c(base_url, "CMD?ACT=", method, "&IKT=1016&SRT=YOP&TRM=", term)
url

website <- url %>% 
  read_html()
```

Die Suchergebnisse werden in der generierten HTML-Datei als Tabellen 
dargestellt. Der `<table>` Tag hat dabei die Attribut-Wert Kombination
`summary="hitlist"`, was wir für unseren CSS Selector nutzen können:

``` {r}
hits <- website %>% 
  html_node(css = "table[summary='hitlist']") %>% 
  html_table() %>% 
  as_tibble()

hits %>% head(n=10)
```

Dies hat zwar funktioniert, wir sehen aber, dass die Tabelle vor allem aus 
leeren Zeilen und Zellen besteht. Diese sind auf der Website unsichtbar, werden
aber zur Formatierung der Darstellung genutzt. Statt die Tabelle nun 
nachträglich zu reparieren, macht es mehr Sinn, gezielt nur die Zellen zu 
extrahieren, in welchen die von uns gesuchte Information enthalten sind. Dies 
sind die `<td>` Tags mit `class="hit"` und der Attribut-Wert Kombination 
`align="left"`. Auf dieser Basis können wir einen eindeutigen CSS Selector 
konstruieren.

``` {r}
hits <- website %>% 
  html_nodes(css = "td.hit[align='left']") %>% 
  html_text(trim = TRUE)

hits %>% head(n = 5)
```

### Weiterführende Quellen

Um diese Informationen weiter zu verarbeiten und beispielsweise in Daten zu
Autor, Titel, Jahr usw. zu trennen, sind erweiterte Kenntnisse im Umgang mit
Strings notwendig, die leider über diese Einführung hinausgehen. Eine gute erste
Übersicht finden sie im Kapitel "Strings" aus "R for Data Science" von Wickham
und Grolemund: <https://r4ds.had.co.nz/strings.html>{target="_blank"}

Dazu sei auch der passende "Cheat Sheet" empfohlen:
<https://raw.githubusercontent.com/rstudio/cheatsheets/master/strings.pdf>{target="_blank"}