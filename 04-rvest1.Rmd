# (PART) Scraping {-}

``` {r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

# Erstes Scraping mit rvest {#rvest1}
Mit dem Wissen, wie eine HTML-Datei aufgebaut ist und wie R und R-Studio in den
Grundzügen funktionieren, sind wir mit den nötigen Werkzeugen ausgestattet, um
unsere ersten Schritte im Web Scraping zu unternehmen. In dieser Sitzung werden
wir lernen, wie wir mit Hilfe des R Packages *rvest* HTML-Quellcode in R-Studio
einlesen, gezielt bestimmte Inhalte, an denen wir interessiert sind, extrahieren
und die gesammelten Daten in ein R-Objekt überführen, um sie in der Zukunft weiter
analysieren zu können.

## Das rvest package
Teil des *tidyverse* ist auch ein Package namens *rvest*, welches uns alle
grundlegenden Funktionen für eine Vielzahl typischer Web Scraping Aufgaben
bereitstellt. Dieses wurde bei der Installation des *tidyverse* Packages zwar
bereits mitgeliefert, ist aber nicht Teil des *core tidyverse* und wird damit
auch nicht mit `library(tidyverse)` in die aktuelle R Session geladen.
Deshalb müssen wir dies explizit tun:

``` {r}
library(rvest)
```

Der Output in der R-Studio Konsole informiert uns darüber, dass neben *rvest*
außerdem das Package *xml2* geladen wurde, auf dem rvest in Teilen basiert und
dessen Funktion zum Einlesen von HTML-Dateien wir im Folgenden benötigen werden.


## hello_world.html
Als erste Übung bietet es sich an, das bereits in
Kapitel \@ref(html) beschriebene [Hello World Beispiel](https://webscraping-tures.github.io/hello_world.html){target="_blank"} zu Scrapen.
Zur Erinnerung folgt nochmals der Quelltext der HTML-Datei:

```
<!DOCTYPE html>

<html>
  <head>
    <title>Hello World!</title>
  </head>
  <body>
    <b>Hello World!</b>
  </body>
</html>
```

### read_html()
Der erste Schritt im Web Scraping ist es, die Seite, an der wir interessiert
sind in ein R-Objekt zu überführen. Dies ermöglicht uns die Funktion
`read_html()` aus dem *xml2* Package. Diese benötigt als erstes Argument die
URL, also die Adresse der Website, die wir einlesen möchten. Die URL muss dabei
als String angegeben werden. Die Funktion erlaubt auch, noch weitere Optionen
festzulegen. In den meisten Fällen sind die Voreinstellungen aber
ausreichend. Wir lesen also die hello_world.html Datei ein, weisen sie
gleichzeitig einem neuen R-Objekt zu und lassen uns dieses Objekt im nächsten
Schritt ausgeben:

``` {r}
hello_world <- read_html("https://webscraping-tures.github.io/hello_world.html")
hello_world
```

Wie wir im Output sehen, handelt es sich bei dem R-Objekt `hello_world` um eine
Liste mit zwei Einträgen. Der erste Eintrag beinhaltet alles, was durch den
`<head>` Tag umschlossen wird, der zweite Eintrag alles, was durch den `<body>`
Tag umschlossen wird. Der öffnende und schließende `<html>` Tag ist nicht Teil
des Objektes. Wenn wir uns daran einnern, dass HTML-Code hierarchisch
strukturiert ist, ist die Liste also anhand der höchsten verbleibenden Ebenen --
`<title>` und `<body>` -- organisiert.

Damit haben wir erfolgreich eine Repräsentation der Website in einem R-Objekt
angelegt. Doch was machen wir nun damit? Im Falle dieses einfachen Beispiels
könnten wir eventuell daran interessiert sein, den Titel der Website oder den
auf der Seite dargestellten Text zu extrahieren.


### html_nodes()

Die Funktion `html_nodes()` aus dem *rvest* Package erlaubt es uns, gezielt
einzelne Elemente des HTML-Codes zu extrahieren. Dazu benötigt sie als erstes
Argument das Objekt aus dem extrahiert werden soll und zusätzlich einen
*selector*. In dieser Einführung werden wir uns ausschließlich auf die
sogenannten *CSS Selectors* konzentrieren. Die Alternative *XPath* ist zwar
nochmals etwas flexibler, *CSS Selectors* sind aber in den meisten Fällen
ausreichend und haben eine intuitivere und kürzere Syntax, was sie hier klar zum
Werkzeug der Wahl machen.

Wir werden die Möglichkeiten, die *CSS Selectors* bieten in Kapitel \@ref(css)
genauer behandeln und beschränken uns zunächst auf die Grundlagen. Ein Selector
in der Form `"tag"`, selektiert alle HTML-Tags des angegebenen Namens. Möchten
wir also den `<title>` Tag extrahieren, können wir dies so erreichen:

``` {r}
node_title <- html_nodes(hello_world, css="title")
node_title
```

Möchten wir den auf der Website dargestellten Text **Hello World!** extrahieren,
wäre es eine Möglichkeit den kompletten `<body>` Tag zu selektieren, da ja in
diesem Fall kein anderer Text auf der Seite dargestellt wird.

``` {r}
node_body <- html_nodes(hello_world, css="body")
node_body
```

Dies hat zwar grundsätzlich funktioniert, wir haben aber auch die `<b>` Tags
sowie mehrere Zeilenumbrüche (`\n`) mit extrahiert und benötigen eigentlich
beides nicht. Effizienter wäre es direkt den `<b>` Tag, der den Text umschließt
zu selektieren.

``` {r}
node_b <- html_nodes(hello_world, css="b")
node_b
```


### html_text()

In diesem Fall interessieren wir uns für den Text im Titel und auf der Website,
also den Inhalt der Tags. Diesen können wir in einem weiteren Schritt aus den
selektierten HTML-Elementen extrahieren. Dies ermöglicht die *rvest* Funktion
`html_text()`. Diese benötigt als einziges Argument das zuvor extrahierte
HTML-Element.

``` {r}
html_text(node_title)
html_text(node_b)
```

Damit haben wir unser erstes Web Scraping Ziel, die Extraktion des Titels und
des auf der Seite dargestellten Texts erfolgreich abgeschlossen.

Noch etwas zur Anwendung von `html_text()` auf Elemente, die selbst weitere Tags
enthalten: Weiter oben haben wir das Objekt `node_body` extrahiert, welches
neben dem dargestellten Text auch die `<b>` Tags sowie meherere Zeilenumbrüche
enthält. Auch hier können wir den reinen Text extrahieren.

``` {r}
html_text(node_body)
```

Wir sehen, dass die Funktion praktischerweise die `<b>` Tags, an denen wir nicht
interessiert waren, für uns entfernt hat. Es bleiben aber die Zeilenumbrüche
sowie mehrere Leerzeichen, sogenannter *whitespace*. Beides lässt sich mit dem
Argument `trim=TRUE` entfernen.

``` {r}
html_text(node_body, trim=TRUE)
```

## Countries of the World

Betrachten wir nun eine etwas realitätsnahere Anwendung. Die Website
<https://scrapethissite.com/pages/simple/>{target="_blank"} listet die Namen von
250 Ländern, sowie deren Flagge, Hauptstadt, Einwohnerzahl und Größe in
Quadratkilometern. Unser Ziel könnte es sein, diese Informationen für jedes Land
in R einzulesen, um sie dann potentiell weiter analysieren zu können.

Bevor wir damit beginnen, sollten wir die benötigten Packages laden (das
*tidyverse* Package benötigen wir diese Mal im weiteren Verlauf auch) sowie die
Website mit der Funktion `read_html()` einlesen und einem R-Objekt zuweisen.

``` {r}
library(tidyverse)
library(rvest)

website <- read_html("https://scrapethissite.com/pages/simple/")
```

Um die Struktur der HTML-Datei nachzuvollziehen, hilft auch hier als erster
Schritt der Blick in den Quellcode. Diesen können wir wie immer mit einem
Rechtsklick im Browserfenster und einem weiteren Klick auf "Seitenquelltext
anzeigen" öffnen. Die ersten gut 100 Zeilen des HTML-Codes enthalten vor allem
Informationen zur Gestaltung der Website, die uns an dieser Stelle nicht weiter
ablenken sollten. Wir sind rein an den Daten der Länder interessiert. Das erste
Land, das auf der Website gelistet wird, ist Andorra. Es bietet sich also an,
den Quellcode gezielt nach "Andorra" zu durchsuchen. Mit der Tastenkombination
STRG+F öffnet sich die Suchmaske in Ihrem Browser. In Zeile 128 werden wir
fündig. Da dieser für Übungszwecke konzipierte Quellcode sehr strukturiert
formatiert ist, erkennen wir schnell, dass Zeilen 125--135 als Codeblock auf
Andorra bezogen sind. Betrachten wir diese genauer:

```
<div class="col-md-4 country">
  <h3 class="country-name">
    <i class="flag-icon flag-icon-ad"></i>
    Andorra
  </h3>
  <div class="country-info">
    <strong>Capital:</strong> <span class="country-capital">Andorra la Vella</span><br>
    <strong>Population:</strong> <span class="country-population">84000</span><br>
    <strong>Area (km<sup>2</sup>):</strong> <span class="country-area">468.0</span><br>
  </div>
</div><!--.col-->
```

Die gesamten Informationen zu Andorra sind von einem `<div>` Tag
umschlossen. Als Erinnerung: ein `<div>` definiert eine Gruppierung von Code
über mehrere Zeilen hinweg. In der Webdesign-Praxis, werden diese Gruppierungen
vor allem dazu genutzt, um über das Argument `class=` dem folgenden Code einen
bestimmten CSS Style zuzuweisen, beispielsweise um das Schriftbild
festzulegen. Wie die Stile definiert sind, kann uns aus der Web Scraping
Perspektive in der Regel egal sein. Wir müssen nur wissen, dass wir diese
Zuweisungen von CSS Klassen für unsere Zwecke ausnutzen können. Auf der
nächstniedrigeren Ebene, finden wir zwei Blöcke, einen der unter Anderem den
Namen des Landes umfasst und einen weiteren, der Informationen zu diesem Land
enthält. Betrachten wir zunächst den ersten Block.


### Ländernamen

```
<h3 class="country-name">
  <i class="flag-icon flag-icon-ad"></i>
  Andorra
</h3>
```

Der Name "Andorra" wird von einem `<h3>` Tag eingefasst, also einer Überschrift
der dritten Ebene. Neben dem Namen, finden wir innerhalb des Tags auch einen
weiteren Tag über den das Bild der Flagge eingebunden wird. Da wir hier an den
Grafiken nicht interessiert sind, können wir dies ignorieren.

Auf dieser Website werden alle `<h3>` Tags ausschließlich dazu genutzt, die
Namen der Länder darzustellen. Damit können wir den `<h3>` Tag als *CSS Selector*
nutzen, um analog zum ersten Beispiel den umschlossenen Text auszulesen.

``` {r}
node_country <- html_nodes(website, css="h3")
text_country <- html_text(node_country, trim=TRUE)

head(text_country, n=10)
```

Das Ergebnis sieht vielversprechend aus. Da die Struktur des Codeblocks für
jedes Land gleich ist, wurde auf diesem Wege der Vektor `text_country` mit 250
Einträgen erzeugt, genau der Anzahl Länder, die auf der Website aufgelistet
sind. Aus Gründen der Übersichtlichkeit, ist es häufig sinnvoll, nicht die
kompletten und oft sehr langen Vektoren oder Dataframes auszugeben, sondern sich
mit der Funktion `head()` die über das Argument `n` festgelegte Anzahl von
Einträgen, beginnend mit dem ersten, auflisten zu lassen.

#### Die Pipe %>% 

Spätestens an dieser Stelle sollten wir uns nochmals Gedanken zur Lesbarkeit und
Struktur unseres R-Codes machen. Betrachten wir den vorangegangenen Codeblock:

``` {r, eval = FALSE}
node_country <- html_nodes(website, css="h3")
text_country <- html_text(node_country, trim=TRUE)
```

Wie wir gesehen haben, erreichen wir damit unser Ziel. Wir haben aber dazu auch
das Objekt `node_country` erstellt um den ersten Arbeitsschritt -- das Auslesen
der `<h3>` Tags -- zwischenzuspeichern. Dieses Objekt werden wir nie wieder
benötigen. Nutzen wir stattdessen die bereits vorgestellte Pipe `%>%`, entfällt
die Notwendigkeit des Zwischenspeicherns von Teilergebnissen und wir schreiben
gleichzeitig intuitiveren und leichter verständlichen Code.

``` {r}
country <- website %>% 
  html_nodes(css="h3") %>% 
  html_text(trim=TRUE)

head(country, n=10)
```

Zur Erinnerung: Die Pipe gibt das Ergebnis eines Arbeitschritts an die nächste
Funktion weiter, die im *tidyverse* sowie in vielen anderen R-Funktionen (aber
nicht Allen!) Daten als erstes Argument nimmt, welches wir dann nicht mehr
explizit definieren müssen. Zum besseren Verständnis betrachten wir obiges
Beispiel im Detail. Die erste Zeile übergibt das Objekt `website` an die
Funktion `html_nodes()`. Wir müssen `html_nodes()` also nicht mehr mitteilen,
auf welches Objekt sie angewandt werden soll, da wir dieses mit der Pipe bereits
an die Funktion weitergegeben haben. Die Funktion wird also mit allen weiteren
definierten Argumenten -- hier `css` -- auf das Objekt `website` angewandt und
das Ergebnis erneut an die nächste Zeile weitergegeben, in der die Funktion
`html_text()` auf dieses angewandt wird. Hier endet die Pipe und das Endergebnis
wird dem Objekt country zugewiesen.

Wir benötigen nun zwar drei statt zwei Zeilen um zu dem Selben Ergebnis zu
kommen, die tatsächliche Tipparbeit hat sich aber verringert -- vor allem wenn
man die Pipe mit der Tastenkombination STRG+Shift+M erzeugt -- und wir haben
Code erzeugt der sich mit ein bisschen Übung intuitiver lesen und nachvollziehen
lässt.

Sollten wir also immer alle Arbeitsschritte mit Pipes verbinden? Nein. In vielen
Fällen macht es durchaus Sinn, Zwischenergebnisse in einem Objekt zu speichern,
nämlich immer dann wenn wir mehrfach auf dieses zugreifen werden. Wir könnten in
unserem Beispiel auch das Einlesen der Website in die Pipe integrieren:

``` {r}
country <- read_html("https://scrapethissite.com/pages/simple/") %>% 
  html_nodes(css = "h3") %>% 
  html_text(trim = TRUE)
```

Insgesamt erspart uns dies noch mehr Tipparbeit. Da wir im Weiteren jedoch noch
mehrfach auf die ausgelesene Website zugreifen müssen, würde dies auch bedeuten,
dass der Ausleseschritt jedes Mal wiederholt werden muss. Zum Einen kann dies
bei größeren Datenmengen spürbare Auswirkungen auf die Rechenzeit haben. Zum
Anderen bedeutet dies auch, dass wir jedes Mal auf die Server der Website
zugreifen und die Daten erneut herunterladen. Ohne triftige Gründe erzeugtes
Datenaufkommen, sollten wir aber im Rahmen einer *good practice* des Web
Scrapings vermeiden. Es macht also durchaus Sinn, das Ergebnis der `read_html()`
Funktion in einem R-Objekt zu speichern, um es später mehrfach wiederverwenden
zu können.

### Haupstädte, Einwohnerzahl und Fläche

Wenden wir uns nun den weiteren Informationen für jedes Land zu. Diese befinden
sich im zweiten Block des weiter oben betrachteten HTML-Codes:

```
<div class="country-info">
  <strong>Capital:</strong> <span class="country-capital">Andorra la Vella</span><br>
  <strong>Population:</strong> <span class="country-population">84000</span><br>
  <strong>Area (km<sup>2</sup>):</strong> <span class="country-area">468.0</span><br>
</div>
```

Wie wir sehen, sind sowohl der Name der Hauptstadt, die Einwohnerzahl des Landes
sowie dessen Größe in Quadratkilometern in den Zeilen 2--4 jeweils von einem
`<span>` Tag umschlossen. `<span>` definiert wie `<div>` Gruppierungen,
allerdings nicht über mehrere Zeilen hinweg sondern für eine, oder wie hier
einen Teil einer Zeile. Versuchen wir also die Namen der Haupstädte auszulesen,
indem wir den `<span>` Tag als Selector nutzen.

``` {r}
website %>% 
  html_nodes(css="span") %>% 
  html_text() %>% 
  head(n=10)
```

Wir bekommen so zwar die Namen der Hauptstädte, aber auch die Einwohnerzahl und
die Größe des Landes. `span` war als Selector zu unspezifisch. Da alle drei
Arten von Länderdaten mit `<span>` Tags umfasst sind, werden auch alle drei
ausgelesen. Wir müssen `html_nodes()` also genauer mitteilen, an welchem
`<span>` wir interessiert sind. Hier kommen nun die bereits angesprochenen
CSS-Klassen ins Spiel. Diese unterscheiden sich zwischen den drei
Länderinformationen. So ist dem `<span>`, der den Namen der Hauptstadt umfasst,
die Klasse `"country-capital"` zugewiesen. Diese Klasse können wir mit unserem
*CSS Selector* gezielt ansteuern. Um eine Klasse auszuwählen, können wir die
Syntax `.Klassenname` nutzen. Um also alle `<span>` auszuwählen, die die Klasse
`"country-capital"` haben, können wir wie folgend vorgehen:

``` {r}
capital <- website %>% 
  html_nodes(css="span.country-capital") %>% 
  html_text()

  head(capital, n=10)
```

Dies können wir analog für die Einwohnerzahl mit der Klasse `country-population` wiederholen.

``` {r}
population <- website %>% 
  html_nodes(css="span.country-population") %>% 
  html_text()

  head(population, n=10)
```

Betrachten wir den so erzeugten Vektor genauer, sehen wir, dass es sich dabei um
einen character vector handelt. Dazu könen wir die Funktion `str()` nutzen,
welche uns die Struktur eines R-Objekts ausgibt, darunter auch den genutzten
Datentyp.

``` {r}
str(population)
```

Die Zahlen wurden also nicht als Zahlen ausgelesen sondern als Strings. Dies
erlaubt unter anderem nicht, mit den Zahlen zu rechnen. (`population[1]` wählt
hier das erste Element des Vektors aus, also "84000").

``` {r, error = TRUE}
population[1] / 2
```

Eine Möglichkeit R anzuweisen den aus dem HTML-Code ausgelesenen "Text" als
Zahlen zu speichern, ist die Nutzung der Funktion `as.numeric()`.

``` {r}
population <- website %>% 
  html_nodes(css = "span.country-population") %>% 
  html_text() %>% 
  as.numeric()

str(population)

population[1] / 2
```

Auf dem selben Weg, lässt sich auch die Größe in Quadratkilometern mit der
Klasse `"country-area"` auslesen.

``` {r}
area <- website %>% 
  html_nodes(css = "span.country-area") %>% 
  html_text() %>% 
  as.numeric()

str(area)
```


### Zusammenführen in einem Tibble

Wir haben nun vier Vektoren erstellt, welche respektive die Informationen zum
Namen des Landes, der zugehörigen Hauptstadt, der Bevölkerunsanzahl und der
Größe des Landes beinhalten. Für Andorra:

``` {r}
country[1]
capital[1]
population[1]
area[1]
```

Damit könnten wir bereits weiterarbeiten, für viele Anwendungen ist es aber
praktischer, wenn wir die Daten in Tabellenform zusammenstellen. Im tidyverse
bietet sich dazu die Form des Tibbles an.

``` {r}
countries <- tibble(
  Land = country,
  Hauptstadt = capital,
  Bevoelkerung = population,
  Flaeche = area
)

countries
```

Dies ist nicht nur übersichtlicher sondern erleichtert auch alle weiteren
potentiellen Analyseschritte.

Wenn wir uns sicher sind, dass wir die einzelnen Vektoren nicht benötigen,
können wir das Auslesen der Daten und das Erstellen des Tibbles auch in einem
einzelnen Schritt durchführen. Im Folgenden sehen Sie, wie der komplette
Scraping Prozess in relativ wenigen Zeilen abgeschlossen werden kann.

``` {r}
website <- "https://scrapethissite.com/pages/simple/" %>%
  read_html()

countries_2 <- tibble(
  Land = website %>%
    html_nodes(css="h3") %>% 
    html_text(trim=TRUE),
  Hauptstadt = website %>% 
    html_nodes(css="span.country-capital") %>% 
    html_text(),
  Bevoelkerung=website %>% 
    html_nodes(css="span.country-population") %>% 
    html_text() %>% 
    as.numeric(),
  Flaeche = website %>% 
    html_nodes(css="span.country-area") %>% 
    html_text() %>% 
    as.numeric()
)

countries_2
```